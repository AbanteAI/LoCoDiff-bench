#!/usr/bin/env python3
"""
Analyzes the results of LoCoDiff benchmark runs and generates summaries.

Purpose:
  This script processes the output generated by `3_run_benchmark.py`. It reads
  the benchmark structure metadata and the individual run results for different
  models. It then calculates aggregate statistics, such as success rates per model
  and per prompt token bucket. Finally, it prints summary tables to the console
  and generates a plot visualizing the success rates across different token buckets
  for each model.

Arguments:
  --benchmark-dir (optional): Directory containing the benchmark structure metadata
                              (`metadata.json`). (default: 'generated_prompts').
  --results-dir (optional): Base directory containing the benchmark run results,
                            organized by case, model, and timestamp.
                            (default: 'benchmark_results').
  --plot-file (optional): Path where the output success rate plot image
                          (e.g., PNG) should be saved. The directory will be
                          created if it doesn't exist.
                          (default: 'benchmark_results/benchmark_success_rate.png').
  --no-plot (optional): If set, the script will skip generating and saving the
                        success rate plot.

Inputs:
  - `benchmark-dir/metadata.json`: Contains the overall benchmark structure,
    including the list of benchmark cases organized by prompt token buckets.
    Generated by `2_generate_prompts.py`.
  - Individual run results located within `results-dir`. Specifically, it reads
    `metadata.json` files from the latest timestamped run directory for each
    benchmark case and model found:
    `results-dir/[benchmark_case_prefix]/[model_name]/[timestamp]/metadata.json`.
    Generated by `3_run_benchmark.py`.
  - Command-line arguments.

Outputs:
  - Prints summary tables to the standard output:
    - Overall Model Performance Summary: Shows total defined cases, runs found,
      successful runs, success rate, and total cost per model.
    - Per-Bucket Success Rate (%): Shows the success rate for each model within
      each defined prompt token bucket.
  - Creates or overwrites the plot image file specified by `--plot-file`
    (default: `benchmark_results/benchmark_success_rate.png`), unless `--no-plot`
    is specified. This plot visualizes the per-bucket success rates.

File Modifications:
  - Creates the directory for the `--plot-file` if it doesn't exist.
  - Creates or overwrites the plot image file specified by `--plot-file` (unless `--no-plot` is used).
  - Reads `metadata.json` from `benchmark-dir`.
  - Reads `metadata.json` files from subdirectories within `results-dir`.
  - Does *not* modify any input metadata files.
  - Does *not* modify files in `cached-repos` or `generated_prompts` (other than reading metadata).
"""

import argparse
import os
import json
import glob
import re
import sys
from typing import Any, Dict, Optional, Tuple, List
from collections import defaultdict

# Attempt to import pandas and matplotlib, provide guidance if missing
try:
    import pandas as pd
    import matplotlib.pyplot as plt
    import matplotlib.ticker as mticker  # Import ticker for FuncFormatter
except ImportError as e:
    print(
        f"Error importing libraries: {e}. Please ensure pandas and matplotlib are installed."
    )
    print("You may need to run: pip install pandas matplotlib")
    sys.exit(1)


# --- Helper Functions ---


def format_bucket_key(key_str: str) -> str:
    """Formats a bucket key string 'min-max' into 'min/1k-max/1k k'."""
    try:
        min_val, max_val = map(int, key_str.split("-"))
        # Format as thousands, using 'k' suffix
        min_k = min_val // 1000
        max_k = max_val // 1000
        # Handle the 0 case specifically if needed, but simple division works
        return f"{min_k}-{max_k}k"
    except ValueError:
        # Handle cases where splitting or conversion fails (e.g., unexpected key format)
        return key_str  # Return original key if formatting fails


def load_json_file(filepath: str) -> Optional[Dict[str, Any]]:
    """Loads a JSON file, returning None on error."""
    if not os.path.exists(filepath):
        # print(f"Warning: File not found: {filepath}") # Optional: less verbose
        return None
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            return json.load(f)
    except (json.JSONDecodeError, IOError) as e:
        print(f"Warning: Error reading or parsing JSON file {filepath}: {e}")
        return None


# --- Core Logic ---


def scan_results_directory(
    results_base_dir: str,
) -> Tuple[List[str], Dict[str, Dict[str, Optional[Dict[str, Any]]]]]:
    """
    Scans the results directory to find all models and their latest run metadata for each benchmark case.

    Args:
        results_base_dir: Path to the benchmark_results directory.

    Returns:
        A tuple containing:
        - A sorted list of unique model names found.
        - A dictionary mapping model_name -> benchmark_case_prefix -> latest_run_metadata (or None if not found/error).
          Example: {"openai_gpt-4o": {"case1_prefix": {...metadata...}, "case2_prefix": None}, ...}
    """
    print(f"Scanning results directory: {results_base_dir}...")
    # Structure: {model_name: {case_prefix: (latest_timestamp, metadata_path)}}
    latest_runs: Dict[str, Dict[str, Tuple[str, str]]] = defaultdict(
        lambda: defaultdict(lambda: ("", ""))
    )
    models = set()

    # Pattern: results_base_dir / * (benchmark_case) / * (model_name) / * (timestamp) / metadata.json
    pattern = os.path.join(results_base_dir, "*", "*", "*", "metadata.json")
    metadata_files = glob.glob(pattern)

    for metadata_path in metadata_files:
        try:
            parts = metadata_path.split(os.sep)
            # Ensure path structure is as expected before accessing indices
            if len(parts) >= 4:
                timestamp = parts[-2]
                model_name = parts[-3]
                benchmark_case_prefix = parts[-4]

                # Validate timestamp format
                if not re.match(r"\d{8}_\d{6}", timestamp):
                    continue  # Skip if the directory name isn't a valid timestamp

                models.add(model_name)

                # Check if this timestamp is later than the one currently stored
                current_latest_ts, _ = latest_runs[model_name][benchmark_case_prefix]
                if timestamp > current_latest_ts:
                    latest_runs[model_name][benchmark_case_prefix] = (
                        timestamp,
                        metadata_path,
                    )
            else:
                # Handle unexpected path structure if necessary
                # print(f"Warning: Skipping unexpected path structure: {metadata_path}")
                pass

        except IndexError:
            # Handle cases where splitting the path doesn't yield enough parts
            # print(f"Warning: Could not parse path components for: {metadata_path}")
            pass

    # Now load the metadata for the latest runs identified
    results_data: Dict[str, Dict[str, Optional[Dict[str, Any]]]] = defaultdict(dict)
    for model_name, cases in latest_runs.items():
        for case_prefix, (_, metadata_path) in cases.items():
            results_data[model_name][case_prefix] = load_json_file(metadata_path)

    sorted_models = sorted(list(models))
    print(f"Scan complete. Found {len(sorted_models)} models.")
    return sorted_models, results_data


def analyze_results(
    benchmark_metadata: dict,
    models_found: List[str],
    results_data: Dict[str, Dict[str, Optional[Dict[str, Any]]]],
) -> dict:
    """
    Analyzes benchmark results using pre-scanned data.

    Args:
        benchmark_metadata: Loaded metadata from generated_prompts/metadata.json.
        models_found: List of model names found during the scan.
        results_data: Dictionary mapping model -> case -> latest metadata (or None).

    Returns:
        A dictionary containing aggregated analysis results per model and per bucket.
        (Structure is the same as the previous version).
    """
    # Initialize analysis structure explicitly
    analysis: dict[str, Any] = {"models": {}}
    # Ensure bucket keys are sorted for consistent processing and output
    all_bucket_keys = sorted(
        benchmark_metadata.get("benchmark_buckets", {}).keys(),
        key=lambda k: int(k.split("-")[0]),  # Sort numerically by lower bound
    )
    analysis["bucket_keys"] = all_bucket_keys

    print(f"Analyzing results for models: {models_found}")
    print(f"Using {len(all_bucket_keys)} buckets defined in benchmark metadata.")

    # Initialize structure for all models found and all defined buckets
    for model_name in models_found:
        analysis["models"][model_name] = {
            "total_benchmarks": 0,
            "runs_found": 0,
            "successful_runs": 0,
            "total_cost_usd": 0.0,
            "success_rate": 0.0,
            "buckets": {
                key: {
                    "total_in_bucket": 0,
                    "runs_found": 0,
                    "successful_runs": 0,
                    "success_rate": 0.0,
                }
                for key in all_bucket_keys
            },
        }

    # Iterate through the defined benchmark structure
    defined_buckets = benchmark_metadata.get("benchmark_buckets", {})
    if not defined_buckets:
        print(
            "Warning: No benchmark buckets found in metadata. Analysis might be empty."
        )
        return analysis  # Return the initialized (empty) structure

    for bucket_key, benchmark_cases in defined_buckets.items():
        for case_info in benchmark_cases:
            benchmark_case_prefix = case_info["benchmark_case_prefix"]

            # Update total counts for each model
            for model_name in models_found:
                analysis["models"][model_name]["total_benchmarks"] += 1
                analysis["models"][model_name]["buckets"][bucket_key][
                    "total_in_bucket"
                ] += 1

                # Get the pre-loaded result metadata for this case and model
                result_meta = results_data.get(model_name, {}).get(
                    benchmark_case_prefix
                )

                # Check if a result was found and metadata loaded successfully
                if result_meta is not None:
                    # Increment found counts
                    analysis["models"][model_name]["runs_found"] += 1
                    analysis["models"][model_name]["buckets"][bucket_key][
                        "runs_found"
                    ] += 1

                    # Check success
                    if result_meta.get("success", False):
                        analysis["models"][model_name]["successful_runs"] += 1
                        analysis["models"][model_name]["buckets"][bucket_key][
                            "successful_runs"
                        ] += 1

                    # Accumulate cost
                    cost = result_meta.get("cost_usd", 0.0) or 0.0  # Handle None
                    analysis["models"][model_name]["total_cost_usd"] += float(cost)

    # Calculate success rates
    for model_name, model_stats in analysis["models"].items():
        if model_stats["runs_found"] > 0:
            model_stats["success_rate"] = (
                model_stats["successful_runs"] / model_stats["runs_found"]
            )
        for bucket_key, bucket_stats in model_stats["buckets"].items():
            if bucket_stats["runs_found"] > 0:
                bucket_stats["success_rate"] = (
                    bucket_stats["successful_runs"] / bucket_stats["runs_found"]
                )

    return analysis


def print_summary_tables(analysis_results: dict):
    """Prints formatted summary tables to the console."""
    print("\n--- Overall Model Performance Summary ---")
    models_data = []
    # Sort models alphabetically for consistent table order
    sorted_model_names = sorted(analysis_results["models"].keys())

    for model_name in sorted_model_names:
        stats = analysis_results["models"][model_name]
        models_data.append(
            {
                "Model": model_name,
                "Defined": stats["total_benchmarks"],
                "Runs Found": stats["runs_found"],
                "Successful": stats["successful_runs"],
                "Success Rate": f"{stats['success_rate']:.1%}",
                "Total Cost (USD)": f"${stats['total_cost_usd']:.4f}",
            }
        )

    if not models_data:
        print("No model results found to summarize.")
        return

    df_overall = pd.DataFrame(models_data)
    print(df_overall.to_string(index=False))

    print("\n--- Per-Bucket Success Rate (%) ---")
    bucket_keys = analysis_results["bucket_keys"]
    # Format bucket keys using the helper function and update header
    formatted_keys = [format_bucket_key(k) for k in bucket_keys]
    bucket_data = {"Bucket (k tk)": formatted_keys}  # Updated header

    for model_name in sorted_model_names:  # Use sorted names here too
        stats = analysis_results["models"][model_name]
        rates = []
        for key in bucket_keys:
            bucket_stats = stats["buckets"][key]
            # Display rate if runs were found, otherwise indicate N/A or 0 runs
            if bucket_stats["runs_found"] > 0:
                rates.append(f"{bucket_stats['success_rate']:.1%}")
            elif bucket_stats["total_in_bucket"] > 0:
                rates.append("0/0")  # Indicate 0 runs found for defined benchmarks
            else:
                rates.append(
                    "-"
                )  # Bucket was empty for this model (shouldn't happen with current logic)
        bucket_data[model_name] = rates

    df_buckets = pd.DataFrame(bucket_data)
    print(df_buckets.to_string(index=False))


def generate_plot(analysis_results: dict, output_filename: str):
    """Generates and saves a plot of per-bucket success rates."""
    print(f"\nGenerating plot and saving to {output_filename}...")
    # Sort models alphabetically for consistent plot order/colors
    models = sorted(list(analysis_results["models"].keys()))
    bucket_keys = analysis_results["bucket_keys"]
    # Use bucket midpoints for plotting x-axis? Or just indices? Indices are simpler.
    # Let's use indices for simplicity, label ticks with bucket ranges.
    x_indices = range(len(bucket_keys))
    # Format bucket keys for tick labels
    formatted_bucket_labels = [format_bucket_key(k) for k in bucket_keys]

    plt.figure(figsize=(12, 7))

    for model_name in models:
        success_rates = []
        for key in bucket_keys:
            # Get rate, default to NaN if no runs found so it creates gaps in lines
            rate = analysis_results["models"][model_name]["buckets"][key][
                "success_rate"
            ]
            runs_found = analysis_results["models"][model_name]["buckets"][key][
                "runs_found"
            ]
            success_rates.append(rate if runs_found > 0 else float("nan"))

        # Get total cost for the legend
        total_cost = analysis_results["models"][model_name].get("total_cost_usd", 0.0)
        legend_label = (
            f"{model_name} (${total_cost:.2f})"  # Format cost to 2 decimal places
        )

        plt.plot(
            x_indices, success_rates, marker="o", linestyle="-", label=legend_label
        )

    # Formatting the plot
    plt.title("Benchmark Success Rate per Prompt Token Bucket")
    plt.xlabel("Prompt Token Bucket (k tk)")  # Updated x-axis label
    plt.ylabel("Success Rate")
    # Format y-axis as percentage
    plt.gca().yaxis.set_major_formatter(
        mticker.FuncFormatter(lambda y, _: f"{y:.0%}")
    )  # Use mticker
    plt.ylim(0, 1.05)  # Extend slightly beyond 100%
    # Use formatted labels for x-ticks
    plt.xticks(x_indices, formatted_bucket_labels, rotation=45, ha="right")
    plt.legend(title="Models", bbox_to_anchor=(1.04, 1), loc="upper left")
    plt.grid(axis="y", linestyle="--", alpha=0.7)
    # Use tuple for rect argument
    plt.tight_layout(rect=(0, 0, 0.85, 1))  # Adjust layout to make space for legend

    # Ensure the output directory exists
    output_dir = os.path.dirname(output_filename)
    if output_dir and not os.path.exists(output_dir):
        try:
            os.makedirs(output_dir, exist_ok=True)
            print(f"Created directory for plot: {output_dir}")
        except OSError as e:
            # Raise error instead of just printing
            raise OSError(f"Error creating directory {output_dir}: {e}") from e
            # Optionally decide if you want to proceed or exit
            # For now, we'll let savefig handle the potential error if dir creation failed

    # Save the plot
    try:
        plt.savefig(output_filename, dpi=300, bbox_inches="tight")
        print(f"Plot saved successfully to {output_filename}")
    except Exception as e:
        # Raise error instead of just printing
        raise RuntimeError(f"Error saving plot to {output_filename}: {e}") from e

    # Optionally display the plot
    # plt.show()


def main():
    parser = argparse.ArgumentParser(
        description="Analyze LoCoDiff benchmark results, print summaries, and generate plots."
    )
    default_benchmark_dir = "generated_prompts"
    default_results_dir = "benchmark_results"
    # Default plot location changed to be inside results dir
    default_plot_file = os.path.join(default_results_dir, "benchmark_success_rate.png")

    parser.add_argument(
        "--benchmark-dir",
        default=default_benchmark_dir,
        help=f"Directory containing the benchmark metadata.json (default: '{default_benchmark_dir}').",
    )
    parser.add_argument(
        "--results-dir",
        default=default_results_dir,
        help=f"Base directory containing the benchmark run results (default: '{default_results_dir}').",
    )
    parser.add_argument(
        "--plot-file",
        default=default_plot_file,
        help=f"Filename to save the generated plot (default: '{default_plot_file}'). Directory will be created if it doesn't exist.",
    )
    parser.add_argument(
        "--no-plot",
        action="store_true",
        help="If set, skip generating and saving the plot.",
    )

    args = parser.parse_args()

    print("--- Starting Benchmark Analysis ---")
    print(f"Benchmark Metadata Directory: {args.benchmark_dir}")
    print(f"Results Directory: {args.results_dir}")
    if not args.no_plot:
        print(f"Output Plot File: {args.plot_file}")
    else:
        print("Plot generation disabled.")

    # 1. Load Benchmark Metadata
    benchmark_metadata_path = os.path.join(args.benchmark_dir, "metadata.json")
    benchmark_metadata = load_json_file(benchmark_metadata_path)
    if benchmark_metadata is None:
        print(
            f"Error: Benchmark metadata file not found or invalid at {benchmark_metadata_path}"
        )
        print("Please run generate_prompts.py first.")
        return 1  # Error loading metadata

    # 2. Scan Results Directory
    models_found, results_data = scan_results_directory(args.results_dir)
    if not models_found:
        print(
            "Warning: No models found in the results directory. No analysis to perform."
        )
        # Still print empty tables for consistency? Or exit? Let's exit.
        print("\n--- Benchmark Analysis Complete (No Results Found) ---")
        return 0

    # 3. Analyze Results using scanned data
    analysis = analyze_results(benchmark_metadata, models_found, results_data)

    # 4. Print Summary Tables
    print_summary_tables(analysis)

    # 5. Generate Plot (if not disabled)
    if not args.no_plot:
        if not analysis["models"]:
            print("\nSkipping plot generation as no model results were analyzed.")
        else:
            generate_plot(analysis, args.plot_file)

    print("\n--- Benchmark Analysis Complete ---")
    return 0


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
