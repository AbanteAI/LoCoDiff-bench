# LoCoDiff Benchmark Explorer

A simple Flask web application to explore the results generated by the LoCoDiff benchmark scripts.

## Setup

1.  **Install Dependencies:**
    Make sure you have `uv` installed. Then, install the required Python packages from the project root directory:
    ```bash
    uv pip install -r requirements.txt
    ```
    *(This assumes you have already run `./.mentat/setup.sh` at least once, which also installs requirements.)*

2.  **Generate Benchmark Data (if needed):**
    Ensure you have run the benchmark steps from the `benchmark_pipeline` directory:
    *   `./benchmark_pipeline/1_clone_repos.py -r <org/repo1> <org/repo2> ...`
    *   `./benchmark_pipeline/2_generate_prompts.py -e .py .js ...`
    *   `./benchmark_pipeline/3_run_benchmark.py --model <model_identifier> --num-runs -1` (e.g., `openai/gpt-4o`)
    *   `./results_explorer/analyze_results.py` (This generates the plot used by the explorer)

## Running the Explorer

From the root directory of the `LoCoDiff-bench` repository, run the application directly using Python:

```bash
python results_explorer/app.py
```

This will:
1.  Start the Flask development server.
2.  By default, it runs on `http://127.0.0.1:5001`.
3.  **Automatically open a new tab** in your default web browser pointing to the application.

**Note:** Running the script directly (`python results_explorer/app.py`) is required for the automatic browser opening feature. If you prefer to use `flask run`, you can still do so, but the browser tab will not open automatically:
```bash
# Alternative: Run using flask command (no auto browser open)
flask --app results_explorer/app.py run --port=5001
```

You can modify the `HOST` and `PORT` variables directly within the `if __name__ == "__main__":` block in `results_explorer/app.py` if you need to change the default host (e.g., to `0.0.0.0` for network access) or port.

## Features

*   **Overview Page:** Shows summary statistics, lists models with results, and displays the overall success rate plot.
*   **Model Results Page:** Lists all benchmark runs for a specific model, grouped by prompt token bucket. Shows success/failure status and links to detailed views.
*   **Case Details Page:** Displays the full details for a single benchmark run, including:
    *   Run metadata (success, cost, tokens, errors, etc.)
    *   Prompt content
    *   Expected output
    *   Raw model response
    *   Extracted output (if successful)
    *   Diff between expected and extracted output
