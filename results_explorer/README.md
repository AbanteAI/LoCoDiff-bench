# LoCoDiff Benchmark Explorer

A simple Flask web application to explore the results generated by the LoCoDiff benchmark scripts.

## Setup

1.  **Install Dependencies:**
    Make sure you have `uv` installed. Then, install the required Python packages from the project root directory:
    ```bash
    uv pip install -r requirements.txt
    ```
    *(This assumes you have already run `./.mentat/setup.sh` at least once, which also installs requirements.)*

2.  **Generate Benchmark Data (if needed):**
    Ensure you have run the benchmark steps from the `benchmark_pipeline` directory:
    *   `./benchmark_pipeline/1_clone_repos.py -r <org/repo1> <org/repo2> ...`
    *   `./benchmark_pipeline/2_generate_prompts.py -e .py .js ...`
    *   `./benchmark_pipeline/3_run_benchmark.py --model <model_identifier> --num-runs -1` (e.g., `openai/gpt-4o`)
    *   *(The analysis and plot generation previously done by `analyze_results.py` are now handled automatically when starting the explorer app below).*

## Running the Explorer

From the root directory of the `LoCoDiff-bench` repository, run the application directly using Python:

```bash
python results_explorer/app.py
```

This will:
1.  **Run the analysis** on existing benchmark results found in `benchmark_results/`.
2.  **Generate the summary plot** (`benchmark_results/benchmark_success_rate.png`) and copy it to the static directory for display.
3.  Start the Flask development server (default: `http://127.0.0.1:5001`).
4.  **Automatically open a new tab** in your default web browser pointing to the application.

**Note:** Running the script directly (`python results_explorer/app.py`) is required for the automatic analysis, plot generation, and browser opening features. If you prefer to use `flask run`, you can still do so, but these automatic steps will not occur, and the browser tab will not open automatically:
```bash
# Alternative: Run using flask command (no auto analysis/plot/browser open)
flask --app results_explorer/app.py run --port=5001
```

You can modify the `HOST` and `PORT` variables directly within the `if __name__ == "__main__":` block in `results_explorer/app.py` if you need to change the default host (e.g., to `0.0.0.0` for network access) or port.

## Features

*   **Overview Page:** Shows summary statistics (overall model performance, per-bucket success rates), lists models with results, and displays the overall success rate plot.
*   **Model Results Page:** Lists all benchmark runs for a specific model, grouped by prompt token bucket. Shows success/failure status and links to detailed views.
*   **Case Details Page:** Displays the full details for a single benchmark run, including:
    *   Run metadata (success, cost, tokens, errors, etc.)
    *   Prompt content
    *   Expected output
    *   Raw model response
    *   Extracted output (if successful)
    *   Diff between expected and extracted output
