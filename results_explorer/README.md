# LoCoDiff Benchmark Explorer

A simple Flask web application to explore the results generated by the LoCoDiff benchmark scripts.

## Setup

1.  **Install Dependencies:**
    Make sure you have `uv` installed. Then, install the required Python packages from the project root directory:
    ```bash
    uv pip install -r requirements.txt
    ```
    *(This assumes you have already run `./.mentat/setup.sh` at least once, which also installs requirements.)*

2.  **Generate Benchmark Data (if needed):**
    Ensure you have run the benchmark steps from the `benchmark_pipeline` directory:
    *   `./benchmark_pipeline/1_clone_repos.py -r <org/repo1> <org/repo2> ...`
    *   `./benchmark_pipeline/2_generate_prompts.py -e .py .js ...`
    *   `./benchmark_pipeline/3_run_benchmark.py --model <model_identifier> --num-runs -1` (e.g., `openai/gpt-4o`)
    *   *(The analysis and plot generation previously done by `analyze_results.py` are now handled automatically when starting the explorer app below).*

3.  **Preprocess Sliding Window Data (required):**
    To generate the data needed for the sliding window chart:
    ```bash
    ./benchmark_pipeline/3_preprocess_sliding_data.py --benchmark-run-dir path/to/your/benchmark_run
    ```
    This generates a static JSON file with preprocessed sliding window chart data. This step is required for the sliding window chart to work. The app will show a warning if this file is missing.

## Running the Explorer

From the root directory of the `LoCoDiff-bench` repository, run the application directly using Python, providing the path to your benchmark run directory:

```bash
python results_explorer/app.py --benchmark-run-dir path/to/your/benchmark_run
```
*(Replace `path/to/your/benchmark_run` with the actual path you used for scripts 1 and 2).*

This will:
1.  **Run the analysis** on existing benchmark results found in `<benchmark_run_dir>/results/`.
2.  Start the Flask development server (default: `http://127.0.0.1:5001`).
3.  **Automatically open a new tab** in your default web browser pointing to the application.

**Note:** Running the script directly with the `--benchmark-run-dir` argument is required for the automatic analysis and browser opening features. If you prefer to use `flask run`, you can still do so, but you'll need to set the `FLASK_APP` environment variable, and the automatic steps (analysis, browser open) will not occur:
```bash
# Alternative: Run using flask command (no auto analysis/browser open)
# You would need to modify app.py to load the benchmark dir differently, e.g., via env var
export FLASK_APP="results_explorer/app.py"
# export BENCHMARK_RUN_DIR="path/to/your/benchmark_run" # Example env var approach
flask run --host=127.0.0.1 --port=5001
```

You can modify the host and port using the `--host` and `--port` command-line arguments when running `python results_explorer/app.py`. For example:
```bash
python results_explorer/app.py --benchmark-run-dir path/to/run --host 0.0.0.0 --port 8080
```

## Features

*   **Overview Page:** Shows summary statistics (overall model performance, per-bucket success rates), lists models with results, and displays the overall success rate plot.
*   **Model Results Page:** Lists all benchmark runs for a specific model, grouped by prompt token bucket. Shows success/failure status and links to detailed views.
*   **Case Details Page:** Displays the full details for a single benchmark run, including:
    *   Run metadata (success, cost, tokens, errors, etc.)
    *   Prompt content
    *   Expected output
    *   Raw model response
    *   Extracted output (if successful)
    *   Diff between expected and extracted output
