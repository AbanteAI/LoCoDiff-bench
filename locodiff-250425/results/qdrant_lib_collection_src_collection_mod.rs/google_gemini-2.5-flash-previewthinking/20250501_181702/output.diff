
index 71e15f9b..4a729a54 100644
--- a/qdrant_lib_collection_src_collection_mod.rs_expectedoutput.txt (expected):tmp/tmpos87g1v8_expected.txt	
+++ b/qdrant_lib_collection_src_collection_mod.rs_extracted.txt (actual):tmp/tmp3kiqei0k_actual.txt	
@@ -20,6 +20,10 @@ use std::time::Duration;
 
 use clean::ShardCleanTasks;
 use common::budget::ResourceBudget;
+use common::collection_size_stats::{
+    CollectionSizeAtomicStats, CollectionSizeStats, CollectionSizeStatsCache,
+};
+use common::is_ready::IsReady;
 use common::types::{DetailsLevel, TelemetryDetail};
 use io::storage_version::StorageVersion;
 use segment::types::ShardKey;
@@ -29,11 +33,8 @@ use tokio::sync::{Mutex, RwLock, RwLockWriteGuard};
 
 use crate::collection::payload_index_schema::PayloadIndexSchema;
 use crate::collection_state::{ShardInfo, State};
-use crate::common::collection_size_stats::{
-    CollectionSizeAtomicStats, CollectionSizeStats, CollectionSizeStatsCache,
-};
-use crate::common::is_ready::IsReady;
 use crate::config::CollectionConfigInternal;
+use crate::operations::cluster_ops::ReshardingDirection;
 use crate::operations::config_diff::{DiffConfig, OptimizersConfigDiff};
 use crate::operations::shared_storage_config::SharedStorageConfig;
 use crate::operations::types::{CollectionError, CollectionResult, NodeType, OptimizersStatus};
@@ -46,13 +47,14 @@ use crate::shards::replica_set::ReplicaState::{Active, Dead, Initializing, Liste
 use crate::shards::replica_set::{
     ChangePeerFromState, ChangePeerState, ReplicaState, ShardReplicaSet,
 };
+use crate::shards::resharding::{ReshardKey, ReshardingState};
 use crate::shards::shard::{PeerId, ShardId};
 use crate::shards::shard_holder::shard_mapping::ShardKeyMapping;
-use crate::shards::shard_holder::{LockedShardHolder, ShardHolder, shard_not_found_error};
+use crate::shards::shard_holder::{shard_not_found_error, LockedShardHolder, ShardHolder};
 use crate::shards::transfer::helpers::check_transfer_conflicts_strict;
 use crate::shards::transfer::transfer_tasks_pool::{TaskResult, TransferTasksPool};
 use crate::shards::transfer::{ShardTransfer, ShardTransferMethod};
-use crate::shards::{CollectionId, replica_set};
+use crate::shards::{replica_set, CollectionId};
 use crate::telemetry::{
     CollectionConfigTelemetry, CollectionTelemetry, CollectionsAggregatedTelemetry,
 };
@@ -155,7 +157,6 @@ impl Collection {
                 update_runtime.clone().unwrap_or_else(Handle::current),
                 search_runtime.clone().unwrap_or_else(Handle::current),
                 optimizer_resource_budget.clone(),
-                None,
             )
             .await?;
 
@@ -247,8 +248,6 @@ impl Collection {
         });
         collection_config.validate_and_warn();
 
-        let mut shard_holder = ShardHolder::new(path).expect("Can not create shard holder");
-
         let mut effective_optimizers_config = collection_config.optimizer_config.clone();
 
         if let Some(optimizers_overwrite) = optimizers_overwrite.clone() {
@@ -257,6 +256,8 @@ impl Collection {
                 .expect("Can not apply optimizer overwrite");
         }
 
+        let mut shard_holder = ShardHolder::new(path).expect("Can not create shard holder");
+
         let shared_collection_config = Arc::new(RwLock::new(collection_config.clone()));
 
         let payload_index_schema = Arc::new(
@@ -423,7 +424,12 @@ impl Collection {
         }
 
         // 2. Check that `from_state` matches current state
-        if from_state.is_some() && current_state != from_state {
+        // Do not be strict about from state if in shard transfer related state (Partial, Resharding)
+        let is_shard_transfer_related_state = matches!(
+            current_state,
+            Some(ReplicaState::Partial) | Some(ReplicaState::Resharding)
+        );
+        if from_state.is_some() && current_state != from_state && !is_shard_transfer_related_state {
             return Err(CollectionError::bad_input(format!(
                 "Replica {peer_id} of shard {shard_id} has state {current_state:?}, but expected {from_state:?}"
             )));
@@ -443,6 +449,31 @@ impl Collection {
             .ensure_replica_with_state(peer_id, new_state)
             .await?;
 
+        // If not initialized yet, we need to check if it was initialized by this call
+        if !self.is_initialized.check_ready() {
+            let state = self.state().await;
+
+            let mut is_ready = true;
+
+            for (_shard_id, shard_info) in state.shards {
+                let all_replicas_active = shard_info.replicas.into_iter().all(|(_, state)| {
+                    matches!(
+                        state,
+                        ReplicaState::Active | ReplicaState::ReshardingScaleDown
+                    )
+                });
+
+                if !all_replicas_active {
+                    is_ready = false;
+                    break;
+                }
+            }
+
+            if is_ready {
+                self.is_initialized.make_ready();
+            }
+        }
+
         if new_state == ReplicaState::Dead {
             let resharding_state = shard_holder.resharding_state.read().clone();
             let related_transfers = shard_holder.get_related_transfers(shard_id, peer_id);
@@ -469,6 +500,7 @@ impl Collection {
                 .as_ref()
                 .is_some_and(ReplicaState::is_resharding);
             if is_resharding {
+                // check if the state is related to the ongoing resharding before initiating abort
                 if let Some(state) = resharding_state {
                     abort_resharding_result = self.abort_resharding(state.key(), false).await;
                 }
@@ -483,31 +515,6 @@ impl Collection {
             abort_resharding_result?;
         }
 
-        // If not initialized yet, we need to check if it was initialized by this call
-        if !self.is_initialized.check_ready() {
-            let state = self.state().await;
-
-            let mut is_ready = true;
-
-            for (_shard_id, shard_info) in state.shards {
-                let all_replicas_active = shard_info.replicas.into_iter().all(|(_, state)| {
-                    matches!(
-                        state,
-                        ReplicaState::Active | ReplicaState::ReshardingScaleDown
-                    )
-                });
-
-                if !all_replicas_active {
-                    is_ready = false;
-                    break;
-                }
-            }
-
-            if is_ready {
-                self.is_initialized.make_ready();
-            }
-        }
-
         Ok(())
     }
 
@@ -545,6 +552,7 @@ impl Collection {
         let shards_holder = self.shards_holder.read().await;
         let transfers = shards_holder.shard_transfers.read().clone();
         let resharding = shards_holder.resharding_state.read().clone();
+
         State {
             config: self.collection_config.read().await.clone(),
             shards: shards_holder
@@ -587,6 +595,7 @@ impl Collection {
             .await
     }
 
+    #[allow(clippy::too_many_arguments)]
     pub async fn sync_local_state(
         &self,
         on_transfer_failure: OnTransferFailure,
@@ -598,13 +607,13 @@ impl Collection {
         // Check for disabled replicas
         let shard_holder = self.shards_holder.read().await;
 
-        let get_shard_transfers = |shard_id, from| {
+        let get_shard_transfers = |shard_id: ShardId, from: PeerId| {
             shard_holder
                 .get_transfers(|transfer| transfer.shard_id == shard_id && transfer.from == from)
         };
 
         for replica_set in shard_holder.all_shards() {
-            replica_set.sync_local_state(get_shard_transfers)?;
+            replica_set.sync_local_state(get_shard_transfers).await?;
         }
 
         // Check for un-reported finished transfers
@@ -640,12 +649,6 @@ impl Collection {
             }
         }
 
-        // Count how many transfers we are now proposing
-        // We must track this here so we can reference it when checking for tranfser limits,
-        // because transfers we propose now will not be in the consensus state within the lifetime
-        // of this function
-        let mut proposed = HashMap::<PeerId, usize>::new();
-
         // Check for proper replica states
         for replica_set in shard_holder.all_shards() {
             let this_peer_id = replica_set.this_peer_id();
@@ -688,34 +691,13 @@ impl Collection {
                 continue;
             }
 
-            // Try to find dead replicas with no active transfers
             let transfers = shard_holder.get_transfers(|_| true);
 
-            // Respect shard transfer limit, consider already proposed transfers in our counts
-            let (mut incoming, outgoing) = shard_holder.count_shard_transfer_io(this_peer_id);
-            incoming += proposed.get(&this_peer_id).copied().unwrap_or(0);
-            if self.check_auto_shard_transfer_limit(incoming, outgoing) {
-                log::trace!(
-                    "Postponing automatic shard {shard_id} transfer to stay below limit on this node (incoming: {incoming}, outgoing: {outgoing})",
-                );
-                continue;
-            }
-
-            // Select shard transfer method, prefer user configured method or choose one now
-            // If all peers are 1.8+, we try WAL delta transfer, otherwise we use the default method
-            let shard_transfer_method = self
-                .shared_storage_config
-                .default_shard_transfer_method
-                .unwrap_or_else(|| {
-                    let all_support_wal_delta = self
-                        .channel_service
-                        .all_peers_at_version(&Version::new(1, 8, 0));
-                    if all_support_wal_delta {
-                        ShardTransferMethod::WalDelta
-                    } else {
-                        ShardTransferMethod::default()
-                    }
-                });
+            // Count how many transfers we are now proposing
+            // We must track this here so we can reference it when checking for tranfser limits,
+            // because transfers we propose now will not be in the consensus state within the lifetime
+            // of this function
+            let mut proposed = HashMap::<PeerId, usize>::new();
 
             // Try to find a replica to transfer from
             //
@@ -728,7 +710,22 @@ impl Collection {
                     to_shard_id: None,
                     sync: true,
                     // For automatic shard transfers, always select some default method from this point on
-                    method: Some(shard_transfer_method),
+                    // Select shard transfer method, prefer user configured method or choose one now
+                    // If all peers are 1.8+, we try WAL delta transfer, otherwise we use the default method
+                    method: Some(
+                        self.shared_storage_config
+                            .default_shard_transfer_method
+                            .unwrap_or_else(|| {
+                                let all_support_wal_delta = self
+                                    .channel_service
+                                    .all_peers_at_version(&Version::new(1, 8, 0));
+                                if all_support_wal_delta {
+                                    ShardTransferMethod::WalDelta
+                                } else {
+                                    ShardTransferMethod::default()
+                                }
+                            }),
+                    ),
                 };
 
                 if check_transfer_conflicts_strict(&transfer, transfers.iter()).is_some() {
@@ -746,14 +743,14 @@ impl Collection {
                 }
 
                 // TODO: Should we, maybe, throttle/backoff this requests a bit?
-                if let Err(err) = replica_set.health_check(replica_id).await {
+                if let Some(err) = replica_set.health_check(replica_id).await.err() {
                     // TODO: This is rather verbose, not sure if we want to log this at all... :/
                     log::trace!(
                         "Replica {replica_id}/{}:{} is not available \
                          to request shard transfer from: \
                          {err}",
                         self.id,
-                        replica_set.shard_id,
+                        shard_id,
                     );
                     continue;
                 }
@@ -774,6 +771,122 @@ impl Collection {
         Ok(())
     }
 
+    // Added in 835da45
+    fn check_auto_shard_transfer_limit(&self, incoming: usize, outgoing: usize) -> bool {
+        // We count both incoming and outgoing transfers for a peer.
+        // So if we have `limit` active transfers in total for a peer, no new task will be scheduled.
+        self.shared_storage_config.auto_shard_transfer_limit.map_or(false, |limit| {
+            incoming + outgoing >= limit
+        })
+    }
+
+    pub async fn shard_recovery_point(&self, shard_id: ShardId) -> CollectionResult<RecoveryPoint> {
+        let shard_holder_read = self.shards_holder.read().await;
+
+        let shard = shard_holder_read.get_shard(shard_id);
+        let Some(replica_set) = shard else {
+            return Err(CollectionError::NotFound {
+                what: format!("Shard {shard_id}"),
+            });
+        };
+
+        replica_set.shard_recovery_point().await
+    }
+
+    pub async fn update_shard_cutoff_point(
+        &self,
+        shard_id: ShardId,
+        cutoff: &RecoveryPoint,
+    ) -> CollectionResult<()> {
+        let shard_holder_read = self.shards_holder.read().await;
+
+        let shard = shard_holder_read.get_shard(shard_id);
+        let Some(replica_set) = shard else {
+            return Err(CollectionError::NotFound {
+                what: format!("Shard {shard_id}"),
+            });
+        };
+
+        replica_set.update_shard_cutoff_point(cutoff).await
+    }
+
+    pub async fn state(&self) -> State {
+        let shards_holder = self.shards_holder.read().await;
+        let transfers = shards_holder.shard_transfers.read().clone();
+        let resharding = shards_holder.resharding_state.read().clone();
+
+        State {
+            config: self.collection_config.read().await.clone(),
+            shards: shards_holder
+                .get_shards()
+                .map(|(shard_id, replicas)| {
+                    let shard_info = ShardInfo {
+                        replicas: replicas.peers(),
+                    };
+                    (shard_id, shard_info)
+                })
+                .collect(),
+            resharding,
+            transfers,
+            shards_key_mapping: shards_holder.get_shard_key_to_ids_mapping(),
+            payload_index_schema: self.payload_index_schema.read().clone(),
+        }
+    }
+
+    pub async fn effective_optimizers_config(&self) -> CollectionResult<OptimizersConfig> {
+        let config = self.collection_config.read().await;
+
+        if let Some(optimizers_overwrite) = self.optimizers_overwrite.clone() {
+            Ok(optimizers_overwrite.update(&config.optimizer_config)?)
+        } else {
+            Ok(config.optimizer_config.clone())
+        }
+    }
+
+    pub async fn lock_updates(&self) -> RwLockWriteGuard<()> {
+        self.updates_lock.write().await
+    }
+
+    pub fn wait_collection_initiated(&self, timeout: Duration) -> bool {
+        self.is_initialized.await_ready_for_timeout(timeout)
+    }
+
+    pub fn request_shard_transfer(&self, shard_transfer: ShardTransfer) {
+        self.request_shard_transfer_cb.deref()(shard_transfer)
+    }
+
+    pub fn snapshots_path(&self) -> &Path {
+        &self.snapshots_path
+    }
+
+    pub fn shards_holder(&self) -> Arc<LockedShardHolder> {
+        self.shards_holder.clone()
+    }
+
+    pub async fn trigger_optimizers(&self) {
+        self.shards_holder.read().await.trigger_optimizers().await;
+    }
+
+    /// Returns estimations of collection sizes. This values are cached and might be not 100% up to date.
+    /// The cache gets updated every 32 calls.
+    pub(crate) async fn estimated_collection_stats(&self) -> Option<&CollectionSizeAtomicStats> {
+        self.collection_stats_cache
+            .get_or_update_cache(|| Self::estimate_collection_size_stats(&self.shards_holder))
+            .await
+    }
+
+    async fn estimate_collection_size_stats(
+        shards_holder: &Arc<RwLock<ShardHolder>>,
+    ) -> Option<CollectionSizeStats> {
+        let shard_lock = shards_holder.read().await;
+        let result = shard_lock.estimate_collection_size_stats().await;
+        result
+    }
+
+    pub fn clean_local_shards_statuses(&self) -> HashMap<ShardId, clean::ShardCleanStatus> {
+        self.shard_clean_tasks.statuses()
+    }
+
     pub async fn get_telemetry_data(&self, detail: TelemetryDetail) -> CollectionTelemetry {
         let (shards_telemetry, transfers, resharding) = {
             if detail.level >= DetailsLevel::Level3 {
@@ -837,55 +950,6 @@ impl Collection {
             params: self.collection_config.read().await.params.clone(),
         }
     }
-
-    pub async fn effective_optimizers_config(&self) -> CollectionResult<OptimizersConfig> {
-        let config = self.collection_config.read().await;
-
-        if let Some(optimizers_overwrite) = self.optimizers_overwrite.clone() {
-            Ok(optimizers_overwrite.update(&config.optimizer_config)?)
-        } else {
-            Ok(config.optimizer_config.clone())
-        }
-    }
-
-    pub async fn lock_updates(&self) -> RwLockWriteGuard<()> {
-        self.updates_lock.write().await
-    }
-
-    pub fn wait_collection_initiated(&self, timeout: Duration) -> bool {
-        self.is_initialized.await_ready_for_timeout(timeout)
-    }
-
-    pub fn request_shard_transfer(&self, shard_transfer: ShardTransfer) {
-        self.request_shard_transfer_cb.deref()(shard_transfer)
-    }
-
-    pub fn snapshots_path(&self) -> &Path {
-        &self.snapshots_path
-    }
-
-    pub fn shards_holder(&self) -> Arc<LockedShardHolder> {
-        self.shards_holder.clone()
-    }
-
-    pub async fn trigger_optimizers(&self) {
-        self.shards_holder.read().await.trigger_optimizers().await;
-    }
-
-    async fn estimate_collection_size_stats(
-        shards_holder: &Arc<RwLock<ShardHolder>>,
-    ) -> Option<CollectionSizeStats> {
-        let shard_lock = shards_holder.read().await;
-        shard_lock.estimate_collection_size_stats().await
-    }
-
-    /// Returns estimations of collection sizes. This values are cached and might be not 100% up to date.
-    /// The cache gets updated every 32 calls.
-    pub(crate) async fn estimated_collection_stats(&self) -> Option<&CollectionSizeAtomicStats> {
-        self.collection_stats_cache
-            .get_or_update_cache(|| Self::estimate_collection_size_stats(&self.shards_holder))
-            .await
-    }
 }
 
 struct CollectionVersion;
