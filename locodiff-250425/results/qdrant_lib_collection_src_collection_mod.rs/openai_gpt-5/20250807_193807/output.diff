
index 71e15f9b2..dabb5ff5a 100644
--- a/qdrant_lib_collection_src_collection_mod.rs_expectedoutput.txt (expected):tmp/tmpgm81hvz6_expected.txt	
+++ b/qdrant_lib_collection_src_collection_mod.rs_extracted.txt (actual):tmp/tmph1eql5rl_actual.txt	
@@ -5,7 +5,6 @@ mod facet;
 pub mod payload_index_schema;
 mod point_ops;
 pub mod query;
-mod resharding;
 mod search;
 mod shard_transfer;
 mod sharding_keys;
@@ -46,6 +45,7 @@ use crate::shards::replica_set::ReplicaState::{Active, Dead, Initializing, Liste
 use crate::shards::replica_set::{
     ChangePeerFromState, ChangePeerState, ReplicaState, ShardReplicaSet,
 };
+use crate::shards::resharding::{ReshardKey, ReshardingState};
 use crate::shards::shard::{PeerId, ShardId};
 use crate::shards::shard_holder::shard_mapping::ShardKeyMapping;
 use crate::shards::shard_holder::{LockedShardHolder, ShardHolder, shard_not_found_error};
@@ -124,6 +124,8 @@ impl Collection {
 
         let payload_index_schema = Arc::new(Self::load_payload_index_schema(path)?);
 
+        let mut shard_holder_inner = shard_holder;
+
         let shared_collection_config = Arc::new(RwLock::new(collection_config.clone()));
         for (shard_id, mut peers) in shard_distribution.shards {
             let is_local = peers.remove(&this_peer_id);
@@ -159,10 +161,10 @@ impl Collection {
             )
             .await?;
 
-            shard_holder.add_shard(shard_id, replica_set, shard_key)?;
+            shard_holder_inner.add_shard(shard_id, replica_set, shard_key)?;
         }
 
-        let locked_shard_holder = Arc::new(LockedShardHolder::new(shard_holder));
+        let locked_shard_holder = Arc::new(LockedShardHolder::new(shard_holder_inner));
 
         let collection_stats_cache = CollectionSizeStatsCache::new_with_values(
             Self::estimate_collection_size_stats(&locked_shard_holder).await,
@@ -640,13 +642,9 @@ impl Collection {
             }
         }
 
-        // Count how many transfers we are now proposing
-        // We must track this here so we can reference it when checking for tranfser limits,
-        // because transfers we propose now will not be in the consensus state within the lifetime
-        // of this function
+        // Check for proper replica states
         let mut proposed = HashMap::<PeerId, usize>::new();
 
-        // Check for proper replica states
         for replica_set in shard_holder.all_shards() {
             let this_peer_id = replica_set.this_peer_id();
             let shard_id = replica_set.shard_id;
@@ -774,6 +772,45 @@ impl Collection {
         Ok(())
     }
 
+    pub async fn resharding_state(&self) -> Option<ReshardingState> {
+        self.shards_holder
+            .read()
+            .await
+            .resharding_state
+            .read()
+            .clone()
+    }
+
+    pub async fn start_resharding(&self, reshard: ReshardKey) -> CollectionResult<()> {
+        let mut shard_holder = self.shards_holder.write().await;
+
+        shard_holder.check_start_resharding(&reshard)?;
+
+        let replica_set = self
+            .create_replica_set(
+                reshard.shard_id,
+                &[reshard.peer_id],
+                Some(ReplicaState::Resharding),
+            )
+            .await?;
+
+        shard_holder.start_resharding_unchecked(reshard, replica_set)?;
+
+        Ok(())
+    }
+
+    pub async fn abort_resharding(
+        &self,
+        reshard: ReshardKey,
+        force_delete_points: bool,
+    ) -> CollectionResult<()> {
+        self.shards_holder
+            .write()
+            .await
+            .abort_resharding(reshard, force_delete_points)
+            .await
+    }
+
     pub async fn get_telemetry_data(&self, detail: TelemetryDetail) -> CollectionTelemetry {
         let (shards_telemetry, transfers, resharding) = {
             if detail.level >= DetailsLevel::Level3 {
