
index c9341cb38..23e1193ca 100644
--- a/qdrant_lib_collection_src_shards_local_shard_mod.rs_expectedoutput.txt (expected):tmp/tmp67b40k6s_expected.txt	
+++ b/qdrant_lib_collection_src_shards_local_shard_mod.rs_extracted.txt (actual):tmp/tmpzvusmvf4_actual.txt	
@@ -313,6 +313,7 @@ impl LocalShard {
         // Uncomment it if you need to debug segment loading.
         // let semaphore = Arc::new(parking_lot::Mutex::new(()));
 
+//         for entry in segment_dirs {
         for segment_path in segment_paths {
             let payload_index_schema = payload_index_schema.clone();
             // let semaphore_clone = semaphore.clone();
@@ -698,7 +699,6 @@ impl LocalShard {
                     }
                 }
             }
-
             // Force a flush after re-applying WAL operations, to ensure we maintain on-disk data
             // consistency, if we happened to only apply *past* operations to a segment with newer
             // version.
@@ -720,35 +720,6 @@ impl LocalShard {
         Ok(())
     }
 
-    /// Check data consistency for all segments
-    ///
-    /// Returns an error at the first inconsistent segment
-    pub fn check_data_consistency(&self) -> CollectionResult<()> {
-        log::info!("Checking data consistency for shard {:?}", self.path);
-        let segments = self.segments.read();
-        for (_idx, segment) in segments.iter() {
-            match segment {
-                LockedSegment::Original(raw_segment) => {
-                    let segment_guard = raw_segment.read();
-                    if let Err(err) = segment_guard.check_data_consistency() {
-                        log::error!(
-                            "Segment {:?} is inconsistent: {}",
-                            segment_guard.current_path,
-                            err
-                        );
-                        return Err(err.into());
-                    }
-                }
-                LockedSegment::Proxy(_) => {
-                    return Err(CollectionError::service_error(
-                        "Proxy segment found in check_data_consistency",
-                    ));
-                }
-            }
-        }
-        Ok(())
-    }
-
     pub async fn on_optimizer_config_update(&self) -> CollectionResult<()> {
         let config = self.collection_config.read().await;
         let mut update_handler = self.update_handler.lock().await;
@@ -868,6 +839,9 @@ impl LocalShard {
             rx.await?;
         }
 
+        let collection_path = self.path.parent().map(Path::to_path_buf).ok_or_else(|| {
+            CollectionError::service_error("Failed to determine collection path for shard")
+        })?;
         let segments_path = Self::segments_path(&self.path);
         let collection_params = self.collection_config.read().await.params.clone();
         let temp_path = temp_path.to_owned();
@@ -1017,6 +991,81 @@ impl LocalShard {
         SegmentsSearcher::read_filtered(segments, filter, runtime_handle, hw_counter).await
     }
 
+    /// Check data consistency for all segments
+    ///
+    /// Returns an error at the first inconsistent segment
+    pub fn check_data_consistency(&self) -> CollectionResult<()> {
+        log::info!("Checking data consistency for shard {:?}", self.path);
+        let segments = self.segments.read();
+        for (_idx, segment) in segments.iter() {
+            match segment {
+                LockedSegment::Original(raw_segment) => {
+                    let segment_guard = raw_segment.read();
+                    if let Err(err) = segment_guard.check_data_consistency() {
+                        log::error!(
+                            "Segment {:?} is inconsistent: {}",
+                            segment_guard.current_path,
+                            err
+                        );
+                        return Err(err.into());
+                    }
+                }
+                LockedSegment::Proxy(_) => {
+                    return Err(CollectionError::service_error(
+                        "Proxy segment found in check_data_consistency",
+                    ));
+                }
+            }
+        }
+        Ok(())
+    }
+
+    /// Get the recovery point for the current shard
+    ///
+    /// This is sourced from the last seen clocks from other nodes that we know about.
+    pub async fn recovery_point(&self) -> RecoveryPoint {
+        self.wal.recovery_point().await
+    }
+
+    /// Update the cutoff point on the current shard
+    ///
+    /// This also updates the highest seen clocks.
+    pub async fn update_cutoff(&self, cutoff: &RecoveryPoint) {
+        self.wal.update_cutoff(cutoff).await
+    }
+
+    /// Check if the read rate limiter allows the operation to proceed
+    /// - hw_measurement_acc: the current hardware measurement accumulator
+    /// - context: the context of the operation to add on the error message
+    /// - cost_fn: the cost of the operation called lazily
+    ///
+    /// Returns an error if the rate limit is exceeded.
+    fn check_read_rate_limiter<F>(
+        &self,
+        hw_measurement_acc: &HwMeasurementAcc,
+        context: &str,
+        cost_fn: F,
+    ) -> CollectionResult<()>
+    where
+        F: FnOnce() -> usize,
+    {
+        // Do not rate limit internal operation tagged with disposable measurement
+        if hw_measurement_acc.is_disposable() {
+            return Ok(());
+        }
+        if let Some(rate_limiter) = &self.read_rate_limiter {
+            let cost = cost_fn();
+            rate_limiter
+                .lock()
+                .try_consume(cost as f64)
+                .map_err(|err| {
+                    log::debug!("Read rate limit error on {context} with {err:?}");
+                    CollectionError::rate_limit_error(err, cost, false)
+                })?;
+        }
+        Ok(())
+    }
+
     pub async fn local_shard_status(&self) -> (ShardStatus, OptimizersStatus) {
         {
             let segments = self.segments().read();
@@ -1108,52 +1157,6 @@ impl LocalShard {
     pub fn update_tracker(&self) -> &UpdateTracker {
         &self.update_tracker
     }
-
-    /// Get the recovery point for the current shard
-    ///
-    /// This is sourced from the last seen clocks from other nodes that we know about.
-    pub async fn recovery_point(&self) -> RecoveryPoint {
-        self.wal.recovery_point().await
-    }
-
-    /// Update the cutoff point on the current shard
-    ///
-    /// This also updates the highest seen clocks.
-    pub async fn update_cutoff(&self, cutoff: &RecoveryPoint) {
-        self.wal.update_cutoff(cutoff).await
-    }
-
-    /// Check if the read rate limiter allows the operation to proceed
-    /// - hw_measurement_acc: the current hardware measurement accumulator
-    /// - context: the context of the operation to add on the error message
-    /// - cost_fn: the cost of the operation called lazily
-    ///
-    /// Returns an error if the rate limit is exceeded.
-    fn check_read_rate_limiter<F>(
-        &self,
-        hw_measurement_acc: &HwMeasurementAcc,
-        context: &str,
-        cost_fn: F,
-    ) -> CollectionResult<()>
-    where
-        F: FnOnce() -> usize,
-    {
-        // Do not rate limit internal operation tagged with disposable measurement
-        if hw_measurement_acc.is_disposable() {
-            return Ok(());
-        }
-        if let Some(rate_limiter) = &self.read_rate_limiter {
-            let cost = cost_fn();
-            rate_limiter
-                .lock()
-                .try_consume(cost as f64)
-                .map_err(|err| {
-                    log::debug!("Read rate limit error on {context} with {err:?}");
-                    CollectionError::rate_limit_error(err, cost, false)
-                })?;
-        }
-        Ok(())
-    }
 }
 
 impl Drop for LocalShard {
