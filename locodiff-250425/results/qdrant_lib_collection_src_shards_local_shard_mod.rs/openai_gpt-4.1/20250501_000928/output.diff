
index c9341cb3..a31d8f7f 100644
--- a/qdrant_lib_collection_src_shards_local_shard_mod.rs_expectedoutput.txt (expected):tmp/tmp26imzjmg_expected.txt	
+++ b/qdrant_lib_collection_src_shards_local_shard_mod.rs_extracted.txt (actual):tmp/tmpmrx2am3s_actual.txt	
@@ -52,12 +52,12 @@ use crate::collection_manager::optimizers::TrackerLog;
 use crate::collection_manager::segments_searcher::SegmentsSearcher;
 use crate::common::file_utils::{move_dir, move_file};
 use crate::config::CollectionConfigInternal;
-use crate::operations::OperationWithClockTag;
 use crate::operations::shared_storage_config::SharedStorageConfig;
 use crate::operations::types::{
     CollectionError, CollectionResult, OptimizersStatus, ShardInfoInternal, ShardStatus,
     check_sparse_compatible_with_segment_config,
 };
+use crate::operations::OperationWithClockTag;
 use crate::optimizers_builder::{OptimizersConfig, build_optimizers, clear_temp_segments};
 use crate::save_on_disk::SaveOnDisk;
 use crate::shards::CollectionId;
@@ -102,7 +102,6 @@ pub struct LocalShard {
     read_rate_limiter: Option<ParkingMutex<RateLimiter>>,
 }
 
-/// Shard holds information about segments and WAL.
 impl LocalShard {
     /// Moves `wal`, `segments` and `clocks` data from one path to another.
     pub async fn move_data(from: &Path, to: &Path) -> CollectionResult<()> {
@@ -173,16 +172,6 @@ impl LocalShard {
         let optimizers_log = Arc::new(ParkingMutex::new(Default::default()));
         let total_optimized_points = Arc::new(AtomicUsize::new(0));
 
-        // default to 2x the WAL capacity
-        let disk_buffer_threshold_mb =
-            2 * (collection_config.read().await.wal_config.wal_capacity_mb);
-
-        let disk_usage_watcher = disk_usage_watcher::DiskUsageWatcher::new(
-            shard_path.to_owned(),
-            disk_buffer_threshold_mb,
-        )
-        .await;
-
         let mut update_handler = UpdateHandler::new(
             shared_storage_config.clone(),
             payload_index_schema.clone(),
@@ -197,6 +186,8 @@ impl LocalShard {
             config.optimizer_config.max_optimization_threads,
             clocks.clone(),
             shard_path.into(),
+            search_runtime.clone(),
+            None,
         );
 
         let (update_sender, update_receiver) =
@@ -238,7 +229,18 @@ impl LocalShard {
         self.segments.deref()
     }
 
-    /// Recovers shard from disk.
+    pub fn shard_path(&self) -> PathBuf {
+        self.path.clone()
+    }
+
+    pub fn wal_path(shard_path: &Path) -> PathBuf {
+        shard_path.join(WAL_PATH)
+    }
+
+    pub fn segments_path(shard_path: &Path) -> PathBuf {
+        shard_path.join(SEGMENTS_PATH)
+    }
+
     #[allow(clippy::too_many_arguments)]
     pub async fn load(
         id: ShardId,
@@ -432,19 +434,6 @@ impl LocalShard {
         Ok(local_shard)
     }
 
-    pub fn shard_path(&self) -> PathBuf {
-        self.path.clone()
-    }
-
-    pub fn wal_path(shard_path: &Path) -> PathBuf {
-        shard_path.join(WAL_PATH)
-    }
-
-    pub fn segments_path(shard_path: &Path) -> PathBuf {
-        shard_path.join(SEGMENTS_PATH)
-    }
-
-    #[allow(clippy::too_many_arguments)]
     pub async fn build_local(
         id: ShardId,
         collection_id: CollectionId,
@@ -459,7 +448,7 @@ impl LocalShard {
     ) -> CollectionResult<LocalShard> {
         // initialize local shard config file
         let local_shard_config = ShardConfig::new_replica_set();
-        let shard = Self::build(
+        let local_shard = Self::build(
             id,
             collection_id,
             shard_path,
@@ -473,7 +462,7 @@ impl LocalShard {
         )
         .await?;
         local_shard_config.save(shard_path)?;
-        Ok(shard)
+        Ok(local_shard)
     }
 
     /// Creates new empty shard with given configuration, initializing all storages, optimizers and directories.
@@ -560,7 +549,7 @@ impl LocalShard {
 
         drop(config); // release `shared_config` from borrow checker
 
-        let collection = LocalShard::new(
+        let local_shard = LocalShard::new(
             segment_holder,
             collection_config,
             shared_storage_config,
@@ -575,7 +564,7 @@ impl LocalShard {
         )
         .await;
 
-        Ok(collection)
+        Ok(local_shard)
     }
 
     pub async fn stop_flush_worker(&self) {
@@ -778,27 +767,6 @@ impl LocalShard {
         Ok(())
     }
 
-    /// Apply shard's strict mode configuration update
-    /// - Update read rate limiter
-    pub async fn on_strict_mode_config_update(&mut self) {
-        let config = self.collection_config.read().await;
-
-        if let Some(strict_mode_config) = &config.strict_mode_config {
-            if strict_mode_config.enabled == Some(true) {
-                // update read rate limiter
-                if let Some(read_rate_limit_per_min) = strict_mode_config.read_rate_limit {
-                    let new_read_rate_limiter =
-                        RateLimiter::new_per_minute(read_rate_limit_per_min);
-                    self.read_rate_limiter
-                        .replace(parking_lot::Mutex::new(new_read_rate_limiter));
-                    return;
-                }
-            }
-        }
-        // remove read rate limiter for all other situations
-        self.read_rate_limiter.take();
-    }
-
     pub fn trigger_optimizers(&self) {
         // Send a trigger signal and ignore errors because all error cases are acceptable:
         // - If receiver is already dead - we do not care
@@ -868,7 +836,6 @@ impl LocalShard {
             rx.await?;
         }
 
-        let segments_path = Self::segments_path(&self.path);
         let collection_params = self.collection_config.read().await.params.clone();
         let temp_path = temp_path.to_owned();
         let payload_index_schema = self.payload_index_schema.clone();
@@ -878,7 +845,7 @@ impl LocalShard {
             // Do not change segments while snapshotting
             SegmentHolder::snapshot_all_segments(
                 segments.clone(),
-                &segments_path,
+                &LocalShard::segments_path(&segments.read().collection_path),
                 Some(&collection_params),
                 &payload_index_schema.read().clone(),
                 &temp_path,
@@ -982,9 +949,9 @@ impl LocalShard {
             .map_err(CollectionError::from)
     }
 
-    pub fn estimate_cardinality<'a>(
-        &'a self,
-        filter: Option<&'a Filter>,
+    pub fn estimate_cardinality(
+        &self,
+        filter: Option<&Filter>,
         hw_counter: &HardwareCounterCell,
     ) -> CollectionResult<CardinalityEstimation> {
         let segments = self.segments().read();
@@ -1007,9 +974,9 @@ impl LocalShard {
         Ok(cardinality)
     }
 
-    pub async fn read_filtered<'a>(
-        &'a self,
-        filter: Option<&'a Filter>,
+    pub async fn read_filtered(
+        &self,
+        filter: Option<&Filter>,
         runtime_handle: &Handle,
         hw_counter: HwMeasurementAcc,
     ) -> CollectionResult<BTreeSet<PointIdType>> {
@@ -1109,9 +1076,6 @@ impl LocalShard {
         &self.update_tracker
     }
 
-    /// Get the recovery point for the current shard
-    ///
-    /// This is sourced from the last seen clocks from other nodes that we know about.
     pub async fn recovery_point(&self) -> RecoveryPoint {
         self.wal.recovery_point().await
     }
@@ -1244,7 +1208,6 @@ impl LocalShardClocks {
             let oldest_clocks_to = Self::oldest_clocks_path(to);
             move_file(oldest_clocks_from, oldest_clocks_to).await?;
         }
-
         Ok(())
     }
 
@@ -1260,7 +1223,6 @@ impl LocalShardClocks {
         if oldest_clocks_path.exists() {
             remove_file(oldest_clocks_path).await?;
         }
-
         Ok(())
     }
 
