
index c9341cb38..be83fdb43 100644
--- a/qdrant_lib_collection_src_shards_local_shard_mod.rs_expectedoutput.txt (expected):tmp/tmpqqms7p4i_expected.txt	
+++ b/qdrant_lib_collection_src_shards_local_shard_mod.rs_extracted.txt (actual):tmp/tmp8zt7fw78_actual.txt	
@@ -26,6 +26,7 @@ use indicatif::{ProgressBar, ProgressStyle};
 use itertools::Itertools;
 use parking_lot::{Mutex as ParkingMutex, RwLock};
 use segment::data_types::segment_manifest::SegmentManifests;
+use segment::data_types::vectors::VectorElementType;
 use segment::entry::entry_point::SegmentEntry as _;
 use segment::index::field_index::CardinalityEstimation;
 use segment::segment::Segment;
@@ -34,6 +35,7 @@ use segment::types::{
     Filter, PayloadIndexInfo, PayloadKeyType, PointIdType, SegmentConfig, SegmentType,
     SnapshotFormat,
 };
+use segment::vector_storage::common::get_async_scorer;
 use tokio::fs::{create_dir_all, remove_dir_all, remove_file};
 use tokio::runtime::Handle;
 use tokio::sync::mpsc::Sender;
@@ -772,7 +774,6 @@ impl LocalShard {
         update_handler.flush_interval_sec = config.optimizer_config.flush_interval_sec;
         update_handler.max_optimization_threads = config.optimizer_config.max_optimization_threads;
         update_handler.run_workers(update_receiver);
-
         self.update_sender.load().send(UpdateSignal::Nop).await?;
 
         Ok(())
@@ -900,6 +901,13 @@ impl LocalShard {
         Ok(())
     }
 
+    pub fn segment_manifests(&self) -> CollectionResult<SegmentManifests> {
+        self.segments()
+            .read()
+            .segment_manifests()
+            .map_err(CollectionError::from)
+    }
+
     /// Create empty WAL which is compatible with currently stored data
     ///
     /// # Panics
@@ -975,13 +983,6 @@ impl LocalShard {
         Ok(())
     }
 
-    pub fn segment_manifests(&self) -> CollectionResult<SegmentManifests> {
-        self.segments()
-            .read()
-            .segment_manifests()
-            .map_err(CollectionError::from)
-    }
-
     pub fn estimate_cardinality<'a>(
         &'a self,
         filter: Option<&'a Filter>,
@@ -1009,266 +1010,4 @@ impl LocalShard {
 
     pub async fn read_filtered<'a>(
         &'a self,
-        filter: Option<&'a Filter>,
-        runtime_handle: &Handle,
-        hw_counter: HwMeasurementAcc,
-    ) -> CollectionResult<BTreeSet<PointIdType>> {
-        let segments = self.segments.clone();
-        SegmentsSearcher::read_filtered(segments, filter, runtime_handle, hw_counter).await
-    }
-
-    pub async fn local_shard_status(&self) -> (ShardStatus, OptimizersStatus) {
-        {
-            let segments = self.segments().read();
-
-            // Red status on failed operation or optimizer error
-            if !segments.failed_operation.is_empty() || segments.optimizer_errors.is_some() {
-                let optimizer_status = segments
-                    .optimizer_errors
-                    .as_ref()
-                    .map_or(OptimizersStatus::Ok, |err| {
-                        OptimizersStatus::Error(err.to_string())
-                    });
-                return (ShardStatus::Red, optimizer_status);
-            }
-
-            // Yellow status if we have a special segment, indicates a proxy segment used during optimization
-            // TODO: snapshotting also creates temp proxy segments. should differentiate.
-            let has_special_segment = segments
-                .iter()
-                .map(|(_, segment)| segment.get().read().info().segment_type)
-                .any(|segment_type| segment_type == SegmentType::Special);
-            if has_special_segment {
-                return (ShardStatus::Yellow, OptimizersStatus::Ok);
-            }
-        }
-
-        // Yellow or grey status if there are pending optimizations
-        // Grey if optimizers were not triggered yet after restart,
-        // we don't automatically trigger them to prevent a crash loop
-        let (has_triggered_any_optimizers, has_suboptimal_optimizers) = self
-            .update_handler
-            .lock()
-            .await
-            .check_optimizer_conditions();
-        if has_suboptimal_optimizers {
-            let status = if has_triggered_any_optimizers {
-                ShardStatus::Yellow
-            } else {
-                ShardStatus::Grey
-            };
-            return (status, OptimizersStatus::Ok);
-        }
-
-        // Green status because everything is fine
-        (ShardStatus::Green, OptimizersStatus::Ok)
-    }
-
-    pub async fn local_shard_info(&self) -> ShardInfoInternal {
-        let collection_config = self.collection_config.read().await.clone();
-        let mut vectors_count = 0;
-        let mut indexed_vectors_count = 0;
-        let mut points_count = 0;
-        let mut segments_count = 0;
-        let mut schema: HashMap<PayloadKeyType, PayloadIndexInfo> = Default::default();
-
-        {
-            let segments = self.segments().read();
-            for (_idx, segment) in segments.iter() {
-                segments_count += 1;
-
-                let segment_info = segment.get().read().info();
-
-                vectors_count += segment_info.num_vectors;
-                indexed_vectors_count += segment_info.num_indexed_vectors;
-                points_count += segment_info.num_points;
-                for (key, val) in segment_info.index_schema {
-                    schema
-                        .entry(key)
-                        .and_modify(|entry| entry.points += val.points)
-                        .or_insert(val);
-                }
-            }
-        }
-
-        let (status, optimizer_status) = self.local_shard_status().await;
-
-        ShardInfoInternal {
-            status,
-            optimizer_status,
-            vectors_count,
-            indexed_vectors_count,
-            points_count,
-            segments_count,
-            config: collection_config,
-            payload_schema: schema,
-        }
-    }
-
-    pub fn update_tracker(&self) -> &UpdateTracker {
-        &self.update_tracker
-    }
-
-    /// Get the recovery point for the current shard
-    ///
-    /// This is sourced from the last seen clocks from other nodes that we know about.
-    pub async fn recovery_point(&self) -> RecoveryPoint {
-        self.wal.recovery_point().await
-    }
-
-    /// Update the cutoff point on the current shard
-    ///
-    /// This also updates the highest seen clocks.
-    pub async fn update_cutoff(&self, cutoff: &RecoveryPoint) {
-        self.wal.update_cutoff(cutoff).await
-    }
-
-    /// Check if the read rate limiter allows the operation to proceed
-    /// - hw_measurement_acc: the current hardware measurement accumulator
-    /// - context: the context of the operation to add on the error message
-    /// - cost_fn: the cost of the operation called lazily
-    ///
-    /// Returns an error if the rate limit is exceeded.
-    fn check_read_rate_limiter<F>(
-        &self,
-        hw_measurement_acc: &HwMeasurementAcc,
-        context: &str,
-        cost_fn: F,
-    ) -> CollectionResult<()>
-    where
-        F: FnOnce() -> usize,
-    {
-        // Do not rate limit internal operation tagged with disposable measurement
-        if hw_measurement_acc.is_disposable() {
-            return Ok(());
-        }
-        if let Some(rate_limiter) = &self.read_rate_limiter {
-            let cost = cost_fn();
-            rate_limiter
-                .lock()
-                .try_consume(cost as f64)
-                .map_err(|err| {
-                    log::debug!("Read rate limit error on {context} with {err:?}");
-                    CollectionError::rate_limit_error(err, cost, false)
-                })?;
-        }
-        Ok(())
-    }
-}
-
-impl Drop for LocalShard {
-    fn drop(&mut self) {
-        thread::scope(|s| {
-            let handle = thread::Builder::new()
-                .name("drop-shard".to_string())
-                .spawn_scoped(s, || {
-                    // Needs dedicated thread to avoid `Cannot start a runtime from within a runtime` error.
-                    self.update_runtime
-                        .block_on(async { self.stop_gracefully().await })
-                });
-            handle.expect("Failed to create thread for shard drop");
-        })
-    }
-}
-
-/// Convenience struct for combining clock maps belonging to a shard
-///
-/// Holds a clock map for tracking the highest clocks and the cutoff clocks.
-#[derive(Clone, Debug, Default)]
-pub struct LocalShardClocks {
-    newest_clocks: Arc<Mutex<ClockMap>>,
-    oldest_clocks: Arc<Mutex<ClockMap>>,
-}
-
-impl LocalShardClocks {
-    fn new(newest_clocks: ClockMap, oldest_clocks: ClockMap) -> Self {
-        Self {
-            newest_clocks: Arc::new(Mutex::new(newest_clocks)),
-            oldest_clocks: Arc::new(Mutex::new(oldest_clocks)),
-        }
-    }
-
-    // Load clock maps from disk
-    pub fn load(shard_path: &Path) -> CollectionResult<Self> {
-        let newest_clocks = ClockMap::load_or_default(&Self::newest_clocks_path(shard_path))?;
-
-        let oldest_clocks = ClockMap::load_or_default(&Self::oldest_clocks_path(shard_path))?;
-
-        Ok(Self::new(newest_clocks, oldest_clocks))
-    }
-
-    /// Persist clock maps to disk
-    pub async fn store_if_changed(&self, shard_path: &Path) -> CollectionResult<()> {
-        self.oldest_clocks
-            .lock()
-            .await
-            .store_if_changed(&Self::oldest_clocks_path(shard_path))?;
-
-        self.newest_clocks
-            .lock()
-            .await
-            .store_if_changed(&Self::newest_clocks_path(shard_path))?;
-
-        Ok(())
-    }
-
-    /// Put clock data from the disk into an archive.
-    pub async fn archive_data(from: &Path, tar: &tar_ext::BuilderExt) -> CollectionResult<()> {
-        let newest_clocks_from = Self::newest_clocks_path(from);
-        let oldest_clocks_from = Self::oldest_clocks_path(from);
-
-        if newest_clocks_from.exists() {
-            tar.append_file(&newest_clocks_from, Path::new(NEWEST_CLOCKS_PATH))
-                .await?;
-        }
-
-        if oldest_clocks_from.exists() {
-            tar.append_file(&oldest_clocks_from, Path::new(OLDEST_CLOCKS_PATH))
-                .await?;
-        }
-
-        Ok(())
-    }
-
-    /// Move clock data on disk from one shard path to another.
-    pub async fn move_data(from: &Path, to: &Path) -> CollectionResult<()> {
-        let newest_clocks_from = Self::newest_clocks_path(from);
-        let oldest_clocks_from = Self::oldest_clocks_path(from);
-
-        if newest_clocks_from.exists() {
-            let newest_clocks_to = Self::newest_clocks_path(to);
-            move_file(newest_clocks_from, newest_clocks_to).await?;
-        }
-
-        if oldest_clocks_from.exists() {
-            let oldest_clocks_to = Self::oldest_clocks_path(to);
-            move_file(oldest_clocks_from, oldest_clocks_to).await?;
-        }
-
-        Ok(())
-    }
-
-    /// Delete clock data from disk at the given shard path.
-    pub async fn delete_data(shard_path: &Path) -> CollectionResult<()> {
-        let newest_clocks_path = Self::newest_clocks_path(shard_path);
-        let oldest_clocks_path = Self::oldest_clocks_path(shard_path);
-
-        if newest_clocks_path.exists() {
-            remove_file(newest_clocks_path).await?;
-        }
-
-        if oldest_clocks_path.exists() {
-            remove_file(oldest_clocks_path).await?;
-        }
-
-        Ok(())
-    }
-
-    fn newest_clocks_path(shard_path: &Path) -> PathBuf {
-        shard_path.join(NEWEST_CLOCKS_PATH)
-    }
-
-    fn oldest_clocks_path(shard_path: &Path) -> PathBuf {
-        shard_path.join(OLDEST_CLOCKS_PATH)
-    }
-}
\ No newline at end of file
+        filter: Option
\ No newline at end of file
