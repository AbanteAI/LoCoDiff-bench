
index 237415b4..e720dbbe 100644
--- a/qdrant_lib_collection_src_collection_manager_optimizers_indexing_optimizer.rs_expectedoutput.txt (expected):tmp/tmpure2h7ox_expected.txt	
+++ b/qdrant_lib_collection_src_collection_manager_optimizers_indexing_optimizer.rs_extracted.txt (actual):tmp/tmpkge9jg99_actual.txt	
@@ -1,20 +1,29 @@
-use std::collections::HashSet;
-use std::path::{Path, PathBuf};
-use std::sync::Arc;
-
-use parking_lot::Mutex;
-use segment::common::operation_time_statistics::OperationDurationsAggregator;
-use segment::types::{HnswConfig, QuantizationConfig, SegmentType};
-
 use crate::collection_manager::holders::segment_holder::{
-    LockedSegmentHolder, SegmentHolder, SegmentId,
+    LockedSegment, LockedSegmentHolder, SegmentId, SegmentHolder,
 };
 use crate::collection_manager::optimizers::segment_optimizer::{
     OptimizerThresholds, SegmentOptimizer,
 };
 use crate::config::CollectionParams;
+use common::budget::ResourceBudget;
+use common::counter::hardware_counter::HardwareCounter;
+use common::types::TelemetryDetail;
+use parking_lot::Mutex;
+use rand::rng;
+use segment::common::operation_time_statistics::OperationDurationsAggregator;
+use segment::entry::entry_point::SegmentEntry;
+use segment::index::hnsw_index::num_rayon_threads;
+use segment::segment::SegmentShared;
+use segment::segment_constructor::build_segment;
+use segment::types::{HnswConfig, QuantizationConfig, SegmentType};
+use segment::{Segment, SegmentState};
+use std::collections::HashSet;
+use std::path::{Path, PathBuf};
+use std::sync::atomic::AtomicBool;
+use std::sync::Arc;
 
 const BYTES_IN_KB: usize = 1024;
+const INDEXING_OPTIMIZER_NAME: &str = "indexing";
 
 /// Looks for the segments, which require to be indexed.
 ///
@@ -81,9 +90,9 @@ impl IndexingOptimizer {
                     return None;
                 }
 
-                Some((idx, vector_size))
+                Some((*idx, vector_size))
             })
-            .min_by_key(|(_, vector_size_bytes)| *vector_size_bytes)
+            .min_by_key(|(_, vector_size)| *vector_size)
             .map(|(idx, size)| (*idx, size))
     }
 
@@ -100,6 +109,7 @@ impl IndexingOptimizer {
             .filter_map(|(idx, segment)| {
                 let segment_entry = segment.get();
                 let read_segment = segment_entry.read();
+                let point_count = read_segment.available_point_count();
                 let max_vector_size_bytes = read_segment
                     .max_available_vectors_size_in_bytes()
                     .unwrap_or_default();
@@ -110,6 +120,10 @@ impl IndexingOptimizer {
                     return None; // Never optimize already optimized segment
                 }
 
+                // Apply indexing to plain segments which have grown too big
+                let are_all_vectors_indexed = segment_config.are_all_vectors_indexed();
+                let is_any_on_disk = segment_config.is_any_on_disk();
+
                 let indexing_threshold_bytes = self
                     .thresholds_config
                     .indexing_threshold_kb
@@ -188,6 +202,7 @@ impl IndexingOptimizer {
         }
         let (selected_segment_id, selected_segment_size) = *selected_segment.unwrap();
 
+        let segments_read_guard = segments.read();
         let number_of_segments = segments_read_guard.len();
 
         // If the number of segments if equal or bigger than the default_segments_number
@@ -233,11 +248,48 @@ impl IndexingOptimizer {
 
         vec![selected_segment_id]
     }
+
+    fn build_indexing_segment(
+        &self,
+        segments: &[SegmentShared],
+        id_tracker: Arc<parking_lot::RwLock<id_tracker::simple_id_tracker::SimpleIdTracker>>,
+        stopped: &AtomicBool,
+        resource_budget: ResourceBudget,
+        telemetry_hardware_counter: &HardwareCounter,
+    ) -> Result<SegmentShared, anyhow::Error> {
+        let optimized_segment_id = segments
+            .first()
+            .and_then(|s| s.read().id())
+            .unwrap_or_else(|| 0.into());
+        let segment_temp_dir = self
+            .collection_temp_dir
+            .join(format!("optimise_{}", optimized_segment_id));
+
+        std::fs::create_dir_all(&segment_temp_dir)?;
+
+        let temp_segment = build_segment(
+            segments,
+            &segment_temp_dir,
+            id_tracker,
+            &self.collection_params,
+            self.hnsw_config(),
+            self.quantization_config(),
+            stopped,
+            resource_budget,
+            telemetry_hardware_counter,
+        )?;
+
+        temp_segment.flush()?;
+
+        Ok(temp_segment)
+    }
 }
 
+use segment::id_tracker;
+
 impl SegmentOptimizer for IndexingOptimizer {
     fn name(&self) -> &str {
-        "indexing"
+        INDEXING_OPTIMIZER_NAME
     }
 
     fn segments_path(&self) -> &Path {
@@ -272,17 +324,114 @@ impl SegmentOptimizer for IndexingOptimizer {
         self.worst_segment(segments, excluded_ids)
     }
 
+    fn get_telemetry_data(&self, detail: TelemetryDetail) -> OperationDurationStatistics {
+        self.get_telemetry_counter().lock().get_statistics(detail)
+    }
+
     fn get_telemetry_counter(&self) -> &Mutex<OperationDurationsAggregator> {
         &self.telemetry_durations_aggregator
     }
+
+    fn optimize(
+        &self,
+        segments_holder: LockedSegmentHolder,
+        segments_to_optimize: Vec<SegmentId>,
+        resource_budget: ResourceBudget,
+        stopped: &AtomicBool,
+    ) -> Result<usize, anyhow::Error> {
+        let segments: Vec<SegmentShared> = segments_holder.read().values(|id| {
+            segments_to_optimize.contains(&id.to_owned().into())
+        });
+        if segments.is_empty() {
+            let segments_read_guard = segments_holder.read();
+            let number_of_segments = segments_read_guard.len();
+            let max_segment_size_bytes = self
+                .thresholds_config
+                .max_segment_size_kb
+                .saturating_mul(BYTES_IN_KB);
+            let total_segments_size_bytes: usize = segments_read_guard
+                .iter()
+                .map(|(_, segment)| {
+                    segment.get().read().available_vectors_size_in_bytes(None).unwrap_or_default()
+                })
+                .sum();
+
+            // Check if new appendable segment is required:
+            // If there are no unindexed large segments to optimize AND
+            // total number of segments is less than default AND
+            // all segments already exceed max_segment_size
+            if number_of_segments < self.default_segments_number && total_segments_size_bytes.saturating_div(number_of_segments) > max_segment_size_bytes {
+                let current_segments: Vec<_> = segments_read_guard
+                    .iter()
+                    .map(|(id, s)| (*id, s.get().read()))
+                    .collect();
+                log::debug!("creating new appendable segment: reason: no segments to optimize; \
+                             number_of_segments: {}; \
+                             default_segments_number: {}; \
+                             max_segment_size_bytes: {}; \
+                             total_segments_size_bytes: {};
+                             current_segments: {:#?};
+                             candidates: {:#?}",
+                             number_of_segments,
+                             self.default_segments_number,
+                             max_segment_size_bytes,
+                             total_segments_size_bytes,
+                             current_segments.iter().map(|(id, s)| (id, s.info())).collect::<Vec<_>>(),
+                             self.worst_segment(segments_holder.clone(), &Default::default()),
+                );
+
+                let new_segment_id = segments_read_guard.generate_new_segment_id();
+                let new_segment = build_segment(
+                    &[],
+                    self.segments_path(),
+                    segments_read_guard.id_tracker().clone(),
+                    &self.collection_params,
+                    self.hnsw_config(),
+                    self.quantization_config(),
+                    stopped,
+                    resource_budget,
+                    &HardwareCounter::default() // No need for hardware counter for creating an empty segment
+                )?;
+                segments_read_guard.drop_replace_merge(segments_to_optimize.iter().copied(), [new_segment_id].iter().copied(), new_segment.into())?;
+                return Ok(0);
+            }
+
+            return Ok(0); // Nothing to optimize
+        }
+
+        let optimized_point_count = segments
+            .iter()
+            .map(|segment| segment.read().available_point_count())
+            .sum();
+
+        let _timer = self.get_telemetry_counter().lock().guard_operation();
+
+        let id_tracker = segments.first().unwrap().read().id_tracker().clone();
+
+        let new_segment = self.build_indexing_segment(
+            &segments,
+            id_tracker,
+            stopped,
+            resource_budget,
+            &HardwareCounter::default()
+        )?;
+
+        segments_holder.read().drop_replace_merge(
+            segments_to_optimize.iter().copied(),
+            [new_segment.read().id().unwrap()].iter().copied(),
+            new_segment.into(),
+        )?;
+
+        Ok(optimized_point_count)
+    }
 }
 
 #[cfg(test)]
 mod tests {
     use std::collections::BTreeMap;
     use std::ops::Deref;
-    use std::sync::Arc;
     use std::sync::atomic::AtomicBool;
+    use std::sync::Arc;
 
     use common::budget::ResourceBudget;
     use common::counter::hardware_counter::HardwareCounterCell;
@@ -295,7 +444,9 @@ mod tests {
     use segment::index::hnsw_index::num_rayon_threads;
     use segment::json_path::JsonPath;
     use segment::payload_json;
-    use segment::segment_constructor::simple_segment_constructor::{VECTOR1_NAME, VECTOR2_NAME};
+    use segment::segment_constructor::simple_segment_constructor::{
+        VECTOR1_NAME, VECTOR2_NAME,
+    };
     use segment::types::{Distance, PayloadSchemaType, VectorNameBuf};
     use tempfile::Builder;
 
@@ -591,10 +742,7 @@ mod tests {
             "Testing that 2 segments are actually indexed"
         );
 
-        let on_disk_count = configs
-            .iter()
-            .filter(|config| config.is_any_on_disk())
-            .count();
+        let on_disk_count = configs.iter().filter(|config| config.is_any_on_disk()).count();
         assert_eq!(
             on_disk_count, 1,
             "Testing that only largest segment is not Mmap"
@@ -718,6 +866,8 @@ mod tests {
         let insert_point_ops =
             PointOperations::UpsertPoints(PointInsertOperationsInternal::from(batch));
 
+        let hw_counter = HardwareCounterCell::new();
+
         process_point_operation(
             locked_holder.deref(),
             opnum.next().unwrap(),
@@ -938,6 +1088,9 @@ mod tests {
             .on_disk
             .take();
 
+        let permit_cpu_count = num_rayon_threads(0);
+        let budget = ResourceBudget::new(permit_cpu_count, permit_cpu_count);
+
         // Optimizers used in test
         let index_optimizer = IndexingOptimizer::new(
             2,
@@ -953,20 +1106,15 @@ mod tests {
             dir.path().to_owned(),
             temp_dir.path().to_owned(),
             collection_params,
-            hnsw_config,
+            hnsw_config.clone(),
             Default::default(),
         );
 
-        let permit_cpu_count = num_rayon_threads(0);
-        let budget = ResourceBudget::new(permit_cpu_count, permit_cpu_count);
-        let permit = budget.try_acquire(0, permit_cpu_count).unwrap();
-
         // Use indexing optimizer to build mmap
         let changed = index_optimizer
             .optimize(
                 locked_holder.clone(),
                 vec![segment_id],
-                permit,
                 budget.clone(),
                 &false.into(),
             )
