
index 237415b4..a1f0bc3f 100644
--- a/qdrant_lib_collection_src_collection_manager_optimizers_indexing_optimizer.rs_expectedoutput.txt (expected):tmp/tmpmum5f90b_expected.txt	
+++ b/qdrant_lib_collection_src_collection_manager_optimizers_indexing_optimizer.rs_extracted.txt (actual):tmp/tmpn9p1mq0y_actual.txt	
@@ -295,7 +295,6 @@ mod tests {
     use segment::index::hnsw_index::num_rayon_threads;
     use segment::json_path::JsonPath;
     use segment::payload_json;
-    use segment::segment_constructor::simple_segment_constructor::{VECTOR1_NAME, VECTOR2_NAME};
     use segment::types::{Distance, PayloadSchemaType, VectorNameBuf};
     use tempfile::Builder;
 
@@ -417,8 +416,8 @@ mod tests {
 
         for config in configs {
             assert_eq!(config.vector_data.len(), 2);
-            assert_eq!(config.vector_data.get(VECTOR1_NAME).unwrap().size, dim1);
-            assert_eq!(config.vector_data.get(VECTOR2_NAME).unwrap().size, dim2);
+            assert_eq!(config.vector_data.get(super::VECTOR1_NAME).unwrap().size, dim1);
+            assert_eq!(config.vector_data.get(super::VECTOR2_NAME).unwrap().size, dim2);
         }
     }
 
@@ -464,19 +463,15 @@ mod tests {
             segments_dir.path().to_owned(),
             segments_temp_dir.path().to_owned(),
             CollectionParams {
-                vectors: VectorsConfig::Single(
-                    VectorParamsBuilder::new(
-                        segment_config.vector_data[DEFAULT_VECTOR_NAME].size as u64,
-                        segment_config.vector_data[DEFAULT_VECTOR_NAME].distance,
-                    )
-                    .build(),
-                ),
+                vectors: VectorsConfig::Single(VectorParamsBuilder::new(
+                    segment_config.vector_data[DEFAULT_VECTOR_NAME].size as u64,
+                    segment_config.vector_data[DEFAULT_VECTOR_NAME].distance,
+                ).build()),
                 ..CollectionParams::empty()
             },
             Default::default(),
             Default::default(),
         );
-
         let locked_holder: Arc<RwLock<_, _>> = Arc::new(RwLock::new(holder));
 
         let excluded_ids = Default::default();
@@ -525,15 +520,137 @@ mod tests {
         )
         .unwrap();
 
+        let infos = locked_holder
+            .read()
+            .iter()
+            .map(|(_sid, segment)| segment.get().read().info())
+            .collect_vec();
+
+        for info in &infos {
+            assert!(
+                info.index_schema.contains_key(&payload_field),
+                "Testing that payload is not lost"
+            );
+            assert_eq!(
+                info.index_schema[&payload_field].data_type,
+                PayloadSchemaType::Integer,
+                "Testing that payload type is not lost"
+            );
+        }
+
+        let point_payload = payload_json! {"number": 10000i64};
+
+        let batch = BatchPersisted {
+            ids: vec![501.into(), 502.into(), 503.into()],
+            vectors: BatchVectorStructPersisted::Single(vec![
+                random_vector(&mut rng, dim),
+                random_vector(&mut rng, dim),
+                random_vector(&mut rng, dim),
+            ]),
+            payloads: Some(vec![
+                Some(point_payload.clone()),
+                Some(point_payload.clone()),
+                Some(point_payload),
+            ]),
+        };
+
+        let insert_point_ops =
+            PointOperations::UpsertPoints(PointInsertOperationsInternal::from(batch));
+
+        let smallest_size = infos
+            .iter()
+            .min_by_key(|info| info.num_vectors)
+            .unwrap()
+            .num_vectors;
+
+        let hw_counter = HardwareCounterCell::new();
+
+        process_point_operation(
+            locked_holder.deref(),
+            opnum.next().unwrap(),
+            insert_point_ops,
+            &hw_counter,
+        )
+        .unwrap();
+
+        let new_infos = locked_holder
+            .read()
+            .iter()
+            .map(|(_sid, segment)| segment.get().read().info())
+            .collect_vec();
+        let new_smallest_size = new_infos
+            .iter()
+            .min_by_key(|info| info.num_vectors)
+            .unwrap()
+            .num_vectors;
+
+        assert_eq!(
+            new_smallest_size,
+            smallest_size + 3,
+            "Testing that new data is added to an appendable segment only"
+        );
+
+        let suggested_to_optimize =
+            index_optimizer.check_condition(locked_holder.clone(), &excluded_ids);
+        assert!(suggested_to_optimize.contains(&large_segment_id));
         let permit_cpu_count = num_rayon_threads(0);
         let budget = ResourceBudget::new(permit_cpu_count, permit_cpu_count);
         let permit = budget.try_acquire(0, permit_cpu_count).unwrap();
+        index_optimizer
+            .optimize(
+                locked_holder.clone(),
+                suggested_to_optimize,
+                permit,
+                budget.clone(),
+                &stopped,
+            )
+            .unwrap();
+
+        let infos = locked_holder
+            .read()
+            .iter()
+            .map(|(_sid, segment)| segment.get().read().info())
+            .collect_vec();
+        let configs = locked_holder
+            .read()
+            .iter()
+            .map(|(_sid, segment)| segment.get().read().config().clone())
+            .collect_vec();
+
+        assert_eq!(
+            infos.len(),
+            2,
+            "Testing info length after the optimization"
+        );
+        assert_eq!(
+            configs.len(),
+            2,
+            "Testing configs length after the optimization"
+        );
+
+        let indexed_count = infos
+            .iter()
+            .filter(|info| info.segment_type == SegmentType::Indexed)
+            .count();
+        assert_eq!(
+            indexed_count, 2,
+            "Testing that 2 segments are actually indexed"
+        );
+
+        let on_disk_count = configs
+            .iter()
+            .filter(|config| config.is_any_on_disk())
+            .count();
+        assert_eq!(
+            on_disk_count, 1,
+            "Testing that only largest segment is not Mmap"
+        );
 
         // ------ Plain -> Mmap & Indexed payload
         let suggested_to_optimize =
             index_optimizer.check_condition(locked_holder.clone(), &excluded_ids);
-        assert!(suggested_to_optimize.contains(&large_segment_id));
-        eprintln!("suggested_to_optimize = {suggested_to_optimize:#?}");
+        log::debug!("suggested_to_optimize = {suggested_to_optimize:#?}");
+        let permit = budget.try_acquire(0, permit_cpu_count).unwrap();
         index_optimizer
             .optimize(
                 locked_holder.clone(),
@@ -543,7 +660,7 @@ mod tests {
                 &stopped,
             )
             .unwrap();
-        eprintln!("Done");
+        log::debug!("Done");
 
         // ------ Plain -> Indexed payload
         let permit = budget.try_acquire(0, permit_cpu_count).unwrap();
@@ -567,16 +684,11 @@ mod tests {
 
         assert_eq!(
             locked_holder.read().len(),
-            3,
+            2,
             "Testing no new segments were created"
         );
 
-        let infos = locked_holder
-            .read()
-            .iter()
-            .map(|(_sid, segment)| segment.get().read().info())
-            .collect_vec();
-        let configs = locked_holder
+        let new_configs = locked_holder
             .read()
             .iter()
             .map(|(_sid, segment)| segment.get().read().config().clone())
@@ -591,7 +703,7 @@ mod tests {
             "Testing that 2 segments are actually indexed"
         );
 
-        let on_disk_count = configs
+        let on_disk_count = new_configs
             .iter()
             .filter(|config| config.is_any_on_disk())
             .count();
@@ -671,6 +783,8 @@ mod tests {
             "Testing that new data is added to an appendable segment only"
         );
 
+        let large_segment_id = *locked_holder.read().iter().next().unwrap().0;
+
         // ---- New appendable segment should be created if none left
 
         // Index even the smallest segment
@@ -696,7 +810,7 @@ mod tests {
             .collect_vec();
 
         let mut has_empty = false;
-        for info in new_infos2 {
+        for info in new_infos2.iter() {
             has_empty |= info.num_vectors == 0;
         }
 
@@ -718,6 +832,8 @@ mod tests {
         let insert_point_ops =
             PointOperations::UpsertPoints(PointInsertOperationsInternal::from(batch));
 
+        let hw_counter = HardwareCounterCell::new();
+
         process_point_operation(
             locked_holder.deref(),
             opnum.next().unwrap(),
@@ -761,7 +877,7 @@ mod tests {
 
         let locked_holder: Arc<RwLock<_, _>> = Arc::new(RwLock::new(holder));
 
-        let index_optimizer = IndexingOptimizer::new(
+        let mut index_optimizer = IndexingOptimizer::new(
             number_of_segments, // Keep the same number of segments
             OptimizerThresholds {
                 max_segment_size_kb: 1000,
@@ -881,15 +997,16 @@ mod tests {
                 dir.path().to_owned(),
                 temp_dir.path().to_owned(),
                 collection_params.clone(),
-                hnsw_config.clone(),
+                hnsw_config,
                 Default::default(),
             );
             let config_mismatch_optimizer = ConfigMismatchOptimizer::new(
+                2,
                 thresholds_config,
                 dir.path().to_owned(),
                 temp_dir.path().to_owned(),
                 collection_params.clone(),
-                hnsw_config.clone(),
+                hnsw_config,
                 Default::default(),
             );
 
@@ -945,10 +1062,11 @@ mod tests {
             dir.path().to_owned(),
             temp_dir.path().to_owned(),
             collection_params.clone(),
-            hnsw_config.clone(),
+            hnsw_config,
             Default::default(),
         );
         let config_mismatch_optimizer = ConfigMismatchOptimizer::new(
+            2,
             thresholds_config,
             dir.path().to_owned(),
             temp_dir.path().to_owned(),
