
index 9edafacaa..cc1532612 100644
--- a/qdrant_lib_segment_src_index_field_index_full_text_index_text_index.rs_expectedoutput.txt (expected):tmp/tmp2f2twosz_expected.txt	
+++ b/qdrant_lib_segment_src_index_field_index_full_text_index_text_index.rs_extracted.txt (actual):tmp/tmp9kn_18yd_actual.txt	
@@ -14,10 +14,10 @@ use super::inverted_index::{Document, InvertedIndex, ParsedQuery, TokenId};
 use super::mmap_text_index::{FullTextMmapIndexBuilder, MmapFullTextIndex};
 use super::mutable_text_index::MutableFullTextIndex;
 use super::tokenizers::Tokenizer;
-use crate::common::Flusher;
 use crate::common::operation_error::{OperationError, OperationResult};
 use crate::common::rocksdb_buffered_delete_wrapper::DatabaseColumnScheduledDeleteWrapper;
 use crate::common::rocksdb_wrapper::DatabaseColumnWrapper;
+use crate::common::Flusher;
 use crate::data_types::index::TextIndexParams;
 use crate::index::field_index::{
     CardinalityEstimation, FieldIndexBuilderTrait, PayloadBlockCondition, PayloadFieldIndex,
@@ -38,16 +38,16 @@ impl FullTextIndex {
         config: TextIndexParams,
         field: &str,
         is_appendable: bool,
-    ) -> Self {
+    ) -> FullTextIndex {
         let store_cf_name = Self::storage_cf_name(field);
         let db_wrapper = DatabaseColumnScheduledDeleteWrapper::new(DatabaseColumnWrapper::new(
             db,
             &store_cf_name,
         ));
         if is_appendable {
-            Self::Mutable(MutableFullTextIndex::new(db_wrapper, config))
+            FullTextIndex::Mutable(MutableFullTextIndex::new(db_wrapper, config))
         } else {
-            Self::Immutable(ImmutableFullTextIndex::new(db_wrapper, config))
+            FullTextIndex::Immutable(ImmutableFullTextIndex::new(db_wrapper, config))
         }
     }
 
@@ -191,37 +191,6 @@ impl FullTextIndex {
         }
     }
 
-    pub(super) fn store_key(id: PointOffsetType) -> Vec<u8> {
-        bincode::serialize(&id).unwrap()
-    }
-
-    pub(super) fn restore_key(data: &[u8]) -> PointOffsetType {
-        bincode::deserialize(data).unwrap()
-    }
-
-    pub(super) fn serialize_document_tokens(tokens: BTreeSet<String>) -> OperationResult<Vec<u8>> {
-        #[derive(Serialize)]
-        struct StoredDocument {
-            tokens: BTreeSet<String>,
-        }
-        let doc = StoredDocument { tokens };
-        serde_cbor::to_vec(&doc).map_err(|e| {
-            OperationError::service_error(format!("Failed to serialize document: {e}"))
-        })
-    }
-
-    pub(super) fn deserialize_document(data: &[u8]) -> OperationResult<BTreeSet<String>> {
-        #[derive(Deserialize)]
-        struct StoredDocument {
-            tokens: BTreeSet<String>,
-        }
-        serde_cbor::from_slice::<StoredDocument>(data)
-            .map_err(|e| {
-                OperationError::service_error(format!("Failed to deserialize document: {e}"))
-            })
-            .map(|doc| doc.tokens)
-    }
-
     pub fn get_telemetry_data(&self) -> PayloadIndexTelemetry {
         PayloadIndexTelemetry {
             field_name: None,
@@ -236,7 +205,11 @@ impl FullTextIndex {
         }
     }
 
-    pub fn parse_query(&self, text: &str, hw_counter: &HardwareCounterCell) -> ParsedQuery {
+    pub fn parse_query(
+        &self,
+        text: &str,
+        hw_counter: &HardwareCounterCell,
+    ) -> ParsedQuery {
         let mut tokens = HashSet::new();
         Tokenizer::tokenize_query(text, self.config(), |token| {
             tokens.insert(self.get_token(token, hw_counter));
@@ -246,7 +219,11 @@ impl FullTextIndex {
         }
     }
 
-    pub fn parse_document(&self, text: &str, hw_counter: &HardwareCounterCell) -> Document {
+    pub fn parse_document(
+        &self,
+        text: &str,
+        hw_counter: &HardwareCounterCell,
+    ) -> Document {
         let mut document_tokens = vec![];
         Tokenizer::tokenize_doc(text, self.config(), |token| {
             if let Some(token_id) = self.get_token(token, hw_counter) {
@@ -278,7 +255,7 @@ impl FullTextIndex {
     /// Block until all pages are populated.
     pub fn populate(&self) -> OperationResult<()> {
         match self {
-            FullTextIndex::Mutable(_) => {}   // Not a mmap
+            FullTextIndex::Mutable(_) => {} // Not a mmap
             FullTextIndex::Immutable(_) => {} // Not a mmap
             FullTextIndex::Mmap(index) => index.populate()?,
         }
@@ -288,12 +265,43 @@ impl FullTextIndex {
     /// Drop disk cache.
     pub fn clear_cache(&self) -> OperationResult<()> {
         match self {
-            FullTextIndex::Mutable(_) => {}   // Not a mmap
+            FullTextIndex::Mutable(_) => {} // Not a mmap
             FullTextIndex::Immutable(_) => {} // Not a mmap
             FullTextIndex::Mmap(index) => index.clear_cache()?,
         }
         Ok(())
     }
+
+    pub(super) fn store_key(id: PointOffsetType) -> Vec<u8> {
+        bincode::serialize(&id).unwrap()
+    }
+
+    pub(super) fn restore_key(data: &[u8]) -> PointOffsetType {
+        bincode::deserialize(data).unwrap()
+    }
+
+    pub(super) fn serialize_document_tokens(tokens: BTreeSet<String>) -> OperationResult<Vec<u8>> {
+        #[derive(Serialize)]
+        struct StoredDocument {
+            tokens: BTreeSet<String>,
+        }
+        let doc = StoredDocument { tokens };
+        serde_cbor::to_vec(&doc).map_err(|e| {
+            OperationError::service_error(format!("Failed to serialize document: {e}"))
+        })
+    }
+
+    pub(super) fn deserialize_document(data: &[u8]) -> OperationResult<BTreeSet<String>> {
+        #[derive(Deserialize)]
+        struct StoredDocument {
+            tokens: BTreeSet<String>,
+        }
+        serde_cbor::from_slice::<StoredDocument>(data)
+            .map_err(|e| {
+                OperationError::service_error(format!("Failed to deserialize document: {e}"))
+            })
+            .map(|doc| doc.tokens)
+    }
 }
 
 pub struct FullTextIndexBuilder(FullTextIndex);
