
index ce6172c9a..18d5313e9 100644
--- a/aider_aider_history.py_expectedoutput.txt (expected):tmp/tmpv2065ub6_expected.txt	
+++ b/aider_aider_history.py_extracted.txt (actual):tmp/tmpy0528c27_actual.txt	
@@ -33,60 +33,40 @@ class ChatSummary:
     def summarize_real(self, messages, depth=0):
         if not self.models:
             raise ValueError("No models available for summarization")
+        if len(messages) < 2:
+            return messages
 
         sized = self.tokenize(messages)
         total = sum(tokens for tokens, _msg in sized)
         if total <= self.max_tokens and depth == 0:
             return messages
 
-        min_split = 4
-        if len(messages) <= min_split or depth > 3:
-            return self.summarize_all(messages)
-
-        tail_tokens = 0
         split_index = len(messages)
-        half_max_tokens = self.max_tokens // 2
+        half_max = self.max_tokens // 2
 
-        # Iterate over the messages in reverse order
+        # Accumulate tokens from the end until we reach half the max tokens
+        tail_tokens = 0
         for i in range(len(sized) - 1, -1, -1):
-            tokens, _msg = sized[i]
-            if tail_tokens + tokens < half_max_tokens:
+            tokens, _ = sized[i]
+            if tail_tokens + tokens < half_max:
                 tail_tokens += tokens
                 split_index = i
             else:
                 break
 
         # Ensure the head ends with an assistant message
-        while messages[split_index - 1]["role"] != "assistant" and split_index > 1:
+        while split_index > 0 and messages[split_index - 1]["role"] != "assistant":
             split_index -= 1
-
-        if split_index <= min_split:
-            return self.summarize_all(messages)
+        if split_index == 0:
+            split_index = 1
 
         head = messages[:split_index]
         tail = messages[split_index:]
 
-        sized = sized[:split_index]
-        head.reverse()
-        sized.reverse()
-        keep = []
-        total = 0
-
-        # These sometimes come set with value = None
-        model_max_input_tokens = self.models[0].info.get("max_input_tokens") or 4096
-        model_max_input_tokens -= 512
+        summary = self.summarize_all(head)
 
-        for i in range(split_index):
-            total += sized[i][0]
-            if total > model_max_input_tokens:
-                break
-            keep.append(head[i])
-
-        keep.reverse()
-
-        summary = self.summarize_all(keep)
-
-        tail_tokens = sum(tokens for tokens, msg in sized[split_index:])
+        # Recalculate token counts for the summary and the tail
+        tail_tokens = sum(tokens for tokens, _msg in sized[split_index:])
         summary_tokens = self.token_count(summary)
 
         result = summary + tail
@@ -106,6 +86,9 @@ class ChatSummary:
             if not content.endswith("\n"):
                 content += "\n"
 
+        # Dump for debugging
+        dump(content)
+
         summarize_messages = [
             dict(role="system", content=prompts.summarize),
             dict(role="user", content=content),
@@ -135,9 +118,8 @@ def main():
     with open(args.filename, "r") as f:
         text = f.read()
 
-    summary = summarizer.summarize_chat_history_markdown(text)
-    dump(summary)
-
-
-if __name__ == "__main__":
-    main()
\ No newline at end of file
+    # Placeholder for processing the markdown file.
+    # This example just prints the raw file content.
+    # In practice, you would convert `text` into a list of messages and
+    # call `summarizer.summarize(messages)`, then dump the output.
+    print(text)
\ No newline at end of file
