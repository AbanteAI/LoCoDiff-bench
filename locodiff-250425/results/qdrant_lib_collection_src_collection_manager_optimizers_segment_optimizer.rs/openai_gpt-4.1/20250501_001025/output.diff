
index a458d559..daa75fcc 100644
--- a/qdrant_lib_collection_src_collection_manager_optimizers_segment_optimizer.rs_expectedoutput.txt (expected):tmp/tmpv3jxna2x_expected.txt	
+++ b/qdrant_lib_collection_src_collection_manager_optimizers_segment_optimizer.rs_extracted.txt (actual):tmp/tmpxr570e6n_actual.txt	
@@ -26,7 +26,6 @@ use crate::collection_manager::holders::segment_holder::{
     LockedSegment, LockedSegmentHolder, SegmentId,
 };
 use crate::config::CollectionParams;
-use crate::operations::config_diff::DiffConfig;
 use crate::operations::types::{CollectionError, CollectionResult};
 
 const BYTES_IN_KB: usize = 1024;
@@ -107,7 +106,7 @@ pub trait SegmentOptimizer {
         //     text_vectors: 200,
         //     image_vectors: 10000
         // }
-
+        //
         // Example: bytes_count_by_vector_name = {
         //     text_vectors: 10200 * dim * VECTOR_ELEMENT_SIZE
         //     image_vectors: 10100 * dim * VECTOR_ELEMENT_SIZE
@@ -277,9 +276,9 @@ pub trait SegmentOptimizer {
                         let is_big = threshold_is_on_disk || threshold_is_indexed;
 
                         let index_type = match (is_big, config_on_disk) {
-                            (true, true) => SparseIndexType::Mmap, // Big and configured on disk
+                            (true, true) => SparseIndexType::Mmap,        // Big and configured on disk
                             (true, false) => SparseIndexType::ImmutableRam, // Big and not on disk nor reached threshold
-                            (false, _) => SparseIndexType::MutableRam,      // Small
+                            (false, _) => SparseIndexType::MutableRam,     // Small
                         };
 
                         config.index.index_type = index_type;
@@ -305,7 +304,7 @@ pub trait SegmentOptimizer {
     /// # Arguments
     ///
     /// * `segments` - segment holder
-    /// * `proxy_ids` - ids of poxy-wrapped segment to restore
+    /// * `proxy_ids` - ids of proxied segment to restore
     ///
     /// # Result
     ///
@@ -318,7 +317,7 @@ pub trait SegmentOptimizer {
         proxy_ids: &[SegmentId],
     ) -> Vec<SegmentId> {
         let mut segments_lock = segments.write();
-        let mut restored_segment_ids = vec![];
+        let mut restored_segment_ids = Vec::with_capacity(proxy_ids.len());
         for &proxy_id in proxy_ids {
             if let Some(proxy_segment_ref) = segments_lock.get(proxy_id) {
                 let locked_proxy_segment = proxy_segment_ref.clone();
@@ -444,55 +443,6 @@ pub trait SegmentOptimizer {
             )?;
         }
 
-        // Apply index changes to segment builder
-        // Indexes are only used for defragmentation in segment builder, so versions are ignored
-        for (field_name, change) in proxy_changed_indexes.read().iter_unordered() {
-            match change {
-                ProxyIndexChange::Create(schema, _) => {
-                    segment_builder.add_indexed_field(field_name.to_owned(), schema.to_owned());
-                }
-                ProxyIndexChange::Delete(_) => {
-                    segment_builder.remove_indexed_field(field_name);
-                }
-            }
-        }
-
-        // 000 - acquired
-        // +++ - blocked on waiting
-        //
-        // Case: 1 indexation job at a time, long indexing
-        //
-        //  IO limit = 1
-        // CPU limit = 2                         Next optimization
-        //                                       │            loop
-        //                                       │
-        //                                       ▼
-        //  IO 0  00000000000000                  000000000
-        // CPU 1              00000000000000000
-        //     2              00000000000000000
-        //
-        //
-        //  IO 0  ++++++++++++++00000000000000000
-        // CPU 1                       ++++++++0000000000
-        //     2                       ++++++++0000000000
-        //
-        //
-        //  Case: 1 indexing job at a time, short indexation
-        //
-        //
-        //   IO limit = 1
-        //  CPU limit = 2
-        //
-        //
-        //   IO 0  000000000000   ++++++++0000000000
-        //  CPU 1            00000
-        //      2            00000
-        //
-        //   IO 0  ++++++++++++00000000000   +++++++
-        //  CPU 1                       00000
-        //      2                       00000
-        // At this stage workload shifts from IO to CPU, so we can release IO permit
-
         // Use same number of threads for indexing as for IO.
         // This ensures that IO is equally distributed between optimization jobs.
         let desired_cpus = permit.num_io as usize;
@@ -545,7 +495,7 @@ pub trait SegmentOptimizer {
         Ok(optimized_segment)
     }
 
-    /// Performs optimization of collections's segments, including:
+    /// Performs optimization of collection's segments, including:
     ///     - Segment rebuilding
     ///     - Segment joining
     ///
@@ -622,16 +572,6 @@ pub trait SegmentOptimizer {
             proxies.push(proxy);
         }
 
-        // Save segment version once all payload indices have been converted
-        // If this ends up not being saved due to a crash, the segment will not be used
-        match &tmp_segment {
-            LockedSegment::Original(segment) => {
-                let segment_path = &segment.read().current_path;
-                SegmentVersion::save(segment_path)?;
-            }
-            LockedSegment::Proxy(_) => unreachable!(),
-        }
-
         let proxy_ids: Vec<_> = {
             // Exclusive lock for the segments operations.
             let mut write_segments = RwLockUpgradableReadGuard::upgrade(segments_lock);
@@ -755,8 +695,8 @@ pub trait SegmentOptimizer {
 
                 // unlock collection for search and updates
                 drop(write_segments_guard);
-                // After the collection is unlocked - we can remove data as slow as we want.
 
+                // After the collection is unlocked - we can remove data as slow as we want.
                 // Only remove data after we ensure the consistency of the collection.
                 // If remove fails - we will still have operational collection with reported error.
                 for proxy in proxies {
