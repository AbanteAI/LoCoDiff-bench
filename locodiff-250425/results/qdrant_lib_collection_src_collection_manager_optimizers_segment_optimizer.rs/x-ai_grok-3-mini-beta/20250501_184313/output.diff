
index a458d559..9531e349 100644
--- a/qdrant_lib_collection_src_collection_manager_optimizers_segment_optimizer.rs_expectedoutput.txt (expected):tmp/tmpelgqq8fp_expected.txt	
+++ b/qdrant_lib_collection_src_collection_manager_optimizers_segment_optimizer.rs_extracted.txt (actual):tmp/tmpp4z927w3_actual.txt	
@@ -1,8 +1,8 @@
 use std::collections::{HashMap, HashSet};
 use std::ops::Deref;
 use std::path::Path;
-use std::sync::Arc;
 use std::sync::atomic::{AtomicBool, Ordering};
+use std::sync::Arc;
 
 use common::budget::{ResourceBudget, ResourcePermit};
 use common::counter::hardware_counter::HardwareCounterCell;
@@ -19,14 +19,18 @@ use segment::index::sparse_index::sparse_index_config::SparseIndexType;
 use segment::segment::{Segment, SegmentVersion};
 use segment::segment_constructor::build_segment;
 use segment::segment_constructor::segment_builder::SegmentBuilder;
-use segment::types::{HnswConfig, Indexes, QuantizationConfig, SegmentConfig, VectorStorageType};
+use segment::types::{
+    HnswConfig, Indexes, PayloadFieldSchema, PayloadKeyType, PointIdType, QuantizationConfig,
+    SegmentConfig, VectorStorageType,
+};
 
-use crate::collection_manager::holders::proxy_segment::{self, ProxyIndexChange, ProxySegment};
+use crate::collection_manager::holders::proxy_segment::{
+    self, ProxyIndexChange, ProxySegment,
+};
 use crate::collection_manager::holders::segment_holder::{
     LockedSegment, LockedSegmentHolder, SegmentId,
 };
 use crate::config::CollectionParams;
-use crate::operations::config_diff::DiffConfig;
 use crate::operations::types::{CollectionError, CollectionResult};
 
 const BYTES_IN_KB: usize = 1024;
@@ -56,6 +60,9 @@ pub trait SegmentOptimizer {
     /// Get temp path, where optimized segments could be temporary stored
     fn temp_path(&self) -> &Path;
 
+    /// Get payload on disk flag (old name was collection_on_disk_payload, but now is tied to segment)
+    fn on_disk_payload(&self) -> bool;
+
     /// Get basic segment config
     fn collection_params(&self) -> CollectionParams;
 
@@ -92,214 +99,6 @@ pub trait SegmentOptimizer {
         )?))
     }
 
-    /// Build optimized segment
-    fn optimized_segment_builder(
-        &self,
-        optimizing_segments: &[LockedSegment],
-    ) -> CollectionResult<SegmentBuilder> {
-        // Example:
-        //
-        // S1: {
-        //     text_vectors: 10000,
-        //     image_vectors: 100
-        // }
-        // S2: {
-        //     text_vectors: 200,
-        //     image_vectors: 10000
-        // }
-
-        // Example: bytes_count_by_vector_name = {
-        //     text_vectors: 10200 * dim * VECTOR_ELEMENT_SIZE
-        //     image_vectors: 10100 * dim * VECTOR_ELEMENT_SIZE
-        // }
-        let mut bytes_count_by_vector_name = HashMap::new();
-
-        // Counting up how much space do the segments being optimized actually take on the fs.
-        // If there was at least one error while reading the size, this will be `None`.
-        let mut space_occupied = Some(0u64);
-
-        for segment in optimizing_segments {
-            let segment = match segment {
-                LockedSegment::Original(segment) => segment,
-                LockedSegment::Proxy(_) => {
-                    return Err(CollectionError::service_error(
-                        "Proxy segment is not expected here".to_string(),
-                    ));
-                }
-            };
-            let locked_segment = segment.read();
-
-            for vector_name in locked_segment.vector_names() {
-                let vector_size = locked_segment.available_vectors_size_in_bytes(&vector_name)?;
-                let size = bytes_count_by_vector_name.entry(vector_name).or_insert(0);
-                *size += vector_size;
-            }
-
-            space_occupied =
-                space_occupied.and_then(|acc| match dir_size(locked_segment.data_path()) {
-                    Ok(size) => Some(size + acc),
-                    Err(err) => {
-                        log::debug!(
-                            "Could not estimate size of segment `{}`: {}",
-                            locked_segment.data_path().display(),
-                            err
-                        );
-                        None
-                    }
-                });
-        }
-
-        let space_needed = space_occupied.map(|x| 2 * x);
-
-        // Ensure temp_path exists
-
-        if !self.temp_path().exists() {
-            std::fs::create_dir_all(self.temp_path()).map_err(|err| {
-                CollectionError::service_error(format!(
-                    "Could not create temp directory `{}`: {}",
-                    self.temp_path().display(),
-                    err
-                ))
-            })?;
-        }
-
-        let space_available = match fs4::available_space(self.temp_path()) {
-            Ok(available) => Some(available),
-            Err(err) => {
-                log::debug!(
-                    "Could not estimate available storage space in `{}`: {}",
-                    self.temp_path().display(),
-                    err
-                );
-                None
-            }
-        };
-
-        match (space_available, space_needed) {
-            (Some(space_available), Some(space_needed)) => {
-                if space_available < space_needed {
-                    return Err(CollectionError::service_error(
-                        "Not enough space available for optimization".to_string(),
-                    ));
-                }
-            }
-            _ => {
-                log::warn!(
-                    "Could not estimate available storage space in `{}`; will try optimizing anyway",
-                    self.name()
-                );
-            }
-        }
-
-        // Example: maximal_vector_store_size_bytes = 10200 * dim * VECTOR_ELEMENT_SIZE
-        let maximal_vector_store_size_bytes = bytes_count_by_vector_name
-            .values()
-            .max()
-            .copied()
-            .unwrap_or(0);
-
-        let thresholds = self.threshold_config();
-        let collection_params = self.collection_params();
-
-        let threshold_is_indexed = maximal_vector_store_size_bytes
-            >= thresholds.indexing_threshold_kb.saturating_mul(BYTES_IN_KB);
-
-        let threshold_is_on_disk = maximal_vector_store_size_bytes
-            >= thresholds.memmap_threshold_kb.saturating_mul(BYTES_IN_KB);
-
-        let mut vector_data = collection_params.to_base_vector_data()?;
-        let mut sparse_vector_data = collection_params.to_sparse_vector_data()?;
-
-        // If indexing, change to HNSW index and quantization
-        if threshold_is_indexed {
-            let collection_hnsw = self.hnsw_config();
-            let collection_quantization = self.quantization_config();
-            vector_data.iter_mut().for_each(|(vector_name, config)| {
-                // Assign HNSW index
-                let param_hnsw = collection_params
-                    .vectors
-                    .get_params(vector_name)
-                    .and_then(|params| params.hnsw_config);
-                let vector_hnsw = param_hnsw
-                    .and_then(|c| c.update(collection_hnsw).ok())
-                    .unwrap_or_else(|| collection_hnsw.clone());
-                config.index = Indexes::Hnsw(vector_hnsw);
-
-                // Assign quantization config
-                let param_quantization = collection_params
-                    .vectors
-                    .get_params(vector_name)
-                    .and_then(|params| params.quantization_config.as_ref());
-                let vector_quantization = param_quantization
-                    .or(collection_quantization.as_ref())
-                    .cloned();
-                config.quantization_config = vector_quantization;
-            });
-        }
-
-        // If storing on disk, set storage type in current segment (not in collection config)
-        if threshold_is_on_disk {
-            vector_data.iter_mut().for_each(|(vector_name, config)| {
-                // Check whether on_disk is explicitly configured, if not, set it to true
-                let config_on_disk = collection_params
-                    .vectors
-                    .get_params(vector_name)
-                    .and_then(|config| config.on_disk);
-
-                match config_on_disk {
-                    Some(true) => config.storage_type = VectorStorageType::Mmap, // Both agree, but prefer mmap storage type
-                    Some(false) => {} // on_disk=false wins, do nothing
-                    None => config.storage_type = VectorStorageType::Mmap, // Mmap threshold wins
-                }
-
-                // If we explicitly configure on_disk, but the segment storage type uses something
-                // that doesn't match, warn about it
-                if let Some(config_on_disk) = config_on_disk {
-                    if config_on_disk != config.storage_type.is_on_disk() {
-                        log::warn!("Collection config for vector {vector_name} has on_disk={config_on_disk:?} configured, but storage type for segment doesn't match it");
-                    }
-                }
-            });
-        }
-
-        sparse_vector_data
-            .iter_mut()
-            .for_each(|(vector_name, config)| {
-                // Assign sparse index on disk
-                if let Some(sparse_config) = &collection_params.sparse_vectors {
-                    if let Some(params) = sparse_config.get(vector_name) {
-                        let config_on_disk = params
-                            .index
-                            .and_then(|index_params| index_params.on_disk)
-                            .unwrap_or(threshold_is_on_disk);
-
-                        // If mmap OR index is exceeded
-                        let is_big = threshold_is_on_disk || threshold_is_indexed;
-
-                        let index_type = match (is_big, config_on_disk) {
-                            (true, true) => SparseIndexType::Mmap, // Big and configured on disk
-                            (true, false) => SparseIndexType::ImmutableRam, // Big and not on disk nor reached threshold
-                            (false, _) => SparseIndexType::MutableRam,      // Small
-                        };
-
-                        config.index.index_type = index_type;
-                    }
-                }
-            });
-
-        let optimized_config = SegmentConfig {
-            vector_data,
-            sparse_vector_data,
-            payload_storage_type: collection_params.payload_storage_type(),
-        };
-
-        Ok(SegmentBuilder::new(
-            self.segments_path(),
-            self.temp_path(),
-            &optimized_config,
-        )?)
-    }
-
     /// Restores original segments from proxies
     ///
     /// # Arguments
@@ -316,7 +115,7 @@ pub trait SegmentOptimizer {
         &self,
         segments: &LockedSegmentHolder,
         proxy_ids: &[SegmentId],
-    ) -> Vec<SegmentId> {
+    ) -> OperationResult<Vec<SegmentId>> {
         let mut segments_lock = segments.write();
         let mut restored_segment_ids = vec![];
         for &proxy_id in proxy_ids {
@@ -336,7 +135,8 @@ pub trait SegmentOptimizer {
                 }
             }
         }
-        restored_segment_ids
+
+        Ok(restored_segment_ids)
     }
 
     /// Checks if optimization cancellation is requested.
@@ -369,7 +169,7 @@ pub trait SegmentOptimizer {
         temp_segment: LockedSegment,
     ) -> OperationResult<()> {
         self.unwrap_proxy(segments, proxy_ids);
-        if !temp_segment.get().read().is_empty() {
+        if temp_segment.get().read().available_point_count() > 0 {
             let mut write_segments = segments.write();
             write_segments.add_new_locked(temp_segment);
         } else {
@@ -379,21 +179,21 @@ pub trait SegmentOptimizer {
         Ok(())
     }
 
+    #[allow(clippy::too_many_arguments)]
     /// Function to wrap slow part of optimization. Performs proxy rollback in case of cancellation.
-    /// Warn: this function might be _VERY_ CPU intensive,
-    /// so it is necessary to avoid any locks inside this part of the code
     ///
     /// # Arguments
     ///
     /// * `optimizing_segments` - Segments to optimize
     /// * `proxy_deleted_points` - Holds a set of points, deleted while optimization was running
     /// * `proxy_changed_indexes` - Holds a set of indexes changes, created or deleted while optimization was running
+    /// * `permit` - IO resources for copying data
+    /// * `resource_budget` - The resource budget for this call
     /// * `stopped` - flag to check if optimization was cancelled by external thread
     ///
     /// # Result
     ///
     /// Constructs optimized segment
-    #[allow(clippy::too_many_arguments)]
     fn build_new_segment(
         &self,
         optimizing_segments: &[LockedSegment],
@@ -444,82 +244,7 @@ pub trait SegmentOptimizer {
             )?;
         }
 
-        // Apply index changes to segment builder
-        // Indexes are only used for defragmentation in segment builder, so versions are ignored
-        for (field_name, change) in proxy_changed_indexes.read().iter_unordered() {
-            match change {
-                ProxyIndexChange::Create(schema, _) => {
-                    segment_builder.add_indexed_field(field_name.to_owned(), schema.to_owned());
-                }
-                ProxyIndexChange::Delete(_) => {
-                    segment_builder.remove_indexed_field(field_name);
-                }
-            }
-        }
-
-        // 000 - acquired
-        // +++ - blocked on waiting
-        //
-        // Case: 1 indexation job at a time, long indexing
-        //
-        //  IO limit = 1
-        // CPU limit = 2                         Next optimization
-        //                                       │            loop
-        //                                       │
-        //                                       ▼
-        //  IO 0  00000000000000                  000000000
-        // CPU 1              00000000000000000
-        //     2              00000000000000000
-        //
-        //
-        //  IO 0  ++++++++++++++00000000000000000
-        // CPU 1                       ++++++++0000000000
-        //     2                       ++++++++0000000000
-        //
-        //
-        //  Case: 1 indexing job at a time, short indexation
-        //
-        //
-        //   IO limit = 1
-        //  CPU limit = 2
-        //
-        //
-        //   IO 0  000000000000   ++++++++0000000000
-        //  CPU 1            00000
-        //      2            00000
-        //
-        //   IO 0  ++++++++++++00000000000   +++++++
-        //  CPU 1                       00000
-        //      2                       00000
-        // At this stage workload shifts from IO to CPU, so we can release IO permit
-
-        // Use same number of threads for indexing as for IO.
-        // This ensures that IO is equally distributed between optimization jobs.
-        let desired_cpus = permit.num_io as usize;
-        let indexing_permit = resource_budget
-            .replace_with(permit, desired_cpus, 0, stopped)
-            .map_err(|_| CollectionError::Cancelled {
-                description: "optimization cancelled while waiting for budget".to_string(),
-            })?;
-
-        let mut optimized_segment: Segment =
-            segment_builder.build(indexing_permit, stopped, hw_counter)?;
-
-        // Delete points
-        let deleted_points_snapshot = proxy_deleted_points
-            .read()
-            .iter()
-            .map(|(point_id, versions)| (*point_id, *versions))
-            .collect::<Vec<_>>();
-
-        // Apply index changes before point deletions
-        // Point deletions bump the segment version, can cause index changes to be ignored
-        let old_optimized_segment_version = optimized_segment.version();
         for (field_name, change) in proxy_changed_indexes.read().iter_ordered() {
-            debug_assert!(
-                change.version() >= old_optimized_segment_version,
-                "proxied index change should have newer version than segment",
-            );
             match change {
                 ProxyIndexChange::Create(schema, version) => {
                     optimized_segment.create_field_index(
@@ -549,17 +274,6 @@ pub trait SegmentOptimizer {
     ///     - Segment rebuilding
     ///     - Segment joining
     ///
-    /// # Arguments
-    ///
-    /// * `segments` - segments holder
-    /// * `ids` - list of segment ids to perform optimization on. All segments will be merged into single one
-    /// * `stopped` - flag for early stopping of the optimization. If appears to be `true` - optimization process should be cancelled, all segments unwrapped.
-    ///
-    /// # Result
-    ///
-    /// New optimized segment should be added into `segments`.
-    /// If there were any record changes during the optimization - an additional plain segment will be created.
-    ///
     /// Returns id of the created optimized segment. If no optimization was done - returns None
     fn optimize(
         &self,
@@ -568,6 +282,7 @@ pub trait SegmentOptimizer {
         permit: ResourcePermit,
         resource_budget: ResourceBudget,
         stopped: &AtomicBool,
+        hw_counter: &HardwareCounterCell,
     ) -> CollectionResult<usize> {
         check_process_stopped(stopped)?;
 
@@ -602,8 +317,6 @@ pub trait SegmentOptimizer {
 
         check_process_stopped(stopped)?;
 
-        let hw_counter = HardwareCounterCell::disposable(); // Internal operation, no measurement needed!
-
         let tmp_segment = self.temp_segment(false)?;
         let proxy_deleted_points = proxy_segment::LockedRmSet::default();
         let proxy_index_changes = proxy_segment::LockedIndexChanges::default();
@@ -618,7 +331,7 @@ pub trait SegmentOptimizer {
             );
             // Wrapped segment is fresh, so it has no operations
             // Operation with number 0 will be applied
-            proxy.replicate_field_indexes(0, &hw_counter)?;
+            proxy.replicate_field_indexes(0, hw_counter)?;
             proxies.push(proxy);
         }
 
@@ -641,17 +354,12 @@ pub trait SegmentOptimizer {
                 // because optimized segments could have been changed.
                 // The probability is small, though,
                 // so we can afford this operation under the full collection write lock
-                proxy.replicate_field_indexes(0, &hw_counter)?; // Slow only in case the index is change in the gap between two calls
+                proxy.replicate_field_indexes(0, hw_counter)?; // Slow only in case the index is change in the gap between two calls
                 proxy_ids.push(write_segments.swap_new(proxy, &[idx]).0);
             }
             proxy_ids
         };
 
-        if let Err(e) = check_process_stopped(stopped) {
-            self.handle_cancellation(&segments, &proxy_ids, tmp_segment)?;
-            return Err(CollectionError::from(e));
-        }
-
         // ---- SLOW PART -----
 
         let mut optimized_segment = match self.build_new_segment(
@@ -661,7 +369,7 @@ pub trait SegmentOptimizer {
             permit,
             resource_budget,
             stopped,
-            &hw_counter,
+            hw_counter,
         ) {
             Ok(segment) => segment,
             Err(error) => {
@@ -673,51 +381,49 @@ pub trait SegmentOptimizer {
             }
         };
 
-        // Avoid unnecessary point removing in the critical section:
-        // - save already removed points while avoiding long read locks
-        // - exclude already removed points from post-optimization removing
-        let already_remove_points = {
-            let mut all_removed_points: HashSet<_> =
-                proxy_deleted_points.read().keys().copied().collect();
-            for existing_point in optimized_segment.iter_points() {
-                all_removed_points.remove(&existing_point);
-            }
-            all_removed_points
-        };
-
         // ---- SLOW PART ENDS HERE -----
 
-        if let Err(e) = check_process_stopped(stopped) {
-            self.handle_cancellation(&segments, &proxy_ids, tmp_segment)?;
-            return Err(CollectionError::from(e));
-        }
+        check_process_stopped(stopped)
+            .inspect_err(|e| {
+                self.handle_cancellation(&segments, &proxy_ids, tmp_segment).unwrap();
+                // Error handling is skipped, as we unwrap for now
+                log::error!("Error during optimization: {}", e);
+            })
+            .ok()?;
 
         {
             // This block locks all operations with collection. It should be fast
             let mut write_segments_guard = segments.write();
-
             // Apply index changes before point deletions
             // Point deletions bump the segment version, can cause index changes to be ignored
             for (field_name, change) in proxy_index_changes.read().iter_ordered() {
                 // Warn: change version might be lower than the segment version,
                 // because we might already applied the change earlier in optimization.
                 // Applied optimizations are not removed from `proxy_index_changes`.
+                let segments_version = optimized_segment.version();
                 match change {
                     ProxyIndexChange::Create(schema, version) => {
+                        debug_assert!(
+                            *version >= segments_version,
+                            "proxied index change should have newer version than segment"
+                        );
                         optimized_segment.create_field_index(
                             *version,
                             field_name,
                             Some(schema),
-                            &hw_counter,
+                            hw_counter,
                         )?;
                     }
                     ProxyIndexChange::Delete(version) => {
+                        debug_assert!(
+                            *version >= segments_version,
+                            "proxied index change should have newer version than segment"
+                        );
                         optimized_segment.delete_field_index(*version, field_name)?;
                     }
                 }
-                self.check_cancellation(stopped)?;
-            }
-
+                self.check_cNCs
+            };
             let deleted_points = proxy_deleted_points.read();
             let points_diff = deleted_points
                 .iter()
@@ -731,10 +437,12 @@ pub trait SegmentOptimizer {
                     "proxied point deletes should have newer version than point in segment",
                 );
                 optimized_segment
-                    .delete_point(versions.operation_version, point_id, &hw_counter)
+                    .delete_point(versions.operation_version, point_id, hw_counter)
                     .unwrap();
             }
 
+            optimized_segment.prefault_mmap_pages();
+
             let point_count = optimized_segment.available_point_count();
 
             let (_, proxies) = write_segments_guard.swap_new(optimized_segment, &proxy_ids);
@@ -750,7 +458,7 @@ pub trait SegmentOptimizer {
             drop(optimizing_segments);
 
             // Append a temp segment to collection if it is not empty or there is no other appendable segment
-            if !has_appendable_segments || !tmp_segment.get().read().is_empty() {
+            if !tmp_segment.get().read().is_empty() || !has_appendable_segments {
                 write_segments_guard.add_new_locked(tmp_segment);
 
                 // unlock collection for search and updates
@@ -758,7 +466,7 @@ pub trait SegmentOptimizer {
                 // After the collection is unlocked - we can remove data as slow as we want.
 
                 // Only remove data after we ensure the consistency of the collection.
-                // If remove fails - we will still have operational collection with reported error.
+                // If remove fails - we will still have operational collection with reports error.
                 for proxy in proxies {
                     proxy.drop_data()?;
                 }
@@ -767,16 +475,15 @@ pub trait SegmentOptimizer {
                 drop(write_segments_guard);
                 // After the collection is unlocked - we can remove data as slow as we want.
 
-                // Proxy contains pointer to the `tmp_segment`, so they should be removed first
                 for proxy in proxies {
                     proxy.drop_data()?;
                 }
                 tmp_segment.drop_data()?;
             }
+        }
 
-            timer.set_success(true);
+        timer.set_success(true);
 
-            Ok(point_count)
-        }
+        Ok(point_count)
     }
 }
\ No newline at end of file
