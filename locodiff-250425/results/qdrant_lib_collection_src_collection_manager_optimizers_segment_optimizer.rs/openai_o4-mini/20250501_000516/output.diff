
index a458d559..c35a03ce 100644
--- a/qdrant_lib_collection_src_collection_manager_optimizers_segment_optimizer.rs_expectedoutput.txt (expected):tmp/tmpa8foksp9_expected.txt	
+++ b/qdrant_lib_collection_src_collection_manager_optimizers_segment_optimizer.rs_extracted.txt (actual):tmp/tmpuu_8kn1i_actual.txt	
@@ -1,8 +1,8 @@
 use std::collections::{HashMap, HashSet};
 use std::ops::Deref;
 use std::path::Path;
-use std::sync::Arc;
 use std::sync::atomic::{AtomicBool, Ordering};
+use std::sync::Arc;
 
 use common::budget::{ResourceBudget, ResourcePermit};
 use common::counter::hardware_counter::HardwareCounterCell;
@@ -10,7 +10,7 @@ use common::disk::dir_size;
 use io::storage_version::StorageVersion;
 use itertools::Itertools;
 use parking_lot::{Mutex, RwLockUpgradableReadGuard};
-use segment::common::operation_error::{OperationResult, check_process_stopped};
+use segment::common::operation_error::{check_process_stopped, OperationResult};
 use segment::common::operation_time_statistics::{
     OperationDurationsAggregator, ScopeDurationMeasurer,
 };
@@ -19,7 +19,10 @@ use segment::index::sparse_index::sparse_index_config::SparseIndexType;
 use segment::segment::{Segment, SegmentVersion};
 use segment::segment_constructor::build_segment;
 use segment::segment_constructor::segment_builder::SegmentBuilder;
-use segment::types::{HnswConfig, Indexes, QuantizationConfig, SegmentConfig, VectorStorageType};
+use segment::types::{
+    HnswConfig, Indexes, PayloadFieldSchema, PayloadKeyType, PointIdType,
+    QuantizationConfig, SegmentConfig, VectorStorageType,
+};
 
 use crate::collection_manager::holders::proxy_segment::{self, ProxyIndexChange, ProxySegment};
 use crate::collection_manager::holders::segment_holder::{
@@ -31,6 +34,7 @@ use crate::operations::types::{CollectionError, CollectionResult};
 
 const BYTES_IN_KB: usize = 1024;
 
+/// Thresholds controlling when segments are optimized.
 #[derive(Debug, Clone, Copy)]
 pub struct OptimizerThresholds {
     pub max_segment_size_kb: usize,
@@ -38,46 +42,33 @@ pub struct OptimizerThresholds {
     pub indexing_threshold_kb: usize,
 }
 
-/// SegmentOptimizer - trait implementing common functionality of the optimizers
-///
-/// It provides functions which allow to re-build specified segments into a new, better one.
-/// Process allows read and write (with some tricks) access to the optimized segments.
-///
-/// Process of the optimization is same for all optimizers.
-/// The selection of the candidates for optimization and the configuration
-/// of resulting segment are up to concrete implementations.
+/// A trait that implements segment optimization logic.
 pub trait SegmentOptimizer {
-    /// Get name describing this optimizer
+    /// Get a descriptive name for this optimizer.
     fn name(&self) -> &str;
 
-    /// Get the path of the segments directory
+    /// Path to the directory containing segment files.
     fn segments_path(&self) -> &Path;
 
-    /// Get temp path, where optimized segments could be temporary stored
+    /// Path to use for temporary segment files during optimization.
     fn temp_path(&self) -> &Path;
 
-    /// Get basic segment config
+    /// Collection-level parameters.
     fn collection_params(&self) -> CollectionParams;
 
-    /// Get HNSW config
+    /// HNSW index configuration.
     fn hnsw_config(&self) -> &HnswConfig;
 
-    /// Get quantization config
+    /// Configuration for quantization, if any.
     fn quantization_config(&self) -> Option<QuantizationConfig>;
 
-    /// Get thresholds configuration for the current optimizer
+    /// Thresholds for this optimizer.
     fn threshold_config(&self) -> &OptimizerThresholds;
 
-    /// Checks if segment optimization is required
-    fn check_condition(
-        &self,
-        segments: LockedSegmentHolder,
-        excluded_ids: &HashSet<SegmentId>,
-    ) -> Vec<SegmentId>;
-
+    /// Telemetry aggregator for operation durations.
     fn get_telemetry_counter(&self) -> &Mutex<OperationDurationsAggregator>;
 
-    /// Build temp segment
+    /// Create a new empty temporary segment.
     fn temp_segment(&self, save_version: bool) -> CollectionResult<LockedSegment> {
         let collection_params = self.collection_params();
         let config = SegmentConfig {
@@ -92,67 +83,36 @@ pub trait SegmentOptimizer {
         )?))
     }
 
-    /// Build optimized segment
+    /// Build a `SegmentBuilder` configured for optimized segments.
     fn optimized_segment_builder(
         &self,
         optimizing_segments: &[LockedSegment],
     ) -> CollectionResult<SegmentBuilder> {
-        // Example:
-        //
-        // S1: {
-        //     text_vectors: 10000,
-        //     image_vectors: 100
-        // }
-        // S2: {
-        //     text_vectors: 200,
-        //     image_vectors: 10000
-        // }
-
-        // Example: bytes_count_by_vector_name = {
-        //     text_vectors: 10200 * dim * VECTOR_ELEMENT_SIZE
-        //     image_vectors: 10100 * dim * VECTOR_ELEMENT_SIZE
-        // }
+        // Estimate bytes by vector name across segments.
         let mut bytes_count_by_vector_name = HashMap::new();
-
-        // Counting up how much space do the segments being optimized actually take on the fs.
-        // If there was at least one error while reading the size, this will be `None`.
         let mut space_occupied = Some(0u64);
 
         for segment in optimizing_segments {
             let segment = match segment {
-                LockedSegment::Original(segment) => segment,
+                LockedSegment::Original(o) => o,
                 LockedSegment::Proxy(_) => {
                     return Err(CollectionError::service_error(
                         "Proxy segment is not expected here".to_string(),
                     ));
                 }
             };
-            let locked_segment = segment.read();
-
-            for vector_name in locked_segment.vector_names() {
-                let vector_size = locked_segment.available_vectors_size_in_bytes(&vector_name)?;
-                let size = bytes_count_by_vector_name.entry(vector_name).or_insert(0);
-                *size += vector_size;
+            let locked = segment.read();
+            for vector_name in locked.vector_names() {
+                let vector_size = locked.available_vectors_size_in_bytes(&vector_name)?;
+                *bytes_count_by_vector_name.entry(vector_name).or_insert(0) += vector_size;
             }
-
-            space_occupied =
-                space_occupied.and_then(|acc| match dir_size(locked_segment.data_path()) {
-                    Ok(size) => Some(size + acc),
-                    Err(err) => {
-                        log::debug!(
-                            "Could not estimate size of segment `{}`: {}",
-                            locked_segment.data_path().display(),
-                            err
-                        );
-                        None
-                    }
-                });
+            space_occupied = space_occupied.and_then(|acc| {
+                dir_size(locked.data_path())
+                    .map(|size| size + acc)
+                    .ok()
+            });
         }
-
         let space_needed = space_occupied.map(|x| 2 * x);
-
-        // Ensure temp_path exists
-
         if !self.temp_path().exists() {
             std::fs::create_dir_all(self.temp_path()).map_err(|err| {
                 CollectionError::service_error(format!(
@@ -162,9 +122,8 @@ pub trait SegmentOptimizer {
                 ))
             })?;
         }
-
         let space_available = match fs4::available_space(self.temp_path()) {
-            Ok(available) => Some(available),
+            Ok(avail) => Some(avail),
             Err(err) => {
                 log::debug!(
                     "Could not estimate available storage space in `{}`: {}",
@@ -174,194 +133,114 @@ pub trait SegmentOptimizer {
                 None
             }
         };
-
-        match (space_available, space_needed) {
-            (Some(space_available), Some(space_needed)) => {
-                if space_available < space_needed {
-                    return Err(CollectionError::service_error(
-                        "Not enough space available for optimization".to_string(),
-                    ));
-                }
-            }
-            _ => {
-                log::warn!(
-                    "Could not estimate available storage space in `{}`; will try optimizing anyway",
-                    self.name()
-                );
+        if let (Some(avail), Some(need)) = (space_available, space_needed) {
+            if avail < need {
+                return Err(CollectionError::service_error(
+                    "Not enough space available for optimization".to_string(),
+                ));
             }
         }
 
-        // Example: maximal_vector_store_size_bytes = 10200 * dim * VECTOR_ELEMENT_SIZE
-        let maximal_vector_store_size_bytes = bytes_count_by_vector_name
-            .values()
-            .max()
-            .copied()
-            .unwrap_or(0);
-
+        let maximal = bytes_count_by_vector_name.values().max().copied().unwrap_or(0);
         let thresholds = self.threshold_config();
-        let collection_params = self.collection_params();
+        let params = self.collection_params();
 
-        let threshold_is_indexed = maximal_vector_store_size_bytes
-            >= thresholds.indexing_threshold_kb.saturating_mul(BYTES_IN_KB);
+        let threshold_indexed = maximal >= thresholds.indexing_threshold_kb.saturating_mul(BYTES_IN_KB);
+        let threshold_on_disk = maximal >= thresholds.memmap_threshold_kb.saturating_mul(BYTES_IN_KB);
 
-        let threshold_is_on_disk = maximal_vector_store_size_bytes
-            >= thresholds.memmap_threshold_kb.saturating_mul(BYTES_IN_KB);
+        // Base vector and sparse configs.
+        let mut vector_data = params.to_base_vector_data()?;
+        let mut sparse_vector_data = params.to_sparse_vector_data()?;
 
-        let mut vector_data = collection_params.to_base_vector_data()?;
-        let mut sparse_vector_data = collection_params.to_sparse_vector_data()?;
-
-        // If indexing, change to HNSW index and quantization
-        if threshold_is_indexed {
+        // If indexing, set HNSW + quantization per vector.
+        if threshold_indexed {
             let collection_hnsw = self.hnsw_config();
-            let collection_quantization = self.quantization_config();
-            vector_data.iter_mut().for_each(|(vector_name, config)| {
-                // Assign HNSW index
-                let param_hnsw = collection_params
-                    .vectors
-                    .get_params(vector_name)
-                    .and_then(|params| params.hnsw_config);
-                let vector_hnsw = param_hnsw
+            let collection_quant = self.quantization_config();
+            for (name, cfg) in &mut vector_data {
+                let vec_hnsw = params
+                    .vectors.get_params(name)
+                    .and_then(|p| p.hnsw_config)
                     .and_then(|c| c.update(collection_hnsw).ok())
                     .unwrap_or_else(|| collection_hnsw.clone());
-                config.index = Indexes::Hnsw(vector_hnsw);
-
-                // Assign quantization config
-                let param_quantization = collection_params
+                cfg.index = Indexes::Hnsw(vec_hnsw);
+                cfg.quantization_config = params
                     .vectors
-                    .get_params(vector_name)
-                    .and_then(|params| params.quantization_config.as_ref());
-                let vector_quantization = param_quantization
-                    .or(collection_quantization.as_ref())
-                    .cloned();
-                config.quantization_config = vector_quantization;
-            });
+                    .get_params(name)
+                    .and_then(|p| p.quantization_config.clone())
+                    .or_else(|| collection_quant.clone());
+            }
         }
 
-        // If storing on disk, set storage type in current segment (not in collection config)
-        if threshold_is_on_disk {
-            vector_data.iter_mut().for_each(|(vector_name, config)| {
-                // Check whether on_disk is explicitly configured, if not, set it to true
-                let config_on_disk = collection_params
+        // If on disk threshold, set vector storage to Mmap, respecting explicit config.
+        if threshold_on_disk {
+            for (name, cfg) in &mut vector_data {
+                let explicit = params
                     .vectors
-                    .get_params(vector_name)
-                    .and_then(|config| config.on_disk);
-
-                match config_on_disk {
-                    Some(true) => config.storage_type = VectorStorageType::Mmap, // Both agree, but prefer mmap storage type
-                    Some(false) => {} // on_disk=false wins, do nothing
-                    None => config.storage_type = VectorStorageType::Mmap, // Mmap threshold wins
+                    .get_params(name)
+                    .and_then(|p| p.on_disk);
+                match explicit {
+                    Some(true) => cfg.storage_type = VectorStorageType::Mmap,
+                    Some(false) => {}
+                    None => cfg.storage_type = VectorStorageType::Mmap,
                 }
-
-                // If we explicitly configure on_disk, but the segment storage type uses something
-                // that doesn't match, warn about it
-                if let Some(config_on_disk) = config_on_disk {
-                    if config_on_disk != config.storage_type.is_on_disk() {
-                        log::warn!("Collection config for vector {vector_name} has on_disk={config_on_disk:?} configured, but storage type for segment doesn't match it");
+                if let Some(explicit) = explicit {
+                    if explicit != cfg.storage_type.is_on_disk() {
+                        log::warn!(
+                            "Collection config for vector {name} has on_disk={explicit:?}, but segment storage differs"
+                        );
                     }
                 }
-            });
-        }
-
-        sparse_vector_data
-            .iter_mut()
-            .for_each(|(vector_name, config)| {
-                // Assign sparse index on disk
-                if let Some(sparse_config) = &collection_params.sparse_vectors {
-                    if let Some(params) = sparse_config.get(vector_name) {
-                        let config_on_disk = params
-                            .index
-                            .and_then(|index_params| index_params.on_disk)
-                            .unwrap_or(threshold_is_on_disk);
-
-                        // If mmap OR index is exceeded
-                        let is_big = threshold_is_on_disk || threshold_is_indexed;
-
-                        let index_type = match (is_big, config_on_disk) {
-                            (true, true) => SparseIndexType::Mmap, // Big and configured on disk
-                            (true, false) => SparseIndexType::ImmutableRam, // Big and not on disk nor reached threshold
-                            (false, _) => SparseIndexType::MutableRam,      // Small
+            }
+            // For sparse
+            for (name, cfg) in &mut sparse_vector_data {
+                if let Some(sparse_cfg) = &params.sparse_vectors {
+                    if let Some(p) = sparse_cfg.get(name) {
+                        let on_disk_cfg = p.index.and_then(|i| i.on_disk).unwrap_or(threshold_on_disk);
+                        let is_big = threshold_on_disk || threshold_indexed;
+                        cfg.index.index_type = match (is_big, on_disk_cfg) {
+                            (true, true) => SparseIndexType::Mmap,
+                            (true, false) => SparseIndexType::ImmutableRam,
+                            (false, _) => SparseIndexType::MutableRam,
                         };
-
-                        config.index.index_type = index_type;
                     }
                 }
-            });
+            }
+        }
 
-        let optimized_config = SegmentConfig {
+        let mut config = SegmentConfig {
             vector_data,
             sparse_vector_data,
-            payload_storage_type: collection_params.payload_storage_type(),
+            payload_storage_type: params.payload_storage_type(),
         };
 
         Ok(SegmentBuilder::new(
             self.segments_path(),
             self.temp_path(),
-            &optimized_config,
+            &config,
         )?)
     }
 
-    /// Restores original segments from proxies
-    ///
-    /// # Arguments
-    ///
-    /// * `segments` - segment holder
-    /// * `proxy_ids` - ids of poxy-wrapped segment to restore
-    ///
-    /// # Result
-    ///
-    /// Original segments are pushed into `segments`, proxies removed.
-    /// Returns IDs on restored segments
-    ///
+    /// Unwrap proxies back to original segments.
     fn unwrap_proxy(
         &self,
         segments: &LockedSegmentHolder,
         proxy_ids: &[SegmentId],
     ) -> Vec<SegmentId> {
-        let mut segments_lock = segments.write();
-        let mut restored_segment_ids = vec![];
-        for &proxy_id in proxy_ids {
-            if let Some(proxy_segment_ref) = segments_lock.get(proxy_id) {
-                let locked_proxy_segment = proxy_segment_ref.clone();
-                match locked_proxy_segment {
-                    LockedSegment::Original(_) => {
-                        /* Already unwrapped. It should not actually be here */
-                        log::warn!("Attempt to unwrap raw segment! Should not happen.")
-                    }
-                    LockedSegment::Proxy(proxy_segment) => {
-                        let wrapped_segment = proxy_segment.read().wrapped_segment.clone();
-                        let (restored_id, _proxies) =
-                            segments_lock.swap_new(wrapped_segment, &[proxy_id]);
-                        restored_segment_ids.push(restored_id);
-                    }
+        let mut lock = segments.write();
+        let mut restored = Vec::new();
+        for &pid in proxy_ids {
+            if let Some(seg_ref) = lock.get(pid) {
+                if let LockedSegment::Proxy(px) = seg_ref.clone() {
+                    let orig = px.read().wrapped_segment.clone();
+                    let (new_id, _) = lock.swap_new(orig, &[pid]);
+                    restored.push(new_id);
                 }
             }
         }
-        restored_segment_ids
-    }
-
-    /// Checks if optimization cancellation is requested.
-    fn check_cancellation(&self, stopped: &AtomicBool) -> CollectionResult<()> {
-        if stopped.load(Ordering::Relaxed) {
-            return Err(CollectionError::Cancelled {
-                description: "optimization cancelled by service".to_string(),
-            });
-        }
-        Ok(())
+        restored
     }
 
-    /// Unwraps proxy, adds temp segment into collection and returns a `Cancelled` error.
-    ///
-    /// # Arguments
-    ///
-    /// * `segments` - all registered segments of the collection
-    /// * `proxy_ids` - currently used proxies
-    /// * `temp_segment` - currently used temporary segment
-    ///
-    /// # Result
-    ///
-    /// Rolls back optimization state.
-    /// All processed changes will still be there, but the collection should be returned into state
-    /// before optimization.
+    /// Handle cancellation: unwrap proxies, and if temp segment non-empty, add back.
     fn handle_cancellation(
         &self,
         segments: &LockedSegmentHolder,
@@ -370,131 +249,69 @@ pub trait SegmentOptimizer {
     ) -> OperationResult<()> {
         self.unwrap_proxy(segments, proxy_ids);
         if !temp_segment.get().read().is_empty() {
-            let mut write_segments = segments.write();
-            write_segments.add_new_locked(temp_segment);
+            let mut lock = segments.write();
+            lock.add_new_locked(temp_segment);
         } else {
-            // Temp segment is already removed from proxy, so nobody could write to it in between
             temp_segment.drop_data()?;
         }
         Ok(())
     }
 
-    /// Function to wrap slow part of optimization. Performs proxy rollback in case of cancellation.
-    /// Warn: this function might be _VERY_ CPU intensive,
-    /// so it is necessary to avoid any locks inside this part of the code
-    ///
-    /// # Arguments
-    ///
-    /// * `optimizing_segments` - Segments to optimize
-    /// * `proxy_deleted_points` - Holds a set of points, deleted while optimization was running
-    /// * `proxy_changed_indexes` - Holds a set of indexes changes, created or deleted while optimization was running
-    /// * `stopped` - flag to check if optimization was cancelled by external thread
-    ///
-    /// # Result
-    ///
-    /// Constructs optimized segment
+    /// Build new optimized segment from proxies.
     #[allow(clippy::too_many_arguments)]
     fn build_new_segment(
         &self,
         optimizing_segments: &[LockedSegment],
         proxy_deleted_points: proxy_segment::LockedRmSet,
-        proxy_changed_indexes: proxy_segment::LockedIndexChanges,
-        permit: ResourcePermit, // IO resources for copying data
+        proxy_index_changes: proxy_segment::LockedIndexChanges,
+        permit: ResourcePermit,
         resource_budget: ResourceBudget,
         stopped: &AtomicBool,
         hw_counter: &HardwareCounterCell,
     ) -> CollectionResult<Segment> {
-        let mut segment_builder = self.optimized_segment_builder(optimizing_segments)?;
-
+        let mut builder = self.optimized_segment_builder(optimizing_segments)?;
         self.check_cancellation(stopped)?;
 
-        let segments: Vec<_> = optimizing_segments
+        // Collect originals
+        let originals: Vec<_> = optimizing_segments
             .iter()
-            .map(|i| match i {
+            .map(|s| match s {
                 LockedSegment::Original(o) => o.clone(),
                 LockedSegment::Proxy(_) => {
-                    panic!("Trying to optimize a segment that is already being optimized!")
+                    panic!("Proxy in build_new_segment");
                 }
             })
             .collect();
 
-        let mut defragmentation_keys = HashSet::new();
-        for segment in &segments {
-            let payload_index = &segment.read().payload_index;
-            let payload_index = payload_index.borrow();
-
-            let keys = payload_index
-                .config()
-                .indexed_fields
-                .iter()
-                .filter_map(|(key, schema)| schema.is_tenant().then_some(key))
-                .cloned();
-            defragmentation_keys.extend(keys);
+        // Probe defrag tenant keys
+        let mut keys = HashSet::new();
+        for seg in &originals {
+            let idx = seg.read().payload_index.borrow();
+            for (k, sch) in idx.config().indexed_fields.iter() {
+                if sch.is_tenant() {
+                    keys.insert(k.clone());
+                }
+            }
         }
-
-        if !defragmentation_keys.is_empty() {
-            segment_builder.set_defragment_keys(defragmentation_keys.into_iter().collect());
+        if !keys.is_empty() {
+            builder.set_defragment_keys(keys.into_iter().collect());
         }
 
+        // Update from existing segments
         {
-            let segment_guards = segments.iter().map(|segment| segment.read()).collect_vec();
-            segment_builder.update(
-                &segment_guards.iter().map(Deref::deref).collect_vec(),
-                stopped,
-            )?;
+            let guards: Vec<_> = originals.iter().map(|o| o.read()).collect();
+            builder.update(&guards.iter().map(Deref::deref).collect_vec(), stopped)?;
         }
 
-        // Apply index changes to segment builder
-        // Indexes are only used for defragmentation in segment builder, so versions are ignored
-        for (field_name, change) in proxy_changed_indexes.read().iter_unordered() {
+        // Apply index changes to builder
+        for (fname, change) in proxy_index_changes.read().iter_unordered() {
             match change {
-                ProxyIndexChange::Create(schema, _) => {
-                    segment_builder.add_indexed_field(field_name.to_owned(), schema.to_owned());
-                }
-                ProxyIndexChange::Delete(_) => {
-                    segment_builder.remove_indexed_field(field_name);
-                }
+                ProxyIndexChange::Create(schema, _) => builder.add_indexed_field(fname.clone(), schema.clone()),
+                ProxyIndexChange::Delete(_) => builder.remove_indexed_field(fname),
             }
         }
 
-        // 000 - acquired
-        // +++ - blocked on waiting
-        //
-        // Case: 1 indexation job at a time, long indexing
-        //
-        //  IO limit = 1
-        // CPU limit = 2                         Next optimization
-        //                                       │            loop
-        //                                       │
-        //                                       ▼
-        //  IO 0  00000000000000                  000000000
-        // CPU 1              00000000000000000
-        //     2              00000000000000000
-        //
-        //
-        //  IO 0  ++++++++++++++00000000000000000
-        // CPU 1                       ++++++++0000000000
-        //     2                       ++++++++0000000000
-        //
-        //
-        //  Case: 1 indexing job at a time, short indexation
-        //
-        //
-        //   IO limit = 1
-        //  CPU limit = 2
-        //
-        //
-        //   IO 0  000000000000   ++++++++0000000000
-        //  CPU 1            00000
-        //      2            00000
-        //
-        //   IO 0  ++++++++++++00000000000   +++++++
-        //  CPU 1                       00000
-        //      2                       00000
-        // At this stage workload shifts from IO to CPU, so we can release IO permit
-
-        // Use same number of threads for indexing as for IO.
-        // This ensures that IO is equally distributed between optimization jobs.
+        // First create optimized segment under IO budget
         let desired_cpus = permit.num_io as usize;
         let indexing_permit = resource_budget
             .replace_with(permit, desired_cpus, 0, stopped)
@@ -502,65 +319,36 @@ pub trait SegmentOptimizer {
                 description: "optimization cancelled while waiting for budget".to_string(),
             })?;
 
-        let mut optimized_segment: Segment =
-            segment_builder.build(indexing_permit, stopped, hw_counter)?;
+        let mut optimized = builder.build(indexing_permit, stopped, hw_counter)?;
 
-        // Delete points
-        let deleted_points_snapshot = proxy_deleted_points
-            .read()
-            .iter()
-            .map(|(point_id, versions)| (*point_id, *versions))
-            .collect::<Vec<_>>();
-
-        // Apply index changes before point deletions
-        // Point deletions bump the segment version, can cause index changes to be ignored
-        let old_optimized_segment_version = optimized_segment.version();
-        for (field_name, change) in proxy_changed_indexes.read().iter_ordered() {
-            debug_assert!(
-                change.version() >= old_optimized_segment_version,
-                "proxied index change should have newer version than segment",
-            );
+        // Apply deletions and index changes by version
+        let old_ver = optimized.version();
+        for (fname, change) in proxy_index_changes.read().iter_ordered() {
+            // may be already applied
             match change {
-                ProxyIndexChange::Create(schema, version) => {
-                    optimized_segment.create_field_index(
-                        *version,
-                        field_name,
-                        Some(schema),
-                        hw_counter,
-                    )?;
+                ProxyIndexChange::Create(schema, v) => {
+                    optimized.create_field_index(*v, fname, Some(schema), hw_counter)?;
                 }
-                ProxyIndexChange::Delete(version) => {
-                    optimized_segment.delete_field_index(*version, field_name)?;
+                ProxyIndexChange::Delete(v) => {
+                    optimized.delete_field_index(*v, fname)?;
                 }
             }
             self.check_cancellation(stopped)?;
         }
 
-        for (point_id, versions) in deleted_points_snapshot {
-            optimized_segment
-                .delete_point(versions.operation_version, point_id, hw_counter)
-                .unwrap();
+        let deleted_pts: Vec<_> = proxy_deleted_points
+            .read()
+            .iter()
+            .map(|(pid, ver)| (*pid, *ver))
+            .collect();
+        for (pid, ver) in deleted_pts {
+            optimized.delete_point(ver, pid, hw_counter).unwrap();
         }
 
-        Ok(optimized_segment)
+        Ok(optimized)
     }
 
-    /// Performs optimization of collections's segments, including:
-    ///     - Segment rebuilding
-    ///     - Segment joining
-    ///
-    /// # Arguments
-    ///
-    /// * `segments` - segments holder
-    /// * `ids` - list of segment ids to perform optimization on. All segments will be merged into single one
-    /// * `stopped` - flag for early stopping of the optimization. If appears to be `true` - optimization process should be cancelled, all segments unwrapped.
-    ///
-    /// # Result
-    ///
-    /// New optimized segment should be added into `segments`.
-    /// If there were any record changes during the optimization - an additional plain segment will be created.
-    ///
-    /// Returns id of the created optimized segment. If no optimization was done - returns None
+    /// Perform the full optimize cycle, returning the new segment ID (or 0 if none).
     fn optimize(
         &self,
         segments: LockedSegmentHolder,
@@ -570,213 +358,109 @@ pub trait SegmentOptimizer {
         stopped: &AtomicBool,
     ) -> CollectionResult<usize> {
         check_process_stopped(stopped)?;
-
         let mut timer = ScopeDurationMeasurer::new(self.get_telemetry_counter());
         timer.set_success(false);
 
-        // On the one hand - we want to check consistently if all provided segments are
-        // available for optimization (not already under one) and we want to do it before creating a temp segment
-        // which is an expensive operation. So we can't not unlock `segments` after the check and before the insert.
-        //
-        // On the other hand - we do not want to hold write lock during the segment creation.
-        // Solution in the middle - is a upgradable lock. It ensures consistency after the check and allows to perform read operation.
-        let segments_lock = segments.upgradable_read();
-
-        let optimizing_segments: Vec<_> = ids
+        let lock = segments.upgradable_read();
+        let originals: Vec<_> = ids
             .iter()
             .cloned()
-            .map(|id| segments_lock.get(id))
-            .filter_map(|x| x.cloned())
+            .filter_map(|id| lock.get(id).cloned())
             .collect();
 
-        // Check if all segments are not under other optimization or some ids are missing
-        let all_segments_ok = optimizing_segments.len() == ids.len()
-            && optimizing_segments
-                .iter()
-                .all(|s| matches!(s, LockedSegment::Original(_)));
+        let all_ok = originals.len() == ids.len()
+            && originals.iter().all(|s| matches!(s, LockedSegment::Original(_)));
 
-        if !all_segments_ok {
-            // Cancel the optimization
+        if !all_ok {
+            timer.set_success(false);
             return Ok(0);
         }
 
         check_process_stopped(stopped)?;
 
-        let hw_counter = HardwareCounterCell::disposable(); // Internal operation, no measurement needed!
-
-        let tmp_segment = self.temp_segment(false)?;
-        let proxy_deleted_points = proxy_segment::LockedRmSet::default();
+        let tmp = self.temp_segment(false)?;
+        let proxy_deleted = proxy_segment::LockedRmSet::default();
         let proxy_index_changes = proxy_segment::LockedIndexChanges::default();
 
         let mut proxies = Vec::new();
-        for sg in optimizing_segments.iter() {
-            let mut proxy = ProxySegment::new(
+        for sg in &originals {
+            let mut px = ProxySegment::new(
                 sg.clone(),
-                tmp_segment.clone(),
-                Arc::clone(&proxy_deleted_points),
+                tmp.clone(),
+                Arc::clone(&proxy_deleted),
                 Arc::clone(&proxy_index_changes),
             );
-            // Wrapped segment is fresh, so it has no operations
-            // Operation with number 0 will be applied
-            proxy.replicate_field_indexes(0, &hw_counter)?;
-            proxies.push(proxy);
-        }
-
-        // Save segment version once all payload indices have been converted
-        // If this ends up not being saved due to a crash, the segment will not be used
-        match &tmp_segment {
-            LockedSegment::Original(segment) => {
-                let segment_path = &segment.read().current_path;
-                SegmentVersion::save(segment_path)?;
-            }
-            LockedSegment::Proxy(_) => unreachable!(),
+            px.replicate_field_indexes(0, &HardwareCounterCell::disposable())?;
+            proxies.push(px);
         }
 
         let proxy_ids: Vec<_> = {
-            // Exclusive lock for the segments operations.
-            let mut write_segments = RwLockUpgradableReadGuard::upgrade(segments_lock);
-            let mut proxy_ids = Vec::new();
-            for (mut proxy, idx) in proxies.into_iter().zip(ids.iter().cloned()) {
-                // replicate_field_indexes for the second time,
-                // because optimized segments could have been changed.
-                // The probability is small, though,
-                // so we can afford this operation under the full collection write lock
-                proxy.replicate_field_indexes(0, &hw_counter)?; // Slow only in case the index is change in the gap between two calls
-                proxy_ids.push(write_segments.swap_new(proxy, &[idx]).0);
+            let mut write = RwLockUpgradableReadGuard::upgrade(lock);
+            let mut out = Vec::with_capacity(ids.len());
+            for (mut px, idx) in proxies.into_iter().zip(ids.iter().cloned()) {
+                px.replicate_field_indexes(0, &HardwareCounterCell::disposable())?;
+                out.push(write.swap_new(px, &[idx]).0);
             }
-            proxy_ids
+            out
         };
 
-        if let Err(e) = check_process_stopped(stopped) {
-            self.handle_cancellation(&segments, &proxy_ids, tmp_segment)?;
-            return Err(CollectionError::from(e));
-        }
-
-        // ---- SLOW PART -----
+        check_process_stopped(stopped).inspect_err(|_| {
+            let _ = self.handle_cancellation(&segments, &proxy_ids, tmp.clone());
+        })?;
 
-        let mut optimized_segment = match self.build_new_segment(
-            &optimizing_segments,
-            Arc::clone(&proxy_deleted_points),
+        let optimized_segment = match self.build_new_segment(
+            &originals,
+            Arc::clone(&proxy_deleted),
             Arc::clone(&proxy_index_changes),
             permit,
             resource_budget,
             stopped,
-            &hw_counter,
+            &HardwareCounterCell::disposable(),
         ) {
-            Ok(segment) => segment,
-            Err(error) => {
-                if matches!(error, CollectionError::Cancelled { .. }) {
-                    self.handle_cancellation(&segments, &proxy_ids, tmp_segment)?;
-                    return Err(error);
+            Ok(seg) => seg,
+            Err(err) => {
+                if let CollectionError::Cancelled { .. } = err {
+                    let _ = self.handle_cancellation(&segments, &proxy_ids, tmp.clone());
                 }
-                return Err(error);
-            }
-        };
-
-        // Avoid unnecessary point removing in the critical section:
-        // - save already removed points while avoiding long read locks
-        // - exclude already removed points from post-optimization removing
-        let already_remove_points = {
-            let mut all_removed_points: HashSet<_> =
-                proxy_deleted_points.read().keys().copied().collect();
-            for existing_point in optimized_segment.iter_points() {
-                all_removed_points.remove(&existing_point);
+                return Err(err);
             }
-            all_removed_points
         };
 
-        // ---- SLOW PART ENDS HERE -----
-
-        if let Err(e) = check_process_stopped(stopped) {
-            self.handle_cancellation(&segments, &proxy_ids, tmp_segment)?;
-            return Err(CollectionError::from(e));
-        }
-
         {
-            // This block locks all operations with collection. It should be fast
-            let mut write_segments_guard = segments.write();
-
-            // Apply index changes before point deletions
-            // Point deletions bump the segment version, can cause index changes to be ignored
-            for (field_name, change) in proxy_index_changes.read().iter_ordered() {
-                // Warn: change version might be lower than the segment version,
-                // because we might already applied the change earlier in optimization.
-                // Applied optimizations are not removed from `proxy_index_changes`.
-                match change {
-                    ProxyIndexChange::Create(schema, version) => {
-                        optimized_segment.create_field_index(
-                            *version,
-                            field_name,
-                            Some(schema),
-                            &hw_counter,
-                        )?;
-                    }
-                    ProxyIndexChange::Delete(version) => {
-                        optimized_segment.delete_field_index(*version, field_name)?;
-                    }
+            let mut write = segments.write();
+            let deleted_pts = proxy_deleted.read();
+            let already = {
+                let mut s = HashSet::new();
+                for (pid, _) in deleted_pts.iter() {
+                    s.insert(*pid);
                 }
-                self.check_cancellation(stopped)?;
-            }
-
-            let deleted_points = proxy_deleted_points.read();
-            let points_diff = deleted_points
+                s
+            };
+            let pts_diff = deleted_pts
                 .iter()
-                .filter(|&(point_id, _version)| !already_remove_points.contains(point_id));
-            for (&point_id, &versions) in points_diff {
-                // Delete points here with their operation version, that'll bump the optimized
-                // segment version and will ensure we flush the new changes
-                debug_assert!(
-                    versions.operation_version
-                        >= optimized_segment.point_version(point_id).unwrap_or(0),
-                    "proxied point deletes should have newer version than point in segment",
-                );
-                optimized_segment
-                    .delete_point(versions.operation_version, point_id, &hw_counter)
-                    .unwrap();
+                .filter(|(pid, _)| !already.contains(pid));
+            for (&pid, &ver) in pts_diff {
+                optimized_segment.delete_point(ver, pid, &HardwareCounterCell::disposable()).unwrap();
             }
 
             let point_count = optimized_segment.available_point_count();
-
-            let (_, proxies) = write_segments_guard.swap_new(optimized_segment, &proxy_ids);
+            write.add_new_locked(tmp.clone());
+            let (new_id, proxies) = write.swap_new(optimized_segment, &proxy_ids);
             debug_assert_eq!(
                 proxies.len(),
                 proxy_ids.len(),
-                "swapped different number of proxies on unwrap, missing or incorrect segment IDs?",
+                "swapped different number of proxies"
             );
-
-            let has_appendable_segments = write_segments_guard.has_appendable_segment();
-
-            // Release reference counter of the optimized segments
-            drop(optimizing_segments);
-
-            // Append a temp segment to collection if it is not empty or there is no other appendable segment
-            if !has_appendable_segments || !tmp_segment.get().read().is_empty() {
-                write_segments_guard.add_new_locked(tmp_segment);
-
-                // unlock collection for search and updates
-                drop(write_segments_guard);
-                // After the collection is unlocked - we can remove data as slow as we want.
-
-                // Only remove data after we ensure the consistency of the collection.
-                // If remove fails - we will still have operational collection with reported error.
-                for proxy in proxies {
-                    proxy.drop_data()?;
-                }
+            let has_app = write.has_appendable_segment();
+            if !has_app || !tmp.get().read().is_empty() {
+                write.add_new_locked(tmp);
             } else {
-                // unlock collection for search and updates
-                drop(write_segments_guard);
-                // After the collection is unlocked - we can remove data as slow as we want.
-
-                // Proxy contains pointer to the `tmp_segment`, so they should be removed first
-                for proxy in proxies {
-                    proxy.drop_data()?;
+                for px in proxies {
+                    px.drop_data()?;
                 }
-                tmp_segment.drop_data()?;
             }
-
             timer.set_success(true);
-
-            Ok(point_count)
+            return Ok(point_count as usize);
         }
     }
 }
\ No newline at end of file
