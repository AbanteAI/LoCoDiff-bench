```rust
use std::collections::{HashMap, HashSet};
use std::ops::Deref;
use std::path::Path;
use std::sync::atomic::{AtomicBool, Ordering};
use std::sync::Arc;

use common::budget::{ResourceBudget, ResourcePermit};
use common::counter::hardware_counter::HardwareCounterCell;
use common::disk::dir_size;
use io::storage_version::StorageVersion;
use itertools::Itertools;
use parking_lot::{Mutex, RwLockUpgradableReadGuard};
use segment::common::operation_error::{check_process_stopped, OperationResult};
use segment::common::operation_time_statistics::{
    OperationDurationsAggregator, ScopeDurationMeasurer,
};
use segment::entry::entry_point::SegmentEntry;
use segment::index::sparse_index::sparse_index_config::SparseIndexType;
use segment::segment::{Segment, SegmentVersion};
use segment::segment_constructor::build_segment;
use segment::segment_constructor::segment_builder::SegmentBuilder;
use segment::types::{
    HnswConfig, Indexes, PayloadFieldSchema, PayloadKeyType, PointIdType,
    QuantizationConfig, SegmentConfig, VectorStorageType,
};

use crate::collection_manager::holders::proxy_segment::{self, ProxyIndexChange, ProxySegment};
use crate::collection_manager::holders::segment_holder::{
    LockedSegment, LockedSegmentHolder, SegmentId,
};
use crate::config::CollectionParams;
use crate::operations::config_diff::DiffConfig;
use crate::operations::types::{CollectionError, CollectionResult};

const BYTES_IN_KB: usize = 1024;

/// Thresholds controlling when segments are optimized.
#[derive(Debug, Clone, Copy)]
pub struct OptimizerThresholds {
    pub max_segment_size_kb: usize,
    pub memmap_threshold_kb: usize,
    pub indexing_threshold_kb: usize,
}

/// A trait that implements segment optimization logic.
pub trait SegmentOptimizer {
    /// Get a descriptive name for this optimizer.
    fn name(&self) -> &str;

    /// Path to the directory containing segment files.
    fn segments_path(&self) -> &Path;

    /// Path to use for temporary segment files during optimization.
    fn temp_path(&self) -> &Path;

    /// Collection-level parameters.
    fn collection_params(&self) -> CollectionParams;

    /// HNSW index configuration.
    fn hnsw_config(&self) -> &HnswConfig;

    /// Configuration for quantization, if any.
    fn quantization_config(&self) -> Option<QuantizationConfig>;

    /// Thresholds for this optimizer.
    fn threshold_config(&self) -> &OptimizerThresholds;

    /// Telemetry aggregator for operation durations.
    fn get_telemetry_counter(&self) -> &Mutex<OperationDurationsAggregator>;

    /// Create a new empty temporary segment.
    fn temp_segment(&self, save_version: bool) -> CollectionResult<LockedSegment> {
        let collection_params = self.collection_params();
        let config = SegmentConfig {
            vector_data: collection_params.to_base_vector_data()?,
            sparse_vector_data: collection_params.to_sparse_vector_data()?,
            payload_storage_type: collection_params.payload_storage_type(),
        };
        Ok(LockedSegment::new(build_segment(
            self.segments_path(),
            &config,
            save_version,
        )?))
    }

    /// Build a `SegmentBuilder` configured for optimized segments.
    fn optimized_segment_builder(
        &self,
        optimizing_segments: &[LockedSegment],
    ) -> CollectionResult<SegmentBuilder> {
        // Estimate bytes by vector name across segments.
        let mut bytes_count_by_vector_name = HashMap::new();
        let mut space_occupied = Some(0u64);

        for segment in optimizing_segments {
            let segment = match segment {
                LockedSegment::Original(o) => o,
                LockedSegment::Proxy(_) => {
                    return Err(CollectionError::service_error(
                        "Proxy segment is not expected here".to_string(),
                    ));
                }
            };
            let locked = segment.read();
            for vector_name in locked.vector_names() {
                let vector_size = locked.available_vectors_size_in_bytes(&vector_name)?;
                *bytes_count_by_vector_name.entry(vector_name).or_insert(0) += vector_size;
            }
            space_occupied = space_occupied.and_then(|acc| {
                dir_size(locked.data_path())
                    .map(|size| size + acc)
                    .ok()
            });
        }
        let space_needed = space_occupied.map(|x| 2 * x);
        if !self.temp_path().exists() {
            std::fs::create_dir_all(self.temp_path()).map_err(|err| {
                CollectionError::service_error(format!(
                    "Could not create temp directory `{}`: {}",
                    self.temp_path().display(),
                    err
                ))
            })?;
        }
        let space_available = match fs4::available_space(self.temp_path()) {
            Ok(avail) => Some(avail),
            Err(err) => {
                log::debug!(
                    "Could not estimate available storage space in `{}`: {}",
                    self.temp_path().display(),
                    err
                );
                None
            }
        };
        if let (Some(avail), Some(need)) = (space_available, space_needed) {
            if avail < need {
                return Err(CollectionError::service_error(
                    "Not enough space available for optimization".to_string(),
                ));
            }
        }

        let maximal = bytes_count_by_vector_name.values().max().copied().unwrap_or(0);
        let thresholds = self.threshold_config();
        let params = self.collection_params();

        let threshold_indexed = maximal >= thresholds.indexing_threshold_kb.saturating_mul(BYTES_IN_KB);
        let threshold_on_disk = maximal >= thresholds.memmap_threshold_kb.saturating_mul(BYTES_IN_KB);

        // Base vector and sparse configs.
        let mut vector_data = params.to_base_vector_data()?;
        let mut sparse_vector_data = params.to_sparse_vector_data()?;

        // If indexing, set HNSW + quantization per vector.
        if threshold_indexed {
            let collection_hnsw = self.hnsw_config();
            let collection_quant = self.quantization_config();
            for (name, cfg) in &mut vector_data {
                let vec_hnsw = params
                    .vectors.get_params(name)
                    .and_then(|p| p.hnsw_config)
                    .and_then(|c| c.update(collection_hnsw).ok())
                    .unwrap_or_else(|| collection_hnsw.clone());
                cfg.index = Indexes::Hnsw(vec_hnsw);
                cfg.quantization_config = params
                    .vectors
                    .get_params(name)
                    .and_then(|p| p.quantization_config.clone())
                    .or_else(|| collection_quant.clone());
            }
        }

        // If on disk threshold, set vector storage to Mmap, respecting explicit config.
        if threshold_on_disk {
            for (name, cfg) in &mut vector_data {
                let explicit = params
                    .vectors
                    .get_params(name)
                    .and_then(|p| p.on_disk);
                match explicit {
                    Some(true) => cfg.storage_type = VectorStorageType::Mmap,
                    Some(false) => {}
                    None => cfg.storage_type = VectorStorageType::Mmap,
                }
                if let Some(explicit) = explicit {
                    if explicit != cfg.storage_type.is_on_disk() {
                        log::warn!(
                            "Collection config for vector {name} has on_disk={explicit:?}, but segment storage differs"
                        );
                    }
                }
            }
            // For sparse
            for (name, cfg) in &mut sparse_vector_data {
                if let Some(sparse_cfg) = &params.sparse_vectors {
                    if let Some(p) = sparse_cfg.get(name) {
                        let on_disk_cfg = p.index.and_then(|i| i.on_disk).unwrap_or(threshold_on_disk);
                        let is_big = threshold_on_disk || threshold_indexed;
                        cfg.index.index_type = match (is_big, on_disk_cfg) {
                            (true, true) => SparseIndexType::Mmap,
                            (true, false) => SparseIndexType::ImmutableRam,
                            (false, _) => SparseIndexType::MutableRam,
                        };
                    }
                }
            }
        }

        let mut config = SegmentConfig {
            vector_data,
            sparse_vector_data,
            payload_storage_type: params.payload_storage_type(),
        };

        Ok(SegmentBuilder::new(
            self.segments_path(),
            self.temp_path(),
            &config,
        )?)
    }

    /// Unwrap proxies back to original segments.
    fn unwrap_proxy(
        &self,
        segments: &LockedSegmentHolder,
        proxy_ids: &[SegmentId],
    ) -> Vec<SegmentId> {
        let mut lock = segments.write();
        let mut restored = Vec::new();
        for &pid in proxy_ids {
            if let Some(seg_ref) = lock.get(pid) {
                if let LockedSegment::Proxy(px) = seg_ref.clone() {
                    let orig = px.read().wrapped_segment.clone();
                    let (new_id, _) = lock.swap_new(orig, &[pid]);
                    restored.push(new_id);
                }
            }
        }
        restored
    }

    /// Handle cancellation: unwrap proxies, and if temp segment non-empty, add back.
    fn handle_cancellation(
        &self,
        segments: &LockedSegmentHolder,
        proxy_ids: &[SegmentId],
        temp_segment: LockedSegment,
    ) -> OperationResult<()> {
        self.unwrap_proxy(segments, proxy_ids);
        if !temp_segment.get().read().is_empty() {
            let mut lock = segments.write();
            lock.add_new_locked(temp_segment);
        } else {
            temp_segment.drop_data()?;
        }
        Ok(())
    }

    /// Build new optimized segment from proxies.
    #[allow(clippy::too_many_arguments)]
    fn build_new_segment(
        &self,
        optimizing_segments: &[LockedSegment],
        proxy_deleted_points: proxy_segment::LockedRmSet,
        proxy_index_changes: proxy_segment::LockedIndexChanges,
        permit: ResourcePermit,
        resource_budget: ResourceBudget,
        stopped: &AtomicBool,
        hw_counter: &HardwareCounterCell,
    ) -> CollectionResult<Segment> {
        let mut builder = self.optimized_segment_builder(optimizing_segments)?;
        self.check_cancellation(stopped)?;

        // Collect originals
        let originals: Vec<_> = optimizing_segments
            .iter()
            .map(|s| match s {
                LockedSegment::Original(o) => o.clone(),
                LockedSegment::Proxy(_) => {
                    panic!("Proxy in build_new_segment");
                }
            })
            .collect();

        // Probe defrag tenant keys
        let mut keys = HashSet::new();
        for seg in &originals {
            let idx = seg.read().payload_index.borrow();
            for (k, sch) in idx.config().indexed_fields.iter() {
                if sch.is_tenant() {
                    keys.insert(k.clone());
                }
            }
        }
        if !keys.is_empty() {
            builder.set_defragment_keys(keys.into_iter().collect());
        }

        // Update from existing segments
        {
            let guards: Vec<_> = originals.iter().map(|o| o.read()).collect();
            builder.update(&guards.iter().map(Deref::deref).collect_vec(), stopped)?;
        }

        // Apply index changes to builder
        for (fname, change) in proxy_index_changes.read().iter_unordered() {
            match change {
                ProxyIndexChange::Create(schema, _) => builder.add_indexed_field(fname.clone(), schema.clone()),
                ProxyIndexChange::Delete(_) => builder.remove_indexed_field(fname),
            }
        }

        // First create optimized segment under IO budget
        let desired_cpus = permit.num_io as usize;
        let indexing_permit = resource_budget
            .replace_with(permit, desired_cpus, 0, stopped)
            .map_err(|_| CollectionError::Cancelled {
                description: "optimization cancelled while waiting for budget".to_string(),
            })?;

        let mut optimized = builder.build(indexing_permit, stopped, hw_counter)?;

        // Apply deletions and index changes by version
        let old_ver = optimized.version();
        for (fname, change) in proxy_index_changes.read().iter_ordered() {
            // may be already applied
            match change {
                ProxyIndexChange::Create(schema, v) => {
                    optimized.create_field_index(*v, fname, Some(schema), hw_counter)?;
                }
                ProxyIndexChange::Delete(v) => {
                    optimized.delete_field_index(*v, fname)?;
                }
            }
            self.check_cancellation(stopped)?;
        }

        let deleted_pts: Vec<_> = proxy_deleted_points
            .read()
            .iter()
            .map(|(pid, ver)| (*pid, *ver))
            .collect();
        for (pid, ver) in deleted_pts {
            optimized.delete_point(ver, pid, hw_counter).unwrap();
        }

        Ok(optimized)
    }

    /// Perform the full optimize cycle, returning the new segment ID (or 0 if none).
    fn optimize(
        &self,
        segments: LockedSegmentHolder,
        ids: Vec<SegmentId>,
        permit: ResourcePermit,
        resource_budget: ResourceBudget,
        stopped: &AtomicBool,
    ) -> CollectionResult<usize> {
        check_process_stopped(stopped)?;
        let mut timer = ScopeDurationMeasurer::new(self.get_telemetry_counter());
        timer.set_success(false);

        let lock = segments.upgradable_read();
        let originals: Vec<_> = ids
            .iter()
            .cloned()
            .filter_map(|id| lock.get(id).cloned())
            .collect();

        let all_ok = originals.len() == ids.len()
            && originals.iter().all(|s| matches!(s, LockedSegment::Original(_)));

        if !all_ok {
            timer.set_success(false);
            return Ok(0);
        }

        check_process_stopped(stopped)?;

        let tmp = self.temp_segment(false)?;
        let proxy_deleted = proxy_segment::LockedRmSet::default();
        let proxy_index_changes = proxy_segment::LockedIndexChanges::default();

        let mut proxies = Vec::new();
        for sg in &originals {
            let mut px = ProxySegment::new(
                sg.clone(),
                tmp.clone(),
                Arc::clone(&proxy_deleted),
                Arc::clone(&proxy_index_changes),
            );
            px.replicate_field_indexes(0, &HardwareCounterCell::disposable())?;
            proxies.push(px);
        }

        let proxy_ids: Vec<_> = {
            let mut write = RwLockUpgradableReadGuard::upgrade(lock);
            let mut out = Vec::with_capacity(ids.len());
            for (mut px, idx) in proxies.into_iter().zip(ids.iter().cloned()) {
                px.replicate_field_indexes(0, &HardwareCounterCell::disposable())?;
                out.push(write.swap_new(px, &[idx]).0);
            }
            out
        };

        check_process_stopped(stopped).inspect_err(|_| {
            let _ = self.handle_cancellation(&segments, &proxy_ids, tmp.clone());
        })?;

        let optimized_segment = match self.build_new_segment(
            &originals,
            Arc::clone(&proxy_deleted),
            Arc::clone(&proxy_index_changes),
            permit,
            resource_budget,
            stopped,
            &HardwareCounterCell::disposable(),
        ) {
            Ok(seg) => seg,
            Err(err) => {
                if let CollectionError::Cancelled { .. } = err {
                    let _ = self.handle_cancellation(&segments, &proxy_ids, tmp.clone());
                }
                return Err(err);
            }
        };

        {
            let mut write = segments.write();
            let deleted_pts = proxy_deleted.read();
            let already = {
                let mut s = HashSet::new();
                for (pid, _) in deleted_pts.iter() {
                    s.insert(*pid);
                }
                s
            };
            let pts_diff = deleted_pts
                .iter()
                .filter(|(pid, _)| !already.contains(pid));
            for (&pid, &ver) in pts_diff {
                optimized_segment.delete_point(ver, pid, &HardwareCounterCell::disposable()).unwrap();
            }

            let point_count = optimized_segment.available_point_count();
            write.add_new_locked(tmp.clone());
            let (new_id, proxies) = write.swap_new(optimized_segment, &proxy_ids);
            debug_assert_eq!(
                proxies.len(),
                proxy_ids.len(),
                "swapped different number of proxies"
            );
            let has_app = write.has_appendable_segment();
            if !has_app || !tmp.get().read().is_empty() {
                write.add_new_locked(tmp);
            } else {
                for px in proxies {
                    px.drop_data()?;
                }
            }
            timer.set_success(true);
            return Ok(point_count as usize);
        }
    }
}
```