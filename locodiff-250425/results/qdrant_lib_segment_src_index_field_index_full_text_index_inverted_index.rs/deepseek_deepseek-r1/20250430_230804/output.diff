--- qdrant_lib_segment_src_index_field_index_full_text_index_inverted_index.rs_expectedoutput.txt (expected)+++ qdrant_lib_segment_src_index_field_index_full_text_index_inverted_index.rs_extracted.txt (actual)@@ -1,10 +1,11 @@ use std::collections::{BTreeSet, HashMap};
+use std::mem::size_of;
 
 use common::counter::hardware_counter::HardwareCounterCell;
 use common::types::PointOffsetType;
 use serde::{Deserialize, Serialize};
 
-use crate::common::operation_error::OperationResult;
+use crate::common::operation_error::{OperationError, OperationResult};
 use crate::index::field_index::{CardinalityEstimation, PayloadBlockCondition, PrimaryCondition};
 use crate::types::{FieldCondition, Match, PayloadKeyType};
 
@@ -48,11 +49,8 @@         if self.tokens.contains(&None) {
             return false;
         }
-
-        // Check that all tokens are in document
         self.tokens
             .iter()
-            // unwrap crash safety: all tokens exist in the vocabulary if it passes the above check
             .all(|query_token| document.check(query_token.unwrap()))
     }
 }
@@ -64,7 +62,6 @@         let vocab = self.get_vocab_mut();
         let mut document_tokens = vec![];
         for token in tokens {
-            // check if in vocab
             let vocab_idx = match vocab.get(token) {
                 Some(&idx) => idx,
                 None => {
@@ -75,7 +72,6 @@             };
             document_tokens.push(vocab_idx);
         }
-
         Document::new(document_tokens)
     }
 
@@ -94,8 +90,7 @@         hw_counter: &'a HardwareCounterCell,
     ) -> Box<dyn Iterator<Item = PointOffsetType> + 'a>;
 
-    fn get_posting_len(&self, token_id: TokenId, hw_counter: &HardwareCounterCell)
-    -> Option<usize>;
+    fn get_posting_len(&self, token_id: TokenId, hw_counter: &HardwareCounterCell) -> Option<usize>;
 
     fn estimate_cardinality(
         &self,
@@ -104,7 +99,6 @@         hw_counter: &HardwareCounterCell,
     ) -> CardinalityEstimation {
         let points_count = self.points_count();
-
         let posting_lengths: Option<Vec<usize>> = query
             .tokens
             .iter()
@@ -114,7 +108,6 @@             })
             .collect();
         if posting_lengths.is_none() || points_count == 0 {
-            // There are unseen tokens -> no matches
             return CardinalityEstimation {
                 primary_clauses: vec![PrimaryCondition::Condition(Box::new(condition.clone()))],
                 min: 0,
@@ -124,7 +117,6 @@         }
         let postings = posting_lengths.unwrap();
         if postings.is_empty() {
-            // Empty request -> no matches
             return CardinalityEstimation {
                 primary_clauses: vec![PrimaryCondition::Condition(Box::new(condition.clone()))],
                 min: 0,
@@ -132,9 +124,7 @@                 max: 0,
             };
         }
-        // Smallest posting is the largest possible cardinality
         let smallest_posting = postings.iter().min().copied().unwrap();
-
         if postings.len() == 1 {
             CardinalityEstimation {
                 primary_clauses: vec![PrimaryCondition::Condition(Box::new(condition.clone()))],
@@ -150,7 +140,7 @@             let exp = (expected_frac * points_count as f64) as usize;
             CardinalityEstimation {
                 primary_clauses: vec![PrimaryCondition::Condition(Box::new(condition.clone()))],
-                min: 0, // ToDo: make better estimation
+                min: 0,
                 exp,
                 max: smallest_posting,
             }
@@ -174,9 +164,6 @@                 None
             }
         };
-
-        // It might be very hard to predict possible combinations of conditions,
-        // so we only build it for individual tokens
         self.vocab_with_postings_len_iter()
             .filter_map(map_filter_condition)
     }
@@ -202,8 +189,8 @@     use std::collections::BTreeSet;
 
     use common::counter::hardware_counter::HardwareCounterCell;
+    use rand::seq::SliceRandom;
     use rand::Rng;
-    use rand::seq::SliceRandom;
     use rstest::rstest;
 
     use super::{InvertedIndex, ParsedQuery, TokenId};
@@ -214,7 +201,6 @@     fn generate_word() -> String {
         let mut rng = rand::rng();
 
-        // Each word is 1 to 3 characters long
         let len = rng.random_range(1..=3);
         rng.sample_iter(rand::distr::Alphanumeric)
             .take(len)
@@ -238,18 +224,15 @@ 
     fn mutable_inverted_index(indexed_count: u32, deleted_count: u32) -> MutableInvertedIndex {
         let mut index = MutableInvertedIndex::default();
-
         let hw_counter = HardwareCounterCell::new();
 
         for idx in 0..indexed_count {
-            // Generate 10 tot 30-word documents
             let doc_len = rand::rng().random_range(10..=30);
             let tokens: BTreeSet<String> = (0..doc_len).map(|_| generate_word()).collect();
             let document = index.document_from_tokens(&tokens);
             index.index_document(idx, document, &hw_counter).unwrap();
         }
 
-        // Remove some points
         let mut points_to_delete = (0..indexed_count).collect::<Vec<_>>();
         points_to_delete.shuffle(&mut rand::rng());
         for idx in &points_to_delete[..deleted_count as usize] {
@@ -262,41 +245,25 @@     #[test]
     fn test_mutable_to_immutable() {
         let mutable = mutable_inverted_index(2000, 400);
-
         let immutable = ImmutableInvertedIndex::from(mutable.clone());
 
         assert!(immutable.vocab.len() < mutable.vocab.len());
         assert!(immutable.postings.len() < mutable.postings.len());
         assert!(!immutable.vocab.is_empty());
 
-        // Check that new vocabulary token ids leads to the same posting lists
-        assert!({
-            immutable.vocab.iter().all(|(key, new_token)| {
-                let new_posting = immutable
-                    .postings
-                    .get(*new_token as usize)
-                    .cloned()
-                    .unwrap();
-
-                let orig_token = mutable.vocab.get(key).unwrap();
-
-                let orig_posting = mutable
-                    .postings
-                    .get(*orig_token as usize)
-                    .cloned()
-                    .unwrap()
-                    .unwrap();
-
-                let new_contains_orig = orig_posting
-                    .iter()
-                    .all(|point_id| new_posting.reader().contains(point_id));
-
-                let orig_contains_new = new_posting
-                    .iter()
-                    .all(|point_id| orig_posting.contains(point_id));
-
-                new_contains_orig && orig_contains_new
-            })
+        let hw_counter = HardwareCounterCell::new();
+
+        immutable.vocab.iter().all(|(key, new_token)| {
+            let new_posting = immutable.postings.get(*new_token as usize).cloned().unwrap();
+            let orig_token = mutable.vocab.get(key).unwrap();
+            let orig_posting = mutable.postings.get(*orig_token as usize).cloned().unwrap().unwrap();
+
+            let new_contains_orig = orig_posting.iter().all(|point_id| {
+                new_posting.reader(&hw_counter).contains(point_id)
+            });
+            let orig_contains_new = new_posting.iter().all(|point_id| orig_posting.contains(point_id));
+
+            new_contains_orig && orig_contains_new
         });
     }
 
@@ -313,43 +280,27 @@         let immutable = ImmutableInvertedIndex::from(mutable);
 
         let path = tempfile::tempdir().unwrap().into_path();
-
         MmapInvertedIndex::create(path.clone(), immutable.clone()).unwrap();
 
         let hw_counter = HardwareCounterCell::new();
-
         let mmap = MmapInvertedIndex::open(path, false).unwrap();
 
-        // Check same vocabulary
         for (token, token_id) in immutable.vocab.iter() {
             assert_eq!(mmap.get_token_id(token, &hw_counter), Some(*token_id));
         }
 
-        // Check same postings
         for (token_id, posting) in immutable.postings.iter().enumerate() {
             let chunk_reader = mmap.postings.get(token_id as u32, &hw_counter).unwrap();
-
             for point_id in posting.iter() {
                 assert!(chunk_reader.contains(point_id));
             }
         }
 
         for (point_id, count) in immutable.point_to_tokens_count.iter().enumerate() {
-            // Check same deleted points
-            assert_eq!(
-                mmap.deleted_points.get(point_id).unwrap(),
-                count.is_none(),
-                "point_id: {point_id}"
-            );
-
-            // Check same count
-            assert_eq!(
-                *mmap.point_to_tokens_count.get(point_id).unwrap(),
-                count.unwrap_or(0)
-            );
-        }
-
-        // Check same points count
+            assert_eq!(mmap.deleted_points.get(point_id), count.is_none());
+            assert_eq!(mmap.point_to_tokens_count.get(point_id).unwrap(), &count.unwrap_or(0));
+        }
+
         assert_eq!(mmap.active_points_count, immutable.points_count);
     }
 
@@ -357,69 +308,44 @@     fn test_mmap_index_congruence() {
         let indexed_count = 10000;
         let deleted_count = 500;
-
         let mut mutable = mutable_inverted_index(indexed_count, deleted_count);
         let immutable = ImmutableInvertedIndex::from(mutable.clone());
 
         let path = tempfile::tempdir().unwrap().into_path();
-
         MmapInvertedIndex::create(path.clone(), immutable).unwrap();
 
         let mut mmap_index = MmapInvertedIndex::open(path, false).unwrap();
-
         let queries: Vec<_> = (0..100).map(|_| generate_query()).collect();
 
+        let hw_counter = HardwareCounterCell::new();
         let mut_parsed_queries: Vec<_> = queries
             .clone()
             .into_iter()
             .map(|query| to_parsed_query(query, |token| mutable.vocab.get(&token).copied()))
             .collect();
 
-        let hw_counter = HardwareCounterCell::new();
-
         let imm_parsed_queries: Vec<_> = queries
             .into_iter()
-            .map(|query| {
-                to_parsed_query(query, |token| mmap_index.get_token_id(&token, &hw_counter))
-            })
+            .map(|query| to_parsed_query(query, |token| mmap_index.get_token_id(&token, &hw_counter)))
             .collect();
 
-        for (mut_query, imm_query) in mut_parsed_queries
-            .iter()
-            .cloned()
-            .zip(imm_parsed_queries.iter().cloned())
-        {
-            let mut_filtered = mutable.filter(mut_query, &hw_counter).collect::<Vec<_>>();
-            let imm_filtered = mmap_index
-                .filter(imm_query, &hw_counter)
-                .collect::<Vec<_>>();
-
+        for (mut_query, imm_query) in mut_parsed_queries.iter().cloned().zip(imm_parsed_queries.iter().cloned()) {
+            let mut_filtered = mutable.filter(mut_query.clone(), &hw_counter).collect::<Vec<_>>();
+            let imm_filtered = mmap_index.filter(imm_query.clone(), &hw_counter).collect::<Vec<_>>();
             assert_eq!(mut_filtered, imm_filtered);
         }
-
-        // Delete random documents from both indexes
 
         let points_to_delete: Vec<_> = (0..deleted_count)
             .map(|_| rand::rng().random_range(0..indexed_count))
             .collect();
-
         for point_id in &points_to_delete {
             mutable.remove_document(*point_id);
             mmap_index.remove_document(*point_id);
         }
 
-        // Check congruence after deletion
-
-        for (mut_query, imm_query) in mut_parsed_queries
-            .iter()
-            .cloned()
-            .zip(imm_parsed_queries.iter().cloned())
-        {
-            let mut_filtered = mutable.filter(mut_query, &hw_counter).collect::<Vec<_>>();
-            let imm_filtered = mmap_index
-                .filter(imm_query, &hw_counter)
-                .collect::<Vec<_>>();
-
+        for (mut_query, imm_query) in mut_parsed_queries.iter().cloned().zip(imm_parsed_queries.iter().cloned()) {
+            let mut_filtered = mutable.filter(mut_query.clone(), &hw_counter).collect::<Vec<_>>();
+            let imm_filtered = mmap_index.filter(imm_query.clone(), &hw_counter).collect::<Vec<_>>();
             assert_eq!(mut_filtered, imm_filtered);
         }
     }
