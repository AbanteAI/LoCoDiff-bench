
index a2b4b430..dd99fbbe 100644
--- a/qdrant_lib_segment_src_index_field_index_full_text_index_inverted_index.rs_expectedoutput.txt (expected):tmp/tmpglj_3rhj_expected.txt	
+++ b/qdrant_lib_segment_src_index_field_index_full_text_index_inverted_index.rs_extracted.txt (actual):tmp/tmp_hmytcn2_actual.txt	
@@ -1,6 +1,5 @@
 use std::collections::{BTreeSet, HashMap};
 
-use common::counter::hardware_counter::HardwareCounterCell;
 use common::types::PointOffsetType;
 use serde::{Deserialize, Serialize};
 
@@ -48,73 +47,69 @@ impl ParsedQuery {
         if self.tokens.contains(&None) {
             return false;
         }
-
-        // Check that all tokens are in document
         self.tokens
             .iter()
-            // unwrap crash safety: all tokens exist in the vocabulary if it passes the above check
             .all(|query_token| document.check(query_token.unwrap()))
     }
 }
 
 pub trait InvertedIndex {
+    /// Obtain a mutable reference to the vocabulary map so that
+    /// `document_from_tokens` can assign new token IDs.
     fn get_vocab_mut(&mut self) -> &mut HashMap<String, TokenId>;
 
+    /// Build a `Document` from a set of text tokens, updating the
+    /// vocabulary map as needed.
     fn document_from_tokens(&mut self, tokens: &BTreeSet<String>) -> Document {
         let vocab = self.get_vocab_mut();
-        let mut document_tokens = vec![];
+        let mut document_tokens = Vec::new();
         for token in tokens {
-            // check if in vocab
-            let vocab_idx = match vocab.get(token) {
-                Some(&idx) => idx,
+            let id = match vocab.get(token) {
+                Some(&id) => id,
                 None => {
-                    let next_token_id = vocab.len() as TokenId;
-                    vocab.insert(token.to_string(), next_token_id);
-                    next_token_id
+                    let next_id = vocab.len() as TokenId;
+                    vocab.insert(token.clone(), next_id);
+                    next_id
                 }
             };
-            document_tokens.push(vocab_idx);
+            document_tokens.push(id);
         }
-
         Document::new(document_tokens)
     }
 
+    /// Index a new document under the given point offset.
     fn index_document(
         &mut self,
         idx: PointOffsetType,
         document: Document,
-        hw_counter: &HardwareCounterCell,
     ) -> OperationResult<()>;
 
+    /// Remove a document by point offset. Returns `true` if a document
+    /// was actually removed.
     fn remove_document(&mut self, idx: PointOffsetType) -> bool;
 
-    fn filter<'a>(
-        &'a self,
-        query: ParsedQuery,
-        hw_counter: &'a HardwareCounterCell,
-    ) -> Box<dyn Iterator<Item = PointOffsetType> + 'a>;
+    /// Filter the indexed points by the parsed full-text query.
+    fn filter(&self, query: &ParsedQuery) -> Box<dyn Iterator<Item = PointOffsetType> + '_>;
 
-    fn get_posting_len(&self, token_id: TokenId, hw_counter: &HardwareCounterCell)
-    -> Option<usize>;
+    /// Get the length of the posting list for a given token ID, if any.
+    fn get_posting_len(&self, token_id: TokenId) -> Option<usize>;
 
+    /// Estimate the cardinality of a query under a given field condition.
     fn estimate_cardinality(
         &self,
         query: &ParsedQuery,
         condition: &FieldCondition,
-        hw_counter: &HardwareCounterCell,
     ) -> CardinalityEstimation {
         let points_count = self.points_count();
-
         let posting_lengths: Option<Vec<usize>> = query
             .tokens
             .iter()
-            .map(|&vocab_idx| match vocab_idx {
+            .map(|&opt_id| match opt_id {
                 None => None,
-                Some(idx) => self.get_posting_len(idx, hw_counter),
+                Some(id) => self.get_posting_len(id),
             })
             .collect();
         if posting_lengths.is_none() || points_count == 0 {
-            // There are unseen tokens -> no matches
             return CardinalityEstimation {
                 primary_clauses: vec![PrimaryCondition::Condition(Box::new(condition.clone()))],
                 min: 0,
@@ -124,7 +119,6 @@ pub trait InvertedIndex {
         }
         let postings = posting_lengths.unwrap();
         if postings.is_empty() {
-            // Empty request -> no matches
             return CardinalityEstimation {
                 primary_clauses: vec![PrimaryCondition::Condition(Box::new(condition.clone()))],
                 min: 0,
@@ -132,89 +126,82 @@ pub trait InvertedIndex {
                 max: 0,
             };
         }
-        // Smallest posting is the largest possible cardinality
-        let smallest_posting = postings.iter().min().copied().unwrap();
-
+        let smallest = postings.iter().min().copied().unwrap();
         if postings.len() == 1 {
             CardinalityEstimation {
                 primary_clauses: vec![PrimaryCondition::Condition(Box::new(condition.clone()))],
-                min: smallest_posting,
-                exp: smallest_posting,
-                max: smallest_posting,
+                min: smallest,
+                exp: smallest,
+                max: smallest,
             }
         } else {
             let expected_frac: f64 = postings
                 .iter()
-                .map(|posting| *posting as f64 / points_count as f64)
+                .map(|&len| len as f64 / points_count as f64)
                 .product();
             let exp = (expected_frac * points_count as f64) as usize;
             CardinalityEstimation {
                 primary_clauses: vec![PrimaryCondition::Condition(Box::new(condition.clone()))],
-                min: 0, // ToDo: make better estimation
+                min: 0,
                 exp,
-                max: smallest_posting,
+                max: smallest,
             }
         }
     }
 
+    /// Iterate over the vocabulary, yielding `(token_str, posting_len)`.
     fn vocab_with_postings_len_iter(&self) -> impl Iterator<Item = (&str, usize)> + '_;
 
+    /// Build payload-block conditions for tokens whose posting lists
+    /// meet or exceed `threshold`.
     fn payload_blocks(
         &self,
         threshold: usize,
         key: PayloadKeyType,
     ) -> impl Iterator<Item = PayloadBlockCondition> + '_ {
-        let map_filter_condition = move |(token, postings_len): (&str, usize)| {
-            if postings_len >= threshold {
+        let map_filter = move |(token_str, len): (&str, usize)| {
+            if len >= threshold {
                 Some(PayloadBlockCondition {
-                    condition: FieldCondition::new_match(key.clone(), Match::new_text(token)),
-                    cardinality: postings_len,
+                    condition: FieldCondition::new_match(key.clone(), Match::new_text(token_str)),
+                    cardinality: len,
                 })
             } else {
                 None
             }
         };
-
-        // It might be very hard to predict possible combinations of conditions,
-        // so we only build it for individual tokens
-        self.vocab_with_postings_len_iter()
-            .filter_map(map_filter_condition)
+        self.vocab_with_postings_len_iter().filter_map(map_filter)
     }
 
-    fn check_match(
-        &self,
-        parsed_query: &ParsedQuery,
-        point_id: PointOffsetType,
-        hw_counter: &HardwareCounterCell,
-    ) -> bool;
+    /// Check whether a specific point matches the parsed query.
+    fn check_match(&self, parsed_query: &ParsedQuery, point_id: PointOffsetType) -> bool;
 
+    /// True if there is no document indexed under `point_id`.
     fn values_is_empty(&self, point_id: PointOffsetType) -> bool;
 
+    /// Number of tokens indexed for a given point.
     fn values_count(&self, point_id: PointOffsetType) -> usize;
 
+    /// Total number of indexed points.
     fn points_count(&self) -> usize;
 
-    fn get_token_id(&self, token: &str, hw_counter: &HardwareCounterCell) -> Option<TokenId>;
+    /// Map a token string to its assigned `TokenId`, if any.
+    fn get_token_id(&self, token: &str) -> Option<TokenId>;
 }
 
 #[cfg(test)]
 mod tests {
     use std::collections::BTreeSet;
-
-    use common::counter::hardware_counter::HardwareCounterCell;
     use rand::Rng;
     use rand::seq::SliceRandom;
     use rstest::rstest;
 
-    use super::{InvertedIndex, ParsedQuery, TokenId};
+    use super::{Document, InvertedIndex, ParsedQuery, TokenId};
     use crate::index::field_index::full_text_index::immutable_inverted_index::ImmutableInvertedIndex;
-    use crate::index::field_index::full_text_index::mmap_inverted_index::MmapInvertedIndex;
     use crate::index::field_index::full_text_index::mutable_inverted_index::MutableInvertedIndex;
+    use crate::index::field_index::full_text_index::mmap_inverted_index::MmapInvertedIndex;
 
     fn generate_word() -> String {
         let mut rng = rand::rng();
-
-        // Each word is 1 to 3 characters long
         let len = rng.random_range(1..=3);
         rng.sample_iter(rand::distr::Alphanumeric)
             .take(len)
@@ -232,72 +219,48 @@ mod tests {
         query: Vec<String>,
         token_to_id: impl Fn(String) -> Option<TokenId>,
     ) -> ParsedQuery {
-        let tokens: Vec<_> = query.into_iter().map(token_to_id).collect();
-        ParsedQuery { tokens }
+        ParsedQuery {
+            tokens: query.into_iter().map(token_to_id).collect(),
+        }
     }
 
     fn mutable_inverted_index(indexed_count: u32, deleted_count: u32) -> MutableInvertedIndex {
-        let mut index = MutableInvertedIndex::default();
-
-        let hw_counter = HardwareCounterCell::new();
-
-        for idx in 0..indexed_count {
-            // Generate 10 tot 30-word documents
+        let mut idx = MutableInvertedIndex::default();
+        for pt in 0..indexed_count {
             let doc_len = rand::rng().random_range(10..=30);
             let tokens: BTreeSet<String> = (0..doc_len).map(|_| generate_word()).collect();
-            let document = index.document_from_tokens(&tokens);
-            index.index_document(idx, document, &hw_counter).unwrap();
+            let doc = idx.document_from_tokens(&tokens);
+            idx.index_document(pt, doc).unwrap();
         }
-
-        // Remove some points
-        let mut points_to_delete = (0..indexed_count).collect::<Vec<_>>();
-        points_to_delete.shuffle(&mut rand::rng());
-        for idx in &points_to_delete[..deleted_count as usize] {
-            index.remove_document(*idx);
+        let mut pts: Vec<_> = (0..indexed_count).collect();
+        pts.shuffle(&mut rand::rng());
+        for &pt in &pts[..deleted_count as usize] {
+            idx.remove_document(pt);
         }
-
-        index
+        idx
     }
 
     #[test]
     fn test_mutable_to_immutable() {
-        let mutable = mutable_inverted_index(2000, 400);
-
-        let immutable = ImmutableInvertedIndex::from(mutable.clone());
-
-        assert!(immutable.vocab.len() < mutable.vocab.len());
-        assert!(immutable.postings.len() < mutable.postings.len());
-        assert!(!immutable.vocab.is_empty());
-
-        // Check that new vocabulary token ids leads to the same posting lists
-        assert!({
-            immutable.vocab.iter().all(|(key, new_token)| {
-                let new_posting = immutable
-                    .postings
-                    .get(*new_token as usize)
-                    .cloned()
-                    .unwrap();
-
-                let orig_token = mutable.vocab.get(key).unwrap();
-
-                let orig_posting = mutable
-                    .postings
-                    .get(*orig_token as usize)
-                    .cloned()
-                    .unwrap()
-                    .unwrap();
-
-                let new_contains_orig = orig_posting
-                    .iter()
-                    .all(|point_id| new_posting.reader().contains(point_id));
-
-                let orig_contains_new = new_posting
+        let mut_idx = mutable_inverted_index(2000, 400);
+        let mut_cloned = mut_idx.clone();
+        let imm = ImmutableInvertedIndex::from(mut_idx);
+
+        assert!(imm.vocab.len() < mut_cloned.vocab.len());
+        assert!(imm.postings.len() < mut_cloned.postings.len());
+        assert!(!imm.vocab.is_empty());
+
+        assert!(imm.vocab.iter().all(|(tok, &new_id)| {
+            let new_post = &imm.postings[new_id as usize];
+            let &orig_id = &mut_cloned.vocab[tok];
+            let orig_post = mut_cloned.postings[orig_id as usize].as_ref().unwrap();
+            new_post
+                .iter()
+                .all(|&pt| orig_post.contains(&pt))
+                && orig_post
                     .iter()
-                    .all(|point_id| orig_posting.contains(point_id));
-
-                new_contains_orig && orig_contains_new
-            })
-        });
+                    .all(|&pt| new_post.contains(&pt))
+        }));
     }
 
     #[rstest]
@@ -308,119 +271,71 @@ mod tests {
     #[case(10, 2)]
     #[case(0, 0)]
     #[test]
-    fn test_immutable_to_mmap(#[case] indexed_count: u32, #[case] deleted_count: u32) {
-        let mutable = mutable_inverted_index(indexed_count, deleted_count);
-        let immutable = ImmutableInvertedIndex::from(mutable);
-
-        let path = tempfile::tempdir().unwrap().into_path();
-
-        MmapInvertedIndex::create(path.clone(), immutable.clone()).unwrap();
-
-        let hw_counter = HardwareCounterCell::new();
-
-        let mmap = MmapInvertedIndex::open(path, false).unwrap();
-
-        // Check same vocabulary
-        for (token, token_id) in immutable.vocab.iter() {
-            assert_eq!(mmap.get_token_id(token, &hw_counter), Some(*token_id));
+    fn test_immutable_to_mmap(
+        #[case] indexed_count: u32,
+        #[case] deleted_count: u32,
+    ) {
+        let mut_idx = mutable_inverted_index(indexed_count, deleted_count);
+        let imm = ImmutableInvertedIndex::from(mut_idx.clone());
+        let dir = tempfile::tempdir().unwrap().into_path();
+        MmapInvertedIndex::create(dir.clone(), imm.clone()).unwrap();
+
+        let mmap = MmapInvertedIndex::open(dir.clone(), false).unwrap();
+        for (tok, &tid) in &imm.vocab {
+            assert_eq!(mmap.get_token_id(tok), Some(tid));
         }
-
-        // Check same postings
-        for (token_id, posting) in immutable.postings.iter().enumerate() {
-            let chunk_reader = mmap.postings.get(token_id as u32, &hw_counter).unwrap();
-
-            for point_id in posting.iter() {
-                assert!(chunk_reader.contains(point_id));
+        for (tid, post) in imm.postings.iter().enumerate() {
+            let reader = mmap.postings.get(tid as u32).unwrap();
+            for &pt in post.iter() {
+                assert!(reader.contains(pt));
             }
         }
-
-        for (point_id, count) in immutable.point_to_tokens_count.iter().enumerate() {
-            // Check same deleted points
-            assert_eq!(
-                mmap.deleted_points.get(point_id).unwrap(),
-                count.is_none(),
-                "point_id: {point_id}"
-            );
-
-            // Check same count
-            assert_eq!(
-                *mmap.point_to_tokens_count.get(point_id).unwrap(),
-                count.unwrap_or(0)
-            );
+        for (&pt, &cnt) in imm.point_to_tokens_count.iter().enumerate() {
+            assert_eq!(mmap.deleted_points.get(pt).unwrap(), cnt.is_none());
+            assert_eq!(mmap.point_to_tokens_count[pt], cnt.unwrap_or(0));
         }
-
-        // Check same points count
-        assert_eq!(mmap.active_points_count, immutable.points_count);
+        assert_eq!(mmap.active_points_count, imm.points_count());
     }
 
     #[test]
     fn test_mmap_index_congruence() {
-        let indexed_count = 10000;
-        let deleted_count = 500;
-
-        let mut mutable = mutable_inverted_index(indexed_count, deleted_count);
-        let immutable = ImmutableInvertedIndex::from(mutable.clone());
-
-        let path = tempfile::tempdir().unwrap().into_path();
-
-        MmapInvertedIndex::create(path.clone(), immutable).unwrap();
-
-        let mut mmap_index = MmapInvertedIndex::open(path, false).unwrap();
+        let indexed = 10000;
+        let deleted = 500;
+        let mut_idx = mutable_inverted_index(indexed, deleted);
+        let imm = ImmutableInvertedIndex::from(mut_idx.clone());
+        let dir = tempfile::tempdir().unwrap().into_path();
+        MmapInvertedIndex::create(dir.clone(), imm.clone()).unwrap();
+        let mut mmap_idx = MmapInvertedIndex::open(dir, false).unwrap();
 
         let queries: Vec<_> = (0..100).map(|_| generate_query()).collect();
-
-        let mut_parsed_queries: Vec<_> = queries
+        let mut_qs: Vec<_> = queries
             .clone()
             .into_iter()
-            .map(|query| to_parsed_query(query, |token| mutable.vocab.get(&token).copied()))
+            .map(|q| to_parsed_query(q, |tok| mut_idx.vocab.get(&tok).copied()))
             .collect();
-
-        let hw_counter = HardwareCounterCell::new();
-
-        let imm_parsed_queries: Vec<_> = queries
+        let imm_qs: Vec<_> = queries
             .into_iter()
-            .map(|query| {
-                to_parsed_query(query, |token| mmap_index.get_token_id(&token, &hw_counter))
-            })
+            .map(|q| to_parsed_query(q, |tok| mmap_idx.get_token_id(&tok)))
             .collect();
 
-        for (mut_query, imm_query) in mut_parsed_queries
-            .iter()
-            .cloned()
-            .zip(imm_parsed_queries.iter().cloned())
-        {
-            let mut_filtered = mutable.filter(mut_query, &hw_counter).collect::<Vec<_>>();
-            let imm_filtered = mmap_index
-                .filter(imm_query, &hw_counter)
-                .collect::<Vec<_>>();
-
-            assert_eq!(mut_filtered, imm_filtered);
+        for (mq, iq) in mut_qs.iter().zip(&imm_qs) {
+            let mf: Vec<_> = mut_idx.filter(mq).collect();
+            let imf: Vec<_> = mmap_idx.filter(iq).collect();
+            assert_eq!(mf, imf);
         }
 
-        // Delete random documents from both indexes
-
-        let points_to_delete: Vec<_> = (0..deleted_count)
-            .map(|_| rand::rng().random_range(0..indexed_count))
+        let to_delete: Vec<_> = (0..deleted)
+            .map(|_| rand::rng().random_range(0..indexed))
             .collect();
-
-        for point_id in &points_to_delete {
-            mutable.remove_document(*point_id);
-            mmap_index.remove_document(*point_id);
+        for &pt in &to_delete {
+            mut_idx.remove_document(pt);
+            mmap_idx.remove_document(pt);
         }
 
-        // Check congruence after deletion
-
-        for (mut_query, imm_query) in mut_parsed_queries
-            .iter()
-            .cloned()
-            .zip(imm_parsed_queries.iter().cloned())
-        {
-            let mut_filtered = mutable.filter(mut_query, &hw_counter).collect::<Vec<_>>();
-            let imm_filtered = mmap_index
-                .filter(imm_query, &hw_counter)
-                .collect::<Vec<_>>();
-
-            assert_eq!(mut_filtered, imm_filtered);
+        for (mq, iq) in mut_qs.iter().zip(&imm_qs) {
+            let mf: Vec<_> = mut_idx.filter(mq).collect();
+            let imf: Vec<_> = mmap_idx.filter(iq).collect();
+            assert_eq!(mf, imf);
         }
     }
 }
\ No newline at end of file
