
index a2b4b4303..a9b4c5b64 100644
--- a/qdrant_lib_segment_src_index_field_index_full_text_index_inverted_index.rs_expectedoutput.txt (expected):tmp/tmpfqxu5p7u_expected.txt	
+++ b/qdrant_lib_segment_src_index_field_index_full_text_index_inverted_index.rs_extracted.txt (actual):tmp/tmp3d25ango_actual.txt	
@@ -5,6 +5,8 @@ use common::types::PointOffsetType;
 use serde::{Deserialize, Serialize};
 
 use crate::common::operation_error::OperationResult;
+use crate::index::field_index::full_text_index::immutable_inverted_index::ImmutableInvertedIndex;
+use crate::index::field_index::full_text_index::mutable_inverted_index::MutableInvertedIndex;
 use crate::index::field_index::{CardinalityEstimation, PayloadBlockCondition, PrimaryCondition};
 use crate::types::{FieldCondition, Match, PayloadKeyType};
 
@@ -48,12 +50,9 @@ impl ParsedQuery {
         if self.tokens.contains(&None) {
             return false;
         }
-
-        // Check that all tokens are in document
         self.tokens
             .iter()
-            // unwrap crash safety: all tokens exist in the vocabulary if it passes the above check
-            .all(|query_token| document.check(query_token.unwrap()))
+            .all(|query_token| document.check(*query_token.unwrap()))
     }
 }
 
@@ -62,9 +61,15 @@ pub trait InvertedIndex {
 
     fn document_from_tokens(&mut self, tokens: &BTreeSet<String>) -> Document {
         let vocab = self.get_vocab_mut();
+        Self::document_from_tokens_impl(vocab, tokens)
+    }
+
+    fn document_from_tokens_impl(
+        vocab: &mut HashMap<String, TokenId>,
+        tokens: &BTreeSet<String>,
+    ) -> Document {
         let mut document_tokens = vec![];
         for token in tokens {
-            // check if in vocab
             let vocab_idx = match vocab.get(token) {
                 Some(&idx) => idx,
                 None => {
@@ -75,7 +80,6 @@ pub trait InvertedIndex {
             };
             document_tokens.push(vocab_idx);
         }
-
         Document::new(document_tokens)
     }
 
@@ -94,8 +98,11 @@ pub trait InvertedIndex {
         hw_counter: &'a HardwareCounterCell,
     ) -> Box<dyn Iterator<Item = PointOffsetType> + 'a>;
 
-    fn get_posting_len(&self, token_id: TokenId, hw_counter: &HardwareCounterCell)
-    -> Option<usize>;
+    fn get_posting_len(
+        &self,
+        token_id: TokenId,
+        hw_counter: &HardwareCounterCell,
+    ) -> Option<usize>;
 
     fn estimate_cardinality(
         &self,
@@ -114,7 +121,6 @@ pub trait InvertedIndex {
             })
             .collect();
         if posting_lengths.is_none() || points_count == 0 {
-            // There are unseen tokens -> no matches
             return CardinalityEstimation {
                 primary_clauses: vec![PrimaryCondition::Condition(Box::new(condition.clone()))],
                 min: 0,
@@ -124,7 +130,6 @@ pub trait InvertedIndex {
         }
         let postings = posting_lengths.unwrap();
         if postings.is_empty() {
-            // Empty request -> no matches
             return CardinalityEstimation {
                 primary_clauses: vec![PrimaryCondition::Condition(Box::new(condition.clone()))],
                 min: 0,
@@ -132,7 +137,6 @@ pub trait InvertedIndex {
                 max: 0,
             };
         }
-        // Smallest posting is the largest possible cardinality
         let smallest_posting = postings.iter().min().copied().unwrap();
 
         if postings.len() == 1 {
@@ -175,8 +179,6 @@ pub trait InvertedIndex {
             }
         };
 
-        // It might be very hard to predict possible combinations of conditions,
-        // so we only build it for individual tokens
         self.vocab_with_postings_len_iter()
             .filter_map(map_filter_condition)
     }
@@ -194,7 +196,11 @@ pub trait InvertedIndex {
 
     fn points_count(&self) -> usize;
 
-    fn get_token_id(&self, token: &str, hw_counter: &HardwareCounterCell) -> Option<TokenId>;
+    fn get_token_id(
+        &self,
+        token: &str,
+        hw_counter: &HardwareCounterCell,
+    ) -> Option<TokenId>;
 }
 
 #[cfg(test)]
@@ -206,15 +212,13 @@ mod tests {
     use rand::seq::SliceRandom;
     use rstest::rstest;
 
-    use super::{InvertedIndex, ParsedQuery, TokenId};
+    use super::{ParsedQuery, TokenId};
     use crate::index::field_index::full_text_index::immutable_inverted_index::ImmutableInvertedIndex;
     use crate::index::field_index::full_text_index::mmap_inverted_index::MmapInvertedIndex;
     use crate::index::field_index::full_text_index::mutable_inverted_index::MutableInvertedIndex;
 
     fn generate_word() -> String {
         let mut rng = rand::rng();
-
-        // Each word is 1 to 3 characters long
         let len = rng.random_range(1..=3);
         rng.sample_iter(rand::distr::Alphanumeric)
             .take(len)
@@ -238,38 +242,30 @@ mod tests {
 
     fn mutable_inverted_index(indexed_count: u32, deleted_count: u32) -> MutableInvertedIndex {
         let mut index = MutableInvertedIndex::default();
-
         let hw_counter = HardwareCounterCell::new();
-
         for idx in 0..indexed_count {
-            // Generate 10 tot 30-word documents
             let doc_len = rand::rng().random_range(10..=30);
             let tokens: BTreeSet<String> = (0..doc_len).map(|_| generate_word()).collect();
             let document = index.document_from_tokens(&tokens);
             index.index_document(idx, document, &hw_counter).unwrap();
         }
-
-        // Remove some points
         let mut points_to_delete = (0..indexed_count).collect::<Vec<_>>();
         points_to_delete.shuffle(&mut rand::rng());
         for idx in &points_to_delete[..deleted_count as usize] {
             index.remove_document(*idx);
         }
-
         index
     }
 
     #[test]
     fn test_mutable_to_immutable() {
         let mutable = mutable_inverted_index(2000, 400);
-
         let immutable = ImmutableInvertedIndex::from(mutable.clone());
 
         assert!(immutable.vocab.len() < mutable.vocab.len());
         assert!(immutable.postings.len() < mutable.postings.len());
         assert!(!immutable.vocab.is_empty());
 
-        // Check that new vocabulary token ids leads to the same posting lists
         assert!({
             immutable.vocab.iter().all(|(key, new_token)| {
                 let new_posting = immutable
@@ -290,7 +286,6 @@ mod tests {
                 let new_contains_orig = orig_posting
                     .iter()
                     .all(|point_id| new_posting.reader().contains(point_id));
-
                 let orig_contains_new = new_posting
                     .iter()
                     .all(|point_id| orig_posting.contains(point_id));
@@ -314,18 +309,16 @@ mod tests {
 
         let path = tempfile::tempdir().unwrap().into_path();
 
-        MmapInvertedIndex::create(path.clone(), immutable.clone()).unwrap();
-
-        let hw_counter = HardwareCounterCell::new();
+        MmapInvertedIndex::create(path.clone(), immutable).unwrap();
 
         let mmap = MmapInvertedIndex::open(path, false).unwrap();
 
-        // Check same vocabulary
+        let hw_counter = HardwareCounterCell::new();
+
         for (token, token_id) in immutable.vocab.iter() {
             assert_eq!(mmap.get_token_id(token, &hw_counter), Some(*token_id));
         }
 
-        // Check same postings
         for (token_id, posting) in immutable.postings.iter().enumerate() {
             let chunk_reader = mmap.postings.get(token_id as u32, &hw_counter).unwrap();
 
@@ -335,21 +328,18 @@ mod tests {
         }
 
         for (point_id, count) in immutable.point_to_tokens_count.iter().enumerate() {
-            // Check same deleted points
             assert_eq!(
                 mmap.deleted_points.get(point_id).unwrap(),
                 count.is_none(),
                 "point_id: {point_id}"
             );
 
-            // Check same count
             assert_eq!(
                 *mmap.point_to_tokens_count.get(point_id).unwrap(),
                 count.unwrap_or(0)
             );
         }
 
-        // Check same points count
         assert_eq!(mmap.active_points_count, immutable.points_count);
     }
 
@@ -379,9 +369,7 @@ mod tests {
 
         let imm_parsed_queries: Vec<_> = queries
             .into_iter()
-            .map(|query| {
-                to_parsed_query(query, |token| mmap_index.get_token_id(&token, &hw_counter))
-            })
+            .map(|query| to_parsed_query(query, |token| mmap_index.get_token_id(token, &hw_counter)))
             .collect();
 
         for (mut_query, imm_query) in mut_parsed_queries
@@ -397,8 +385,6 @@ mod tests {
             assert_eq!(mut_filtered, imm_filtered);
         }
 
-        // Delete random documents from both indexes
-
         let points_to_delete: Vec<_> = (0..deleted_count)
             .map(|_| rand::rng().random_range(0..indexed_count))
             .collect();
@@ -408,8 +394,6 @@ mod tests {
             mmap_index.remove_document(*point_id);
         }
 
-        // Check congruence after deletion
-
         for (mut_query, imm_query) in mut_parsed_queries
             .iter()
             .cloned()
