
index cb922e86..26875db1 100644
--- a/qdrant_lib_collection_src_update_handler.rs_expectedoutput.txt (expected):tmp/tmpp1dx5x7r_expected.txt	
+++ b/qdrant_lib_collection_src_update_handler.rs_extracted.txt (actual):tmp/tmpyst6zp29_actual.txt	
@@ -1,25 +1,23 @@
 use std::cmp::min;
 use std::collections::HashSet;
 use std::path::{Path, PathBuf};
-use std::sync::Arc;
 use std::sync::atomic::{AtomicBool, AtomicU64, AtomicUsize, Ordering};
+use std::sync::Arc;
 
 use common::budget::ResourceBudget;
-use common::counter::hardware_accumulator::HwMeasurementAcc;
 use common::counter::hardware_counter::HardwareCounterCell;
 use common::panic;
 use itertools::Itertools;
 use log::{debug, error, info, trace, warn};
 use parking_lot::Mutex;
 use segment::common::operation_error::OperationResult;
-use segment::index::hnsw_index::num_rayon_threads;
 use segment::types::SeqNumberType;
 use tokio::runtime::Handle;
 use tokio::sync::mpsc::{self, Receiver, Sender};
-use tokio::sync::{Mutex as TokioMutex, oneshot};
+use tokio::sync::{oneshot, Mutex as TokioMutex};
 use tokio::task::{self, JoinHandle};
 use tokio::time::error::Elapsed;
-use tokio::time::{Duration, timeout};
+use tokio::time::{timeout, Duration};
 
 use crate::collection::payload_index_schema::PayloadIndexSchema;
 use crate::collection_manager::collection_updater::CollectionUpdater;
@@ -28,102 +26,62 @@ use crate::collection_manager::optimizers::segment_optimizer::{
     OptimizerThresholds, SegmentOptimizer,
 };
 use crate::collection_manager::optimizers::{Tracker, TrackerLog, TrackerStatus};
-use crate::common::stoppable_task::{StoppableTaskHandle, spawn_stoppable};
+use crate::common::stoppable_task::{spawn_stoppable, StoppableTaskHandle};
 use crate::config::CollectionParams;
-use crate::operations::CollectionUpdateOperations;
 use crate::operations::shared_storage_config::SharedStorageConfig;
 use crate::operations::types::{CollectionError, CollectionResult};
+use crate::operations::CollectionUpdateOperations;
 use crate::save_on_disk::SaveOnDisk;
 use crate::shards::local_shard::LocalShardClocks;
 use crate::wal::WalError;
 use crate::wal_delta::LockedWal;
 
-/// Interval at which the optimizer worker cleans up old optimization handles
-///
-/// The longer the duration, the longer it  takes for panicked tasks to be reported.
 const OPTIMIZER_CLEANUP_INTERVAL: Duration = Duration::from_secs(5);
 
 pub type Optimizer = dyn SegmentOptimizer + Sync + Send;
 
-/// Information, required to perform operation and notify regarding the result
 #[derive(Debug)]
 pub struct OperationData {
-    /// Sequential number of the operation
     pub op_num: SeqNumberType,
-    /// Operation
     pub operation: CollectionUpdateOperations,
-    /// If operation was requested to wait for result
     pub wait: bool,
-    /// Callback notification channel
     pub sender: Option<oneshot::Sender<CollectionResult<usize>>>,
-    pub hw_measurements: HwMeasurementAcc,
+    pub hw_measurements: common::counter::hardware_accumulator::HwMeasurementAcc,
 }
 
-/// Signal, used to inform Updater process
-#[derive(Debug)]
 pub enum UpdateSignal {
-    /// Requested operation to perform
     Operation(OperationData),
-    /// Stop all optimizers and listening
     Stop,
-    /// Empty signal used to trigger optimizers
     Nop,
-    /// Ensures that previous updates are applied
     Plunger(oneshot::Sender<()>),
 }
 
-/// Signal, used to inform Optimization process
-#[derive(PartialEq, Eq, Clone, Copy)]
 pub enum OptimizerSignal {
-    /// Sequential number of the operation
     Operation(SeqNumberType),
-    /// Stop all optimizers and listening
-    Stop,
-    /// Empty signal used to trigger optimizers
     Nop,
+    Stop,
 }
 
-/// Structure, which holds object, required for processing updates of the collection
 pub struct UpdateHandler {
     shared_storage_config: Arc<SharedStorageConfig>,
     payload_index_schema: Arc<SaveOnDisk<PayloadIndexSchema>>,
-    /// List of used optimizers
     pub optimizers: Arc<Vec<Arc<Optimizer>>>,
-    /// Log of optimizer statuses
     optimizers_log: Arc<Mutex<TrackerLog>>,
-    /// Total number of optimized points since last start
     total_optimized_points: Arc<AtomicUsize>,
-    /// Global CPU budget in number of cores for all optimization tasks.
-    /// Assigns CPU permits to tasks to limit overall resource utilization.
     optimizer_resource_budget: ResourceBudget,
-    /// How frequent can we flush data
-    /// This parameter depends on the optimizer config and should be updated accordingly.
-    pub flush_interval_sec: u64,
+    flush_interval_sec: u64,
     segments: LockedSegmentHolder,
-    /// Process, that listens updates signals and perform updates
     update_worker: Option<JoinHandle<()>>,
-    /// Process, that listens for post-update signals and performs optimization
     optimizer_worker: Option<JoinHandle<()>>,
-    /// Process that periodically flushes segments and tries to truncate wal
     flush_worker: Option<JoinHandle<()>>,
-    /// Sender to stop flush worker
     flush_stop: Option<oneshot::Sender<()>>,
     runtime_handle: Handle,
-    /// WAL, required for operations
     wal: LockedWal,
-    /// Always keep this WAL version and later and prevent acknowledging/truncating from the WAL.
-    /// This is used when other bits of code still depend on information in the WAL, such as the
-    /// queue proxy shard.
-    /// Defaults to `u64::MAX` to allow acknowledging all confirmed versions.
-    pub(super) wal_keep_from: Arc<AtomicU64>,
+    wal_keep_from: Arc<AtomicU64>,
     optimization_handles: Arc<TokioMutex<Vec<StoppableTaskHandle<bool>>>>,
-    /// Maximum number of concurrent optimization jobs in this update handler.
-    /// This parameter depends on the optimizer config and should be updated accordingly.
-    pub max_optimization_threads: Option<usize>,
-    /// Highest and cutoff clocks for the shard WAL.
+    max_optimization_threads: Option<usize>,
     clocks: LocalShardClocks,
     shard_path: PathBuf,
-    /// Whether we have ever triggered optimizers since starting.
     has_triggered_optimizers: Arc<AtomicBool>,
 }
 
@@ -158,7 +116,7 @@ impl UpdateHandler {
             flush_stop: None,
             runtime_handle,
             wal,
-            wal_keep_from: Arc::new(u64::MAX.into()),
+            wal_keep_from: Arc::new(0.into()),
             flush_interval_sec,
             optimization_handles: Arc::new(TokioMutex::new(vec![])),
             max_optimization_threads,
@@ -203,16 +161,31 @@ impl UpdateHandler {
         self.flush_stop = Some(flush_tx);
     }
 
+    pub fn stop_workers(self) -> JoinHandle<()> {
+        self.runtime_handle.spawn(async move {
+            if let Some(wal) = self.wal.into_inner() {
+                wal.shutdown().await;
+            }
+            if let Some(handle) = self.update_worker {
+                handle.await.unwrap();
+            }
+            if let Some(handle) = self.optimizer_worker {
+                handle.await.unwrap();
+            }
+            if let Some(handle) = self.flush_worker {
+                handle.await.unwrap();
+            }
+        })
+    }
+
     pub fn stop_flush_worker(&mut self) {
         if let Some(flush_stop) = self.flush_stop.take() {
             if let Err(()) = flush_stop.send(()) {
-                warn!("Failed to stop flush worker as it is already stopped.");
+                debug!("Failed to stop flush worker for shard {}", self.shard_path.display());
             }
         }
     }
 
-    /// Gracefully wait before all optimizations stop
-    /// If some optimization is in progress - it will be finished before shutdown.
     pub async fn wait_workers_stops(&mut self) -> CollectionResult<()> {
         let maybe_handle = self.update_worker.take();
         if let Some(handle) = maybe_handle {
@@ -226,24 +199,14 @@ impl UpdateHandler {
         if let Some(handle) = maybe_handle {
             handle.await?;
         }
-
-        let mut opt_handles_guard = self.optimization_handles.lock().await;
-        let opt_handles = std::mem::take(&mut *opt_handles_guard);
-        let stopping_handles = opt_handles
-            .into_iter()
-            .filter_map(|h| h.stop())
-            .collect_vec();
-
-        for res in stopping_handles {
-            res.await?;
-        }
-
+        Self::cleanup_optimization_handles(self.optimization_handles.clone()).await;
+        let mut update_clocks = self.clocks.highest.write().await;
+        update_clocks.apply_cutoff(self.clocks.cutoff.blocking_read());
+        let mut opt_handles_guard = self.optimization_handles.blocking_lock();
         Ok(())
     }
 
-    /// Checks if there are any failed operations.
-    /// If so - attempts to re-apply all failed operations.
-    async fn try_recover(segments: LockedSegmentHolder, wal: LockedWal) -> CollectionResult<usize> {
+    fn try_recover(segments: LockedSegmentHolder, wal: LockedWal) -> CollectionResult<usize> {
         // Try to re-apply everything starting from the first failed operation
         let first_failed_operation_option = segments.read().failed_operation.iter().cloned().min();
         match first_failed_operation_option {
@@ -255,17 +218,15 @@ impl UpdateHandler {
                         &segments,
                         op_num,
                         operation.operation,
-                        &HardwareCounterCell::disposable(), // Internal operation, no measurement needed
+                        &operation.hw_measurements.get_counter_cell(), // Internal operation, no measurement needed
                     )?;
                 }
             }
         };
+
         Ok(0)
     }
 
-    /// Checks conditions for all optimizers until there is no suggested segment
-    /// Starts a task for each optimization
-    /// Returns handles for started tasks
     pub(crate) fn launch_optimization<F>(
         optimizers: Arc<Vec<Arc<Optimizer>>>,
         optimizers_log: Arc<Mutex<TrackerLog>>,
@@ -274,13 +235,12 @@ impl UpdateHandler {
         segments: LockedSegmentHolder,
         callback: F,
         limit: Option<usize>,
-    ) -> Vec<StoppableTaskHandle<bool>>
-    where
+    ) where
         F: Fn(bool) + Send + Clone + Sync + 'static,
     {
         let mut scheduled_segment_ids = HashSet::<_>::default();
         let mut handles = vec![];
-
+        
         'outer: for optimizer in optimizers.iter() {
             loop {
                 // Return early if we reached the optimization job limit
@@ -301,7 +261,9 @@ impl UpdateHandler {
                 // And use same amount of IO threads as CPUs
                 let max_indexing_threads = optimizer.hnsw_config().max_indexing_threads;
                 let desired_io = num_rayon_threads(max_indexing_threads);
-                let Some(mut permit) = optimizer_resource_budget.try_acquire(0, desired_io) else {
+                let Some(mut permit) =
+                    optimizer_resource_budget.try_acquire(0, desired_io)
+                else {
                     // If there is no Resource budget, break outer loop and return early
                     // If we have no handles (no optimizations) trigger callback so that we wake up
                     // our optimization worker to try again later, otherwise it could get stuck
@@ -391,7 +353,7 @@ impl UpdateHandler {
                     },
                     // Panic handler
                     Some(Box::new(move |panic_payload| {
-                        let message = panic::downcast_str(&panic_payload).unwrap_or("");
+                        let message = common::panic::downcast_str(&panic_payload).unwrap_or("");
                         let separator = if !message.is_empty() { ": " } else { "" };
 
                         warn!(
@@ -507,40 +469,6 @@ impl UpdateHandler {
         handles.append(&mut new_handles);
     }
 
-    /// Cleanup finalized optimization task handles
-    ///
-    /// This finds and removes completed tasks from our list of optimization handles.
-    /// It also propagates any panics (and unknown errors) so we properly handle them if desired.
-    ///
-    /// It is essential to call this every once in a while for handling panics in time.
-    ///
-    /// Returns true if any optimization handle was finished, joined and removed.
-    async fn cleanup_optimization_handles(
-        optimization_handles: Arc<TokioMutex<Vec<StoppableTaskHandle<bool>>>>,
-    ) -> bool {
-        // Remove finished handles
-        let finished_handles: Vec<_> = {
-            let mut handles = optimization_handles.lock().await;
-            (0..handles.len())
-                .filter(|i| handles[*i].is_finished())
-                .collect::<Vec<_>>()
-                .into_iter()
-                .rev()
-                .map(|i| handles.swap_remove(i))
-                .collect()
-        };
-
-        let finished_any = !finished_handles.is_empty();
-
-        // Finalize all finished handles to propagate panics
-        for handle in finished_handles {
-            handle.join_and_handle_panic().await;
-        }
-
-        finished_any
-    }
-
-    #[allow(clippy::too_many_arguments)]
     async fn optimization_worker_fn(
         optimizers: Arc<Vec<Arc<Optimizer>>>,
         sender: Sender<OptimizerSignal>,
@@ -553,7 +481,6 @@ impl UpdateHandler {
         optimizer_resource_budget: ResourceBudget,
         max_handles: Option<usize>,
         has_triggered_optimizers: Arc<AtomicBool>,
-        payload_index_schema: Arc<SaveOnDisk<PayloadIndexSchema>>,
     ) {
         let max_handles = max_handles.unwrap_or(usize::MAX);
         let max_indexing_threads = optimizers
@@ -561,7 +488,6 @@ impl UpdateHandler {
             .map(|optimizer| optimizer.hnsw_config().max_indexing_threads)
             .unwrap_or_default();
 
-        // Asynchronous task to trigger optimizers once CPU budget is available again
         let mut resource_available_trigger: Option<JoinHandle<()>> = None;
 
         loop {
@@ -624,10 +550,7 @@ impl UpdateHandler {
                 continue;
             }
 
-            if Self::try_recover(segments.clone(), wal.clone())
-                .await
-                .is_err()
-            {
+            if Self::try_recover(segments.clone(), wal.clone()).is_err() {
                 continue;
             }
 
@@ -641,12 +564,14 @@ impl UpdateHandler {
                     .as_ref()
                     .is_some_and(|t| !t.is_finished());
                 if !trigger_active {
-                    resource_available_trigger.replace(trigger_optimizers_on_resource_budget(
-                        optimizer_resource_budget.clone(),
-                        desired_cpus,
-                        desired_io,
-                        sender.clone(),
-                    ));
+                    resource_available_trigger.replace(
+                        trigger_optimizers_on_resource_budget(
+                            optimizer_resource_budget.clone(),
+                            desired_cpus,
+                            desired_io,
+                            sender.clone(),
+                        ),
+                    );
                 }
                 continue;
             }
@@ -671,8 +596,7 @@ impl UpdateHandler {
                 &optimizer_resource_budget,
                 sender.clone(),
                 limit,
-            )
-            .await;
+            ).await;
         }
     }
 
@@ -714,14 +638,17 @@ impl UpdateHandler {
                         Ok(update_res) => optimize_sender
                             .send(OptimizerSignal::Operation(op_num))
                             .await
-                            .and(Ok(update_res))
-                            .map_err(|send_err| send_err.into()),
+                            .map(|()| update_res)
+                            .map_err(|err| {
+                                CollectionError::service_error(format!("{err}"))
+                            }),
                         Err(err) => Err(err),
                     };
-
                     if let Some(feedback) = sender {
                         feedback.send(res).unwrap_or_else(|_| {
-                            debug!("Can't report operation {op_num} result. Assume already not required");
+                            debug!(
+                                "Can't report operation {op_num} result. Assume already not required",
+                            );
                         });
                     };
                 }
@@ -732,14 +659,16 @@ impl UpdateHandler {
                         .unwrap_or_else(|_| debug!("Optimizer already stopped"));
                     break;
                 }
-                UpdateSignal::Nop => optimize_sender
-                    .send(OptimizerSignal::Nop)
-                    .await
-                    .unwrap_or_else(|_| {
-                        info!(
-                            "Can't notify optimizers, assume process is dead. Restart is required"
-                        );
-                    }),
+                UpdateSignal::Nop => {
+                    optimize_sender
+                        .send(OptimizerSignal::Nop)
+                        .await
+                        .unwrap_or_else(|_| {
+                            debug!(
+                                "Can't notify optimizers, assume process is dead. Restart is required"
+                            );
+                        })
+                }
                 UpdateSignal::Plunger(callback_sender) => {
                     callback_sender.send(()).unwrap_or_else(|_| {
                         debug!("Can't notify sender, assume nobody is waiting anymore");
@@ -747,7 +676,7 @@ impl UpdateHandler {
                 }
             }
         }
-        // Transmitter was destroyed
+
         optimize_sender
             .send(OptimizerSignal::Stop)
             .await
@@ -764,15 +693,13 @@ impl UpdateHandler {
         shard_path: PathBuf,
     ) {
         loop {
-            // Stop flush worker on signal or if sender was dropped
-            // Even if timer did not finish
             tokio::select! {
                 _ = tokio::time::sleep(Duration::from_secs(flush_interval_sec)) => {},
                 _ = &mut stop_receiver => {
                     debug!("Stopping flush worker for shard {}", shard_path.display());
                     return;
                 }
-            }
+            };
 
             trace!("Attempting flushing");
             let wal_flash_job = wal.lock().await.flush_async();
@@ -802,7 +729,7 @@ impl UpdateHandler {
             // This is to prevent truncating WAL entries that other bits of code still depend on
             // such as the queue proxy shard.
             // Default keep_from is `u64::MAX` to allow acknowledging all confirmed.
-            let keep_from = wal_keep_from.load(std::sync::atomic::Ordering::Relaxed);
+            let keep_from = wal_keep_from.load(Ordering::Relaxed);
 
             // If we should keep the first message, do not acknowledge at all
             if keep_from == 0 {
@@ -823,9 +750,6 @@ impl UpdateHandler {
         }
     }
 
-    /// Returns confirmed version after flush of all segments
-    ///
-    /// # Errors
     /// Returns an error on flush failure
     fn flush_segments(segments: LockedSegmentHolder) -> OperationResult<SeqNumberType> {
         let read_segments = segments.read();
@@ -835,25 +759,4 @@ impl UpdateHandler {
             Some(failed_operation) => min(failed_operation, flushed_version),
         })
     }
-}
-
-/// Trigger optimizers when CPU budget is available
-fn trigger_optimizers_on_resource_budget(
-    optimizer_resource_budget: ResourceBudget,
-    desired_cpus: usize,
-    desired_io: usize,
-    sender: Sender<OptimizerSignal>,
-) -> JoinHandle<()> {
-    task::spawn(async move {
-        log::trace!("Skipping optimization checks, waiting for CPU budget to be available");
-        optimizer_resource_budget
-            .notify_on_budget_available(desired_cpus, desired_io)
-            .await;
-        log::trace!("Continue optimization checks, new CPU budget available");
-
-        // Trigger optimizers with Nop operation
-        sender.send(OptimizerSignal::Nop).await.unwrap_or_else(|_| {
-            log::info!("Can't notify optimizers, assume process is dead. Restart is required")
-        });
-    })
 }
\ No newline at end of file
