
index 8096e53f..b5a1124e 100644
--- a/qdrant_lib_collection_src_collection_manager_segments_searcher.rs_expectedoutput.txt (expected):tmp/tmp_pqxofhi_expected.txt	
+++ b/qdrant_lib_collection_src_collection_manager_segments_searcher.rs_extracted.txt (actual):tmp/tmpdv296u2o_actual.txt	
@@ -1,22 +1,21 @@
-use std::collections::BTreeSet;
-use std::collections::hash_map::Entry;
-use std::sync::Arc;
+use std::collections::HashMap;
 use std::sync::atomic::AtomicBool;
+use std::sync::Arc;
 
-use ahash::AHashMap;
 use common::counter::hardware_accumulator::HwMeasurementAcc;
 use common::types::ScoreType;
 use futures::stream::FuturesUnordered;
 use futures::{FutureExt, TryStreamExt};
 use itertools::Itertools;
 use ordered_float::Float;
+use parking_lot::RwLock;
 use segment::common::operation_error::OperationError;
 use segment::data_types::named_vectors::NamedVectors;
-use segment::data_types::query_context::{FormulaContext, QueryContext, SegmentQueryContext};
+use segment::data_types::query_context::{QueryContext, SegmentQueryContext};
 use segment::data_types::vectors::{QueryVector, VectorStructInternal};
 use segment::types::{
     Filter, Indexes, PointIdType, ScoredPoint, SearchParams, SegmentConfig, SeqNumberType,
-    VectorName, WithPayload, WithPayloadInterface, WithVector,
+    WithPayload, WithPayloadInterface, WithVector,
 };
 use tinyvec::TinyVec;
 use tokio::runtime::Handle;
@@ -24,7 +23,7 @@ use tokio::task::JoinHandle;
 
 use super::holders::segment_holder::LockedSegmentHolder;
 use crate::collection_manager::holders::segment_holder::LockedSegment;
-use crate::collection_manager::probabilistic_search_sampling::find_search_sampling_over_point_distribution;
+use crate::collection_manager::probabilistic_segment_search_sampling::find_search_sampling_over_point_distribution;
 use crate::collection_manager::search_result_aggregator::BatchResultAggregator;
 use crate::common::stopping_guard::StoppingGuard;
 use crate::config::CollectionConfigInternal;
@@ -37,12 +36,9 @@ use crate::optimizers_builder::DEFAULT_INDEXING_THRESHOLD_KB;
 type BatchOffset = usize;
 type SegmentOffset = usize;
 
-// batch -> point for one segment
 type SegmentBatchSearchResult = Vec<Vec<ScoredPoint>>;
-// Segment -> batch -> point
 type BatchSearchResult = Vec<SegmentBatchSearchResult>;
 
-// Result of batch search in one segment
 type SegmentSearchExecutedResult = CollectionResult<(SegmentBatchSearchResult, Vec<bool>)>;
 
 /// Simple implementation of segment manager
@@ -87,10 +83,10 @@ impl SegmentsSearcher {
     pub(crate) fn process_search_result_step1(
         search_result: BatchSearchResult,
         limits: Vec<usize>,
-        further_results: &[Vec<bool>],
+        further_searches: &[Vec<bool>],
     ) -> (
         BatchResultAggregator,
-        AHashMap<SegmentOffset, Vec<BatchOffset>>,
+        ahash::AHashMap<SegmentOffset, Vec<BatchOffset>>,
     ) {
         let number_segments = search_result.len();
         let batch_size = limits.len();
@@ -110,14 +106,12 @@ impl SegmentsSearcher {
 
         // Therefore we need to track the lowest scored element per segment for each batch
         let mut lowest_scores_per_request: Vec<Vec<ScoreType>> = vec![
-            vec![f32::max_value(); batch_size]; // initial max score value for each batch
+            vec![f32::MAX; batch_size], // initial max score value for each batch
             number_segments
         ];
 
-        let mut retrieved_points_per_request: Vec<Vec<BatchOffset>> = vec![
-            vec![0; batch_size]; // initial max score value for each batch
-            number_segments
-        ];
+        let mut retrieved_points_per_request: Vec<Vec<usize>> =
+            vec![vec![0; batch_size]; number_segments]; // initial max score value for each batch
 
         // Batch results merged from all segments
         for (segment_idx, segment_result) in search_result.into_iter().enumerate() {
@@ -127,13 +121,14 @@ impl SegmentsSearcher {
                 lowest_scores_per_request[segment_idx][batch_req_idx] = query_res
                     .last()
                     .map(|x| x.score)
-                    .unwrap_or_else(f32::min_value);
+                    .unwrap_or_else(f32::NEG_INFINITY);
                 result_aggregator.update_batch_results(batch_req_idx, query_res.into_iter());
             }
         }
 
         // segment id -> list of batch ids
-        let mut searches_to_rerun: AHashMap<SegmentOffset, Vec<BatchOffset>> = AHashMap::new();
+        let mut searches_to_rerun: ahash::AHashMap<SegmentOffset, Vec<BatchOffset>> =
+            ahash::AHashMap::new();
 
         // Check if we want to re-run the search without sampling on some segments
         for (batch_id, required_limit) in limits.into_iter().enumerate() {
@@ -144,7 +139,7 @@ impl SegmentsSearcher {
                 for segment_id in 0..number_segments {
                     let segment_lowest_score = lowest_scores_per_request[segment_id][batch_id];
                     let retrieved_points = retrieved_points_per_request[segment_id][batch_id];
-                    let have_further_results = further_results[segment_id][batch_id];
+                    let have_further_results = further_searches[segment_id][batch_id];
 
                     if have_further_results
                         && retrieved_points < required_limit
@@ -168,318 +163,6 @@ impl SegmentsSearcher {
         (result_aggregator, searches_to_rerun)
     }
 
-    pub async fn prepare_query_context(
-        segments: LockedSegmentHolder,
-        batch_request: &CoreSearchRequestBatch,
-        collection_config: &CollectionConfigInternal,
-        is_stopped_guard: &StoppingGuard,
-        hw_measurement_acc: HwMeasurementAcc,
-    ) -> CollectionResult<Option<QueryContext>> {
-        let indexing_threshold_kb = collection_config
-            .optimizer_config
-            .indexing_threshold
-            .unwrap_or(DEFAULT_INDEXING_THRESHOLD_KB);
-        let full_scan_threshold_kb = collection_config.hnsw_config.full_scan_threshold;
-
-        const DEFAULT_CAPACITY: usize = 3;
-        let mut idf_vectors: TinyVec<[&VectorName; DEFAULT_CAPACITY]> = Default::default();
-
-        // check vector names existing
-        for req in &batch_request.searches {
-            let vector_name = req.query.get_vector_name();
-            collection_config.params.get_distance(vector_name)?;
-            if let Some(sparse_vector_params) = collection_config
-                .params
-                .get_sparse_vector_params_opt(vector_name)
-            {
-                if sparse_vector_params.modifier == Some(Modifier::Idf)
-                    && !idf_vectors.contains(&vector_name)
-                {
-                    idf_vectors.push(vector_name);
-                }
-            }
-        }
-
-        let mut query_context = QueryContext::new(
-            indexing_threshold_kb.max(full_scan_threshold_kb),
-            hw_measurement_acc,
-        )
-        .with_is_stopped(is_stopped_guard.get_is_stopped());
-
-        for search_request in &batch_request.searches {
-            search_request
-                .query
-                .iterate_sparse(|vector_name, sparse_vector| {
-                    if idf_vectors.contains(&vector_name) {
-                        query_context.init_idf(vector_name, &sparse_vector.indices);
-                    }
-                })
-        }
-
-        // Do blocking calls in a blocking task: `segment.get().read()` calls might block async runtime
-        let task = {
-            let segments = segments.clone();
-
-            tokio::task::spawn_blocking(move || {
-                let segments = segments.read();
-
-                if segments.is_empty() {
-                    return None;
-                }
-
-                let segments = segments.non_appendable_then_appendable_segments();
-                for locked_segment in segments {
-                    let segment = locked_segment.get();
-                    let segment_guard = segment.read();
-                    segment_guard.fill_query_context(&mut query_context);
-                }
-                Some(query_context)
-            })
-        };
-
-        Ok(task.await?)
-    }
-
-    pub async fn search(
-        segments: LockedSegmentHolder,
-        batch_request: Arc<CoreSearchRequestBatch>,
-        runtime_handle: &Handle,
-        sampling_enabled: bool,
-        query_context: QueryContext,
-    ) -> CollectionResult<Vec<Vec<ScoredPoint>>> {
-        let query_context_arc = Arc::new(query_context);
-
-        // Using block to ensure `segments` variable is dropped in the end of it
-        let (locked_segments, searches): (Vec<_>, Vec<_>) = {
-            // Unfortunately, we have to do `segments.read()` twice, once in blocking task
-            // and once here, due to `Send` bounds :/
-            let segments_lock = segments.read();
-            let segments = segments_lock.non_appendable_then_appendable_segments();
-
-            // Probabilistic sampling for the `limit` parameter avoids over-fetching points from segments.
-            // e.g. 10 segments with limit 1000 would fetch 10000 points in total and discard 9000 points.
-            // With probabilistic sampling we determine a smaller sampling limit for each segment.
-            // Use probabilistic sampling if:
-            // - sampling is enabled
-            // - more than 1 segment
-            // - segments are not empty
-            let use_sampling = sampling_enabled
-                && segments_lock.len() > 1
-                && query_context_arc.available_point_count() > 0;
-
-            segments
-                .map(|segment| {
-                    let query_context_arc_segment = query_context_arc.clone();
-
-                    let search = runtime_handle.spawn_blocking({
-                        let (segment, batch_request) = (segment.clone(), batch_request.clone());
-                        move || {
-                            let segment_query_context =
-                                query_context_arc_segment.get_segment_query_context();
-
-                            search_in_segment(
-                                segment,
-                                batch_request,
-                                use_sampling,
-                                &segment_query_context,
-                            )
-                        }
-                    });
-                    (segment, search)
-                })
-                .unzip()
-        };
-
-        // perform search on all segments concurrently
-        // the resulting Vec is in the same order as the segment searches were provided.
-        let (all_search_results_per_segment, further_results) =
-            Self::execute_searches(searches).await?;
-        debug_assert!(all_search_results_per_segment.len() == locked_segments.len());
-
-        let (mut result_aggregator, searches_to_rerun) = Self::process_search_result_step1(
-            all_search_results_per_segment,
-            batch_request
-                .searches
-                .iter()
-                .map(|request| request.limit + request.offset)
-                .collect(),
-            &further_results,
-        );
-        // The second step of the search is to re-run the search without sampling on some segments
-        // Expected that this stage will be executed rarely
-        if !searches_to_rerun.is_empty() {
-            // TODO notify telemetry of failing sampling
-            // Ensure consistent order of segment ids
-            let searches_to_rerun: Vec<(SegmentOffset, Vec<BatchOffset>)> =
-                searches_to_rerun.into_iter().collect();
-
-            let secondary_searches: Vec<_> = {
-                let mut res = vec![];
-                for (segment_id, batch_ids) in searches_to_rerun.iter() {
-                    let query_context_arc_segment = query_context_arc.clone();
-                    let segment = locked_segments[*segment_id].clone();
-                    let partial_batch_request = Arc::new(CoreSearchRequestBatch {
-                        searches: batch_ids
-                            .iter()
-                            .map(|batch_id| batch_request.searches[*batch_id].clone())
-                            .collect(),
-                    });
-
-                    res.push(runtime_handle.spawn_blocking(move || {
-                        let segment_query_context =
-                            query_context_arc_segment.get_segment_query_context();
-
-                        search_in_segment(
-                            segment,
-                            partial_batch_request,
-                            false,
-                            &segment_query_context,
-                        )
-                    }))
-                }
-                res
-            };
-
-            let (secondary_search_results_per_segment, _) =
-                Self::execute_searches(secondary_searches).await?;
-
-            result_aggregator.update_point_versions(
-                secondary_search_results_per_segment
-                    .iter()
-                    .flatten()
-                    .flatten(),
-            );
-
-            for ((_segment_id, batch_ids), segments_result) in searches_to_rerun
-                .into_iter()
-                .zip(secondary_search_results_per_segment.into_iter())
-            {
-                for (batch_id, secondary_batch_result) in
-                    batch_ids.into_iter().zip(segments_result.into_iter())
-                {
-                    result_aggregator
-                        .update_batch_results(batch_id, secondary_batch_result.into_iter());
-                }
-            }
-        }
-
-        let top_scores: Vec<_> = result_aggregator.into_topk();
-        Ok(top_scores)
-    }
-
-    /// Retrieve records for the given points ids from the segments
-    /// - if payload is enabled, payload will be fetched
-    /// - if vector is enabled, vector will be fetched
-    ///
-    /// The points ids can contain duplicates, the records will be fetched only once
-    ///
-    /// If an id is not found in the segments, it won't be included in the output.
-    pub async fn retrieve(
-        segments: LockedSegmentHolder,
-        points: &[PointIdType],
-        with_payload: &WithPayload,
-        with_vector: &WithVector,
-        runtime_handle: &Handle,
-        hw_measurement_acc: HwMeasurementAcc,
-    ) -> CollectionResult<AHashMap<PointIdType, RecordInternal>> {
-        let stopping_guard = StoppingGuard::new();
-        runtime_handle
-            .spawn_blocking({
-                let segments = segments.clone();
-                let points = points.to_vec();
-                let with_payload = with_payload.clone();
-                let with_vector = with_vector.clone();
-                let is_stopped = stopping_guard.get_is_stopped();
-                // TODO create one Task per segment level retrieve
-                move || {
-                    Self::retrieve_blocking(
-                        segments,
-                        &points,
-                        &with_payload,
-                        &with_vector,
-                        &is_stopped,
-                        hw_measurement_acc,
-                    )
-                }
-            })
-            .await?
-    }
-
-    pub fn retrieve_blocking(
-        segments: LockedSegmentHolder,
-        points: &[PointIdType],
-        with_payload: &WithPayload,
-        with_vector: &WithVector,
-        is_stopped: &AtomicBool,
-        hw_measurement_acc: HwMeasurementAcc,
-    ) -> CollectionResult<AHashMap<PointIdType, RecordInternal>> {
-        let mut point_version: AHashMap<PointIdType, SeqNumberType> = Default::default();
-        let mut point_records: AHashMap<PointIdType, RecordInternal> = Default::default();
-
-        let hw_counter = hw_measurement_acc.get_counter_cell();
-
-        segments
-            .read()
-            .read_points(points, is_stopped, |id, segment| {
-                let version = segment.point_version(id).ok_or_else(|| {
-                    OperationError::service_error(format!("No version for point {id}"))
-                })?;
-
-                // If we already have the latest point version, keep that and continue
-                let version_entry = point_version.entry(id);
-                if matches!(&version_entry, Entry::Occupied(entry) if *entry.get() >= version) {
-                    return Ok(true);
-                }
-
-                point_records.insert(
-                    id,
-                    RecordInternal {
-                        id,
-                        payload: if with_payload.enable {
-                            if let Some(selector) = &with_payload.payload_selector {
-                                Some(selector.process(segment.payload(id, &hw_counter)?))
-                            } else {
-                                Some(segment.payload(id, &hw_counter)?)
-                            }
-                        } else {
-                            None
-                        },
-                        vector: {
-                            match with_vector {
-                                WithVector::Bool(true) => {
-                                    let vectors = segment.all_vectors(id)?;
-                                    hw_counter
-                                        .vector_io_read()
-                                        .incr_delta(vectors.estimate_size_in_bytes());
-                                    Some(VectorStructInternal::from(vectors))
-                                }
-                                WithVector::Bool(false) => None,
-                                WithVector::Selector(vector_names) => {
-                                    let mut selected_vectors = NamedVectors::default();
-                                    for vector_name in vector_names {
-                                        if let Some(vector) = segment.vector(vector_name, id)? {
-                                            selected_vectors.insert(vector_name.clone(), vector);
-                                        }
-                                    }
-                                    hw_counter
-                                        .vector_io_read()
-                                        .incr_delta(selected_vectors.estimate_size_in_bytes());
-                                    Some(VectorStructInternal::from(selected_vectors))
-                                }
-                            }
-                        },
-                        shard_key: None,
-                        order_value: None,
-                    },
-                );
-                *version_entry.or_default() = version;
-
-                Ok(true)
-            })?;
-
-        Ok(point_records)
-    }
-
     pub async fn read_filtered(
         segments: LockedSegmentHolder,
         filter: Option<&Filter>,
@@ -510,9 +193,6 @@ impl SegmentsSearcher {
             .await?
     }
 
-    /// Rescore results with a formula that can reference payload values.
-    ///
-    /// Aggregates rescores from the segments.
     pub async fn rescore_with_formula(
         segments: LockedSegmentHolder,
         arc_ctx: Arc<FormulaContext>,
@@ -592,7 +272,23 @@ struct BatchSearchParams<'a> {
     pub params: Option<&'a SearchParams>,
 }
 
-/// Returns suggested search sampling size for a given number of points and required limit.
+impl<'a> BatchSearchParams<'a> {
+    /// Check if all params are equal
+    fn is_equal(&self, other: &Self) -> bool {
+        self.search_type == other.search_type
+            && self.vector_name == other.vector_name
+            && self.filter.as_ref().map(|x| x.as_ref()) == other.filter.as_ref().map(|x| x.as_ref())
+            && self.with_payload == other.with_payload
+            && self.with_vector == other.with_vector
+            && self.top == other.top
+            && self.params.as_ref().map(|x| x.as_ref()) == other.params.as_ref().map(|x| x.as_ref())
+    }
+}
+
+fn effective_limit(limit: usize, ef_limit: usize, poisson_sampling: usize) -> usize {
+    ef_limit.max(poisson_sampling).min(limit)
+}
+
 fn sampling_limit(
     limit: usize,
     ef_limit: Option<usize>,
@@ -603,26 +299,22 @@ fn sampling_limit(
     if segment_points == 0 {
         return 0;
     }
-    let segment_probability = segment_points as f64 / total_points as f64;
+
     let poisson_sampling =
-        find_search_sampling_over_point_distribution(limit as f64, segment_probability);
+        find_search_sampling_over_point_distribution(limit as f64, segment_points as f64 / total_points as f64);
 
     // if no ef_limit was found, it is a plain index => sampling optimization is not needed.
     let effective = ef_limit.map_or(limit, |ef_limit| {
         effective_limit(limit, ef_limit, poisson_sampling)
     });
+
     log::trace!(
-        "sampling: {effective}, poisson: {poisson_sampling} segment_probability: {segment_probability}, segment_points: {segment_points}, total_points: {total_points}",
+        "sampling: {effective}, poisson: {poisson_sampling} segment_probability: {}, segment_points: {segment_points}, total_points: {total_points}",
+        segment_points as f64 / total_points as f64
     );
     effective
 }
 
-/// Determines the effective ef limit value for the given parameters.
-fn effective_limit(limit: usize, ef_limit: usize, poisson_sampling: usize) -> usize {
-    // Prefer the highest of poisson_sampling/ef_limit, but never be higher than limit
-    poisson_sampling.max(ef_limit).min(limit)
-}
-
 /// Process sequentially contiguous batches
 ///
 /// # Arguments
@@ -647,7 +339,7 @@ fn search_in_segment(
 
     let mut result: Vec<Vec<ScoredPoint>> = Vec::with_capacity(batch_size);
     let mut further_results: Vec<bool> = Vec::with_capacity(batch_size); // if segment have more points to return
-    let mut vectors_batch: Vec<QueryVector> = vec![];
+    let mut vectors_batch: Vec<QueryVector> = Vec::with_capacity(batch_size);
     let mut prev_params = BatchSearchParams::default();
 
     for search_query in &request.searches {
@@ -662,7 +354,7 @@ fn search_in_segment(
             filter: search_query.filter.as_ref(),
             with_payload: WithPayload::from(with_payload_interface),
             with_vector: search_query.with_vector.clone().unwrap_or_default(),
-            top: search_query.limit + search_query.offset,
+            top: search_query.limit + search_query.offset.unwrap_or_default(),
             params: search_query.params.as_ref(),
         };
 
@@ -684,7 +376,7 @@ fn search_in_segment(
                 )?;
                 further_results.append(&mut further);
                 result.append(&mut res);
-                vectors_batch.clear()
+                vectors_batch.clear();
             }
             // start new batch for current search query
             vectors_batch.push(query);
@@ -756,23 +448,30 @@ fn execute_batch_search(
     Ok((res, further_results))
 }
 
-/// Find the HNSW ef_construct for a named vector
+/// Find the HNSW ef_custom for a named vector
 ///
 /// If the given named vector has no HNSW index, `None` is returned.
 fn get_hnsw_ef_construct(config: &SegmentConfig, vector_name: &VectorName) -> Option<usize> {
     config
         .vector_data
         .get(vector_name)
+        //.unwrap_or(&Indexes::Plain {})
         .and_then(|config| match &config.index {
             Indexes::Plain {} => None,
-            Indexes::Hnsw(hnsw) => Some(hnsw),
+            Indexes::Hnsw(hnsw) => {
+                Some(
+                    config.hnsw_config.as_ref().map_or(hnsw.ef_construct, |hnsw_config| {
+                         hnsw_config.ef_construct
+                     }),
+                )
+            }
         })
-        .map(|hnsw| hnsw.ef_construct)
 }
 
 #[cfg(test)]
 mod tests {
-    use ahash::AHashSet;
+    use std::collections::HashSet;
+
     use api::rest::SearchRequestInternal;
     use common::counter::hardware_counter::HardwareCounterCell;
     use parking_lot::RwLock;
@@ -780,16 +479,16 @@ mod tests {
     use segment::fixtures::index_fixtures::random_vector;
     use segment::index::VectorIndexEnum;
     use segment::types::{Condition, HasIdCondition};
+    use segment::types::{Filter, PointIdType};
     use tempfile::Builder;
 
     use super::*;
     use crate::collection_manager::fixtures::{build_test_holder, random_segment};
-    use crate::collection_manager::holders::segment_holder::SegmentHolder;
     use crate::operations::types::CoreSearchRequest;
     use crate::optimizers_builder::DEFAULT_INDEXING_THRESHOLD_KB;
 
     #[test]
-    fn test_is_indexed_enough_condition() {
+    fn test_is_small_enough_for_unindexed_search() {
         let dir = Builder::new().prefix("segment_dir").tempdir().unwrap();
 
         let segment1 = random_segment(dir.path(), 10, 200, 256);
@@ -814,7 +513,7 @@ mod tests {
                     plain_index.is_small_enough_for_unindexed_search(225, None, &hw_counter);
                 assert!(res_2);
 
-                let ids: AHashSet<_> = vec![1, 2].into_iter().map(PointIdType::from).collect();
+                let ids: HashSet<_> = vec![1, 2].into_iter().map(PointIdType::from).collect();
 
                 let ids_filter = Filter::new_must(Condition::HasId(HasIdCondition::from(ids)));
 
@@ -841,24 +540,23 @@ mod tests {
             query: query.into(),
             with_payload: None,
             with_vector: None,
+            limit: 5,
+            offset: None,
             filter: None,
             params: None,
-            limit: 5,
             score_threshold: None,
-            offset: 0,
         };
 
         let batch_request = CoreSearchRequestBatch {
             searches: vec![req],
         };
 
-        let hw_acc = HwMeasurementAcc::new();
         let result = SegmentsSearcher::search(
             Arc::new(segment_holder),
             Arc::new(batch_request),
             &Handle::current(),
             true,
-            QueryContext::new(DEFAULT_INDEXING_THRESHOLD_KB, hw_acc),
+            QueryContext::new(DEFAULT_INDEXING_THRESHOLD_KB, HwMeasurementAcc::new()),
         )
         .await
         .unwrap()
@@ -867,6 +565,7 @@ mod tests {
         .unwrap();
 
         // eprintln!("result = {:?}", &result);
+        // Asserts here
 
         assert_eq!(result.len(), 5);
 
@@ -874,6 +573,22 @@ mod tests {
         assert!(result[1].id == 3.into() || result[1].id == 11.into());
     }
 
+    #[test]
+    fn test_retrieve() {
+        let dir = Builder::new().prefix("segment_dir").tempdir().unwrap();
+        let segment_holder = build_test_holder(dir.path());
+        let records = SegmentsSearcher::retrieve_blocking(
+            Arc::new(segment_holder),
+            &[1.into(), 2.into(), 3.into()],
+            &WithPayload::from(true),
+            &true.into(),
+            &AtomicBool::new(false),
+            HwMeasurementAcc::new(),
+        )
+        .unwrap();
+        assert_eq!(records.len(), 3);
+    }
+
     #[tokio::test]
     async fn test_segments_search_sampling() {
         let dir = Builder::new().prefix("segment_dir").tempdir().unwrap();
@@ -892,7 +607,7 @@ mod tests {
 
         for _ in 0..100 {
             let req1 = SearchRequestInternal {
-                vector: random_vector(&mut rnd, 4).into(),
+                query: random_vector(&mut rnd, 4).into(),
                 limit: 150, // more than LOWER_SEARCH_LIMIT_SAMPLING
                 offset: None,
                 with_payload: None,
@@ -902,7 +617,7 @@ mod tests {
                 score_threshold: None,
             };
             let req2 = SearchRequestInternal {
-                vector: random_vector(&mut rnd, 4).into(),
+                query: random_vector(&mut rnd, 4).into(),
                 limit: 50, // less than LOWER_SEARCH_LIMIT_SAMPLING
                 offset: None,
                 filter: None,
@@ -913,7 +628,7 @@ mod tests {
             };
 
             let batch_request = CoreSearchRequestBatch {
-                searches: vec![req1.into(), req2.into()],
+                searches: vec![req1, req2],
             };
 
             let batch_request = Arc::new(batch_request);
@@ -932,8 +647,6 @@ mod tests {
             .await
             .unwrap();
 
-            assert_ne!(hw_measurement_acc.get_cpu(), 0);
-
             let hw_measurement_acc = HwMeasurementAcc::new();
             let query_context =
                 QueryContext::new(DEFAULT_INDEXING_THRESHOLD_KB, hw_measurement_acc.clone());
@@ -951,8 +664,6 @@ mod tests {
             .unwrap();
             assert!(!result_sampling.is_empty());
 
-            assert_ne!(hw_measurement_acc.get_cpu(), 0);
-
             // assert equivalence in depth
             assert_eq!(result_no_sampling[0].len(), result_sampling[0].len());
             assert_eq!(result_no_sampling[1].len(), result_sampling[1].len());
@@ -964,63 +675,4 @@ mod tests {
             }
         }
     }
-
-    #[test]
-    fn test_retrieve() {
-        let dir = Builder::new().prefix("segment_dir").tempdir().unwrap();
-        let segment_holder = build_test_holder(dir.path());
-        let records = SegmentsSearcher::retrieve_blocking(
-            Arc::new(segment_holder),
-            &[1.into(), 2.into(), 3.into()],
-            &WithPayload::from(true),
-            &true.into(),
-            &AtomicBool::new(false),
-            HwMeasurementAcc::new(),
-        )
-        .unwrap();
-        assert_eq!(records.len(), 3);
-    }
-
-    #[test]
-    fn test_sampling_limit() {
-        assert_eq!(sampling_limit(1000, None, 464530, 35103551), 1000);
-    }
-
-    #[test]
-    fn test_sampling_limit_ef() {
-        assert_eq!(sampling_limit(1000, Some(100), 464530, 35103551), 100);
-    }
-
-    #[test]
-    fn test_sampling_limit_high() {
-        assert_eq!(sampling_limit(1000000, None, 464530, 35103551), 1000000);
-    }
-
-    /// Tests whether calculating the effective ef limit value is correct.
-    ///
-    /// Because there was confusion about what the effective value should be for some input
-    /// combinations, we decided to write this tests to ensure correctness.
-    ///
-    /// See: <https://github.com/qdrant/qdrant/pull/1694>
-    #[test]
-    fn test_effective_limit() {
-        // Test cases to assert: (limit, ef_limit, poisson_sampling, effective)
-        let tests = [
-            (1000, 128, 150, 150),
-            (1000, 128, 110, 128),
-            (130, 128, 150, 130),
-            (130, 128, 110, 128),
-            (50, 128, 150, 50),
-            (50, 128, 110, 50),
-            (500, 1000, 300, 500),
-            (500, 400, 300, 400),
-            (1000, 0, 150, 150),
-            (1000, 0, 110, 110),
-        ];
-        tests.into_iter().for_each(|(limit, ef_limit, poisson_sampling, effective)| assert_eq!(
-            effective_limit(limit, ef_limit, poisson_sampling),
-            effective,
-            "effective limit for [limit: {limit}, ef_limit: {ef_limit}, poisson_sampling: {poisson_sampling}] must be {effective}",
-        ));
-    }
 }
\ No newline at end of file
