use std::collections::{HashMap, HashSet};
use std::fs::{create_dir_all, File, remove_file};
use std::path::{Path, PathBuf};
use std::sync::Arc;

use atomic_refcell::AtomicRefCell;
use crate::index::field_index::{CardinalityEstimation, FieldIndex};
use crate::index::payload_config::PayloadConfig;
use crate::payload_storage::payload_storage::{ConditionChecker, PayloadStorage};
use crate::index::field_index::field_index::PayloadFieldIndex;
use crate::index::field_index::index_selector::index_selector;
use crate::index::field_index::numeric_index::PersistedNumericIndex;
use crate::types::{Filter, PayloadKeyType, FieldCondition};
use crate::entry::entry_point::{OperationResult, OperationError};
use std::collections::HashMap as HashMap2;
use std::fs as std_fs;
use std::io::Error;
use std::path::Path;
use std::sync::Arc;
use atomic_refcell::AtomicRefCell;
use itertools::Itertools;
use log::debug;

pub const PAYLOAD_FIELD_INDEX_PATH: &str = "fields";

type IndexesMap = HashMap<PayloadKeyType, Vec<FieldIndex>>;

pub struct StructPayloadIndex {
    condition_checker: Arc<AtomicRefCell<dyn ConditionChecker>>,
    vector_storage: Arc<AtomicRefCell<dyn VectorStorage>>,
    payload: Arc<AtomicRefCell<dyn PayloadStorage>>,
    id_mapper: Arc<AtomicRefCell<dyn IdMapper>>,
    field_indexes: IndexesMap,
    config: PayloadConfig,
    path: PathBuf,
    total_points: usize,
}

impl StructPayloadIndex {
    pub fn estimate_field_condition(&self, condition: &FieldCondition) -> Option<CardinalityEstimation> {
        self.field_indexes.get(&condition.key).and_then(|indexes| {
            let mut result_estimation: Option<CardinalityEstimation> = None;
            for index in indexes {
                result_estimation = index.estimate_cardinality(condition);
                if result_estimation.is_some() {
                    break;
                }
            }
            result_estimation
        })
    }

    fn config_path(&self) -> PathBuf {
        PayloadConfig::get_config_path(&self.path)
    }

    fn save_config(&self) -> OperationResult<()> {
        let config_path = self.config_path();
        self.config.save(&config_path)
    }

    fn get_field_index_dir(path: &Path) -> PathBuf {
        path.join(PAYLOAD_FIELD_INDEX_PATH)
    }

    fn get_field_index_path(path: &Path, field: &PayloadKeyType) -> PathBuf {
        Self::get_field_index_dir(path).join(format!("{}.idx", field))
    }

    fn save_field_index(&self, field: &PayloadKeyType) -> OperationResult<()> {
        let field_index_dir = Self::get_field_index_dir(&self.path);
        let field_index_path = Self::get_field_index_path(&self.path, field);
        create_dir_all(field_index_dir)?;

        match self.field_indexes.get(field) {
            None => {},
            Some(indexes) => {
                let file = File::create(&field_index_path)?;
                serde_cbor::to_writer(file, indexes).map_err(|err| {
                    OperationError::ServiceError {
                        description: format!("Unable to save index: {:?}", err),
                    }
                })?;
            }
        }
        Ok(())
    }

    fn load_or_build_field_index(&self, field: &PayloadKeyType, payload_type: PayloadSchemaType) -> OperationResult<Vec<FieldIndex>> {
        let field_index_path = Self::get_field_index_path(&self.path, field);
        if field_index_path.exists() {
            debug!("Loading field `{}` index from {}", field, field_index_path.to_str().unwrap());
            let file = File::open(field_index_path)?;
            let field_indexes: Vec<FieldIndex> = serde_cbor::from_reader(file).map_err(|err| {
                OperationError::ServiceError { description: format!("Unable to load index: {:?}", err) }
            })?;

            Ok(field_indexes)
        } else {
            debug!("Index for field `{}` not found in {}, building now", field, field_index_path.to_str().unwrap());
            let res = self.build_field_index(field, payload_type, &HardwareCounterCell::disposable())?; // Internal operation.
            self.save_field_index(field)?;
            Ok(res)
        }
    }

    fn load_all_fields(&mut self) -> OperationResult<()> {
        let mut field_indexes: IndexesMap = Default::default();

        for (field, payload_schema) in &self.config.indexed_fields {
            let field_index = self.load_or_build_field_index(field, payload_schema.to_owned())?;
            field_indexes.insert(field.clone(), field_index);
        }
        self.field_indexes = field_indexes;
        Ok(())
    }

    pub fn open(
        payload: Arc<AtomicRefCell<PayloadStorageEnum>>,
        id_tracker: Arc<AtomicRefCell<IdTrackerSS>>,
        vector_storages: HashMap<VectorNameBuf, Arc<AtomicRefCell<VectorStorageEnum>>>,
        path: &Path,
        is_appendable: bool,
    ) -> OperationResult<Self> {
        create_dir_all(path)?;
        let config_path = PayloadConfig::get_config_path(path);
        let config = if config_path.exists() {
            PayloadConfig::load(&config_path)?
        } else {
            let mut new_config = PayloadConfig::default();
            if feature_flags().payload_index_skip_rocksdb && !is_appendable {
                new_config.skip_rocksdb = Some(true);
            }
            new_config
        };

        let skip_rocksdb = config.skip_rocksdb.unwrap_or(false);

        let storage_type = if is_appendable {
            let db = open_db_with_existing_cf(path).map_err(|err| OperationError::service_error(format!("RocksDB open error: {err}")))?;
            StorageType::Appendable(Arc::new(RwLock::new(db)))
        } else if skip_rocksdb {
            StorageType::NonAppendable
        } else {
            let db = open_db_with_existing_cf(path).map_err(|err| OperationError::service_error(format!("RocksDB open error: {err}")))?;
            StorageType::NonAppendableRocksDb(Arc::new(RwLock::new(db)))
        };

        let mut index = StructPayloadIndex {
            payload,
            id_tracker,
            vector_storages,
            field_indexes: Default::default(),
            config,
            path: path.to_owned(),
            visited_pool: Default::default(),
            storage_type,
        };

        if !index.config_path().exists() {
            index.save_config()?;
        }

        index.load_all_fields()?;

        Ok(index)
    }

    pub fn build_field_indexes(
        &self,
        field: PayloadKeyTypeRef,
        payload_schema: &PayloadFieldSchema,
        hw_counter: &HardwareCounterCell,
    ) -> OperationResult<Vec<FieldIndex>> {
        let payload_storage = self.payload.borrow();
        let mut builders = self
            .selector(payload_schema)
            .index_builder(field, payload_schema)?;

        for index in &mut builders {
            index.init()?;
        }

        payload_storage.iter(
            |point_id, point_payload| {
                let field_value = &point_payload.get_value(field);
                for builder in builders.iter_mut() {
                    builder.add_point(point_id, field_value, hw_counter)?;
                }
                Ok(true)
            },
            hw_counter,
        )?;

        builders
            .into_iter()
            .map(|builder| builder.finalize())
            .collect()
    }

    fn build_and_save(
        &mut self,
        field: PayloadKeyType,
        payload_schema: PayloadFieldSchema,
        field_index: Vec<FieldIndex>,
    ) -> OperationResult<()> {
        self.field_indexes.insert(field.clone(), field_index);

        self.config.indexed_fields.insert(field, payload_schema);

        self.save_config()?;

        Ok(())
    }

    /// Number of available points
    ///
    /// - excludes soft deleted points
    pub fn available_point_count(&self) -> usize {
        self.id_tracker.borrow().available_point_count()
    }

    pub fn struct_filtered_context<'a>(
        &'a self,
        filter: &'a Filter,
        hw_counter: &'a HardwareCounterCell,
    ) -> StructFilterContext<'a> {
        let payload_provider = PayloadProvider::new(self.payload.clone());

        let (optimized_filter, _) = self.optimize_filter(
            filter,
            payload_provider,
            self.available_point_count(),
            hw_counter,
        );

        StructFilterContext::new(optimized_filter)
    }

    pub(super) fn condition_cardinality(
        &self,
        condition: &Condition,
        nested_path: Option<&JsonPath>,
        hw_counter: &HardwareCounterCell,
    ) -> CardinalityEstimation {
        match condition {
            Condition::Filter(_) => panic!("Unexpected branching"),
            Condition::Nested(nested) => {
                // propagate complete nested path in case of multiple nested layers
                let full_path = JsonPath::extend_or_new(nested_path, &nested.array_key());
                self.estimate_nested_cardinality(nested.filter(), &full_path, hw_counter)
            }
            Condition::IsEmpty(IsEmptyCondition { is_empty: field }) => {
                let available_points = self.available_point_count();
                let condition = FieldCondition::new_is_empty(field.key.clone());

                self.estimate_field_condition(&condition, nested_path, hw_counter)
                    .unwrap_or_else(|| CardinalityEstimation::unknown(available_points))
            }
            Condition::IsNull(IsNullCondition { is_null: field }) => {
                let available_points = self.available_point_count();
                let condition = FieldCondition::new_is_null(field.key.clone());

                self.estimate_field_condition(&condition, nested_path, hw_counter)
                    .unwrap_or_else(|| CardinalityEstimation::unknown(available_points))
            }
            Condition::HasId(has_id) => {
                let id_tracker_ref = self.id_tracker.borrow();
                let mapped_ids: AHashSet<PointOffsetType> = has_id
                    .has_id
                    .iter()
                    .filter_map(|external_id| id_tracker_ref.internal_id(*external_id))
                    .collect();
                let num_ids = mapped_ids.len();
                CardinalityEstimation::exact(num_ids)
                    .with_primary_clause(PrimaryCondition::Ids(mapped_ids))
            }

            Condition::HasVector(has_vectors) => {
                if let Some(vector_storage) = self.vector_storages.get(&has_vectors.has_vector) {
                    let vector_storage = vector_storage.borrow();
                    let vectors = vector_storage.available_vector_count();
                    CardinalityEstimation::exact(vectors).with_primary_clause(
                        PrimaryCondition::HasVector(has_vectors.has_vector.clone()),
                    )
                } else {
                    CardinalityEstimation::exact(0)
                }
            }
            Condition::Field(field_condition) => self
                .estimate_field_condition(field_condition, nested_path, hw_counter)
                .unwrap_or_else(|| CardinalityEstimation::unknown(self.available_point_count())),

            Condition::CustomIdChecker(cond) => {
                cond.estimate_cardinality(self.id_tracker.borrow().available_point_count())
            }
        }
    }

    fn selector(&self, payload_schema: &PayloadFieldSchema) -> IndexSelector {
        let is_on_disk = payload_schema.is_on_disk();

        match &self.storage_type {
            StorageType::Appendable(db) => IndexSelector::RocksDb(IndexSelectorRocksDb {
                db,
                is_appendable: true,
            }),
            StorageType::NonAppendableRocksDb(db) => {
                // legacy logic: we keep rocksdb, but load mmap indexes
                if is_on_disk {
                    IndexSelector::Mmap(IndexSelectorMmap {
                        dir: &self.path,
                        is_on_disk,
                    })
                } else {
                    IndexSelector::RocksDb(IndexSelectorRocksDb {
                        db,
                        is_appendable: false,
                    })
                }
            }
            StorageType::NonAppendable => IndexSelector::Mmap(IndexSelectorMmap {
                dir: &self.path,
                is_on_disk,
            }),
        }
    }

    pub fn estimate_cardinality(
        &self,
        query: &Filter,
        hw_counter: &HardwareCounterCell,
    ) -> CardinalityEstimation {
        let available_points = self.available_point_count();
        let estimator =
            |condition: &Condition| self.condition_cardinality(condition, None, hw_counter);
        estimate_filter(&estimator, query, available_points)
    }

    pub fn estimate_nested_cardinality(
        &self,
        query: &Filter,
        nested_path: &JsonPath,
        hw_counter: &HardwareCounterCell,
    ) -> CardinalityEstimation {
        let available_points = self.available_point_count();
        let estimator = |condition: &Condition| {
            self.condition_cardinality(condition, Some(nested_path), hw_counter)
        };
        estimate_filter(&estimator, query, available_points)
    }

    pub fn iter_filtered_points<'a>(
        &'a self,
        filter: &'a Filter,
        id_tracker: &'a IdTrackerSS,
        query_cardinality: &'a CardinalityEstimation,
        hw_counter: &'a HardwareCounterCell,
    ) -> impl Iterator<Item = PointOffsetType> + 'a {
        let struct_filtered_context = self.struct_filtered_context(filter, hw_counter);

        if query_cardinality.primary_clauses.is_empty() {
            let full_scan_iterator = id_tracker.iter_ids();

            // Worst case: query expected to return few matches, but index can't be used
            let matched_points =
                full_scan_iterator.filter(move |i| struct_filtered_context.check(*i));

            Either::Left(matched_points)
        } else {
            // CPU-optimized strategy here: points are made unique before applying other filters.
            let mut visited_list = self.visited_pool.get(id_tracker.total_point_count());

            let iter = query_cardinality
                .primary_clauses
                .iter()
                .flat_map(move |clause| {
                    self.query_field(clause, hw_counter)
                        .unwrap_or_else(|| id_tracker.iter_ids() /* index is not built */)
                })
                .filter(move |&id| !visited_list.check_and_update_visited(id))
                .filter(move |&i| struct_filtered_context.check(i));

            Either::Right(iter)
        }
    }

    // Set of public functions that implement PayloadIndex trait.
    // Rest of the functions are public only for testing purposes.

    fn indexed_fields(&self) -> HashMap<PayloadKeyType, PayloadFieldSchema> {
        self.config.indexed_fields.clone()
    }

    fn build_index(
        &self,
        field: PayloadKeyTypeRef,
        payload_schema: &PayloadFieldSchema,
    ) -> OperationResult<Option<Vec<FieldIndex>>> {
        if let Some(prev_schema) = self.config.indexed_fields.get(field) {
            // the field is already indexed with the same schema
            // no need to rebuild index and to save the config
            if prev_schema == payload_schema {
                return Ok(None);
            }
        }

        let indexes = self.build_field_indexes(field, payload_schema, hw_counter)?; // Internal operation.

        Ok(Some(indexes))
    }

    fn apply_index(
        &mut self,
        field: PayloadKeyType,
        payload_schema: PayloadFieldSchema,
        field_index: Vec<FieldIndex>,
    ) -> OperationResult<()> {
        self.field_indexes.insert(field.clone(), field_index);

        self.config.indexed_fields.insert(field, payload_schema);

        self.save_config()?;

        Ok(())
    }

    fn assign(
        &mut self,
        point_id: PointOffsetType,
        payload: &Payload,
        key: &Option<JsonPath>,
        hw_counter: &HardwareCounterCell,
    ) -> OperationResult<()> {
        if let Some(key) = key {
            self.payload
                .borrow_mut()
                .set_by_key(point_id, payload, key, hw_counter)?;
        } else {
            self.payload
                .borrow_mut()
                .set(point_id, payload, hw_counter)?;
        };

        let updated_payload = self.get_payload(point_id, hw_counter)?;
        for (field, field_index) in &mut self.field_indexes {
            if !field.is_affected_by_value_set(&payload.0, key.as_ref()) {
                continue;
            }
            let field_value = updated_payload.get_value(field);
            if !field_value.is_empty() {
                for index in field_index {
                    index.add_point(point_id, &field_value, hw_counter)?;
                }
            } else {
                for index in field_index {
                    index.remove_point(point_id, hw_counter)?;
                }
            }
        }
        Ok(())
    }

    fn get_payload(
        &self,
        point_id: PointOffsetType,
        hw_counter: &HardwareCounterCell,
    ) -> OperationResult<Payload> {
        self.payload.borrow().get(point_id, hw_counter)
    }

    fn delete_payload(
        &mut self,
        point_id: PointOffsetType,
        key: PayloadKeyTypeRef,
        hw_counter: &HardwareCounterCell,
    ) -> OperationResult<Vec<Value>> {
        if let Some(indexes) = self.field_indexes.get_mut(key) {
            for index in indexes {
                index.remove_point(point_id, hw_counter)?;
            }
        }
        self.payload.borrow_mut().delete(point_id, key, hw_counter)
    }

    fn clear_payload(
        &mut self,
        point_id: PointOffsetType,
        hw_counter: &HardwareCounterCell,
    ) -> OperationResult<Option<Payload>> {
        self.clear_index_for_point(point_id, hw_counter)?;
        self.payload.borrow_mut().clear(point_id, hw_counter)
    }

    fn flusher(&self) -> Flusher {
        let mut flushers = Vec::new();
        for (_, field_indexes) in self.field_indexes.iter() {
            for index in field_indexes {
                flushers.push(index.flusher());
            }
        }
        flushers.push(self.payload.borrow().flusher());
        Box::new(move || {
            for flusher in flushers {
                match flusher() {
                    Ok(_) => {}
                    Err(OperationError::RocksDbColumnFamilyNotFound { name }) => {
                        // It is possible, that the index was removed during the flush by user or another thread.
                        // In this case, non-existing column family is not an error, but an expected behavior.

                        // Still we want to log this event, for potential debugging.
                        log::warn!(
                            "Flush: RocksDB cf_handle error: Cannot find column family {name}. Assume index is removed.",
                        );
                    }
                    Err(err) => {
                        return Err(OperationError::service_error(format!(
                            "Failed to flush payload_index: {err}"
                        )));
                    }
                }
            }
            Ok(())
        })
    }

    fn infer_payload_type(
        &self,
        key: PayloadKeyTypeRef,
        hw_counter: &HardwareCounterCell,
    ) -> OperationResult<Option<PayloadSchemaType>> {
        let mut schema = None;
        self.payload.borrow().iter(
            |_id, payload: &Payload| {
                let field_value = payload.get_value(key);
                schema = match field_value.as_slice() {
                    [] => None,
                    [single] => infer_value_type(single),
                    multiple => infer_collection_value_type(multiple.iter().copied()),
                };
                Ok(false)
            },
            hw_counter,
        )?;
        Ok(schema)
    }

    fn take_database_snapshot(&self, path: &Path) -> OperationResult<()> {
        match &self.storage_type {
            StorageType::Appendable(db) => {
                let db_guard = db.read();
                crate::rocksdb_backup::create(&db_guard, path)
            }
            StorageType::NonAppendableRocksDb(db) => {
                let db_guard = db.read();
                crate::rocksdb_backup::create(&db_guard, path)
            }
            StorageType::NonAppendable => Ok(()),
        }
    }

    fn files(&self) -> Vec<PathBuf> {
        let mut files = self
            .field_indexes
            .values()
            .flat_map(|indexes| indexes.iter().flat_map(|index| index.files().into_iter()))
            .collect::<Vec<PathBuf>>();
        files.push(self.config_path());
        files
    }
}