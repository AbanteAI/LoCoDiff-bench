
index 36481d11..531ff7dc 100644
--- a/aider_benchmark_problem_stats.py_expectedoutput.txt (expected):tmp/tmp8gmpyxng_expected.txt	
+++ b/aider_benchmark_problem_stats.py_extracted.txt (actual):tmp/tmpcc0fok77_actual.txt	
@@ -61,7 +61,6 @@ def load_results(dirname):
 
 def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
     PARSE_ERROR_M = 4  # Threshold for number of parse errors to DQ an exercise
-
     if dirs is None:
         # Use leaderboard data if no directories specified
         dir_entries = get_dirs_from_leaderboard()
@@ -84,7 +83,9 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
             # Calculate pass rate for sorting when using custom dirs
             if dirs is not None:
                 pass_rate = sum(
-                    1 for r in results if r.get("tests_outcomes", []) and r["tests_outcomes"][-1]
+                    1
+                    for r in results
+                    if r.get("tests_outcomes", []) and r["tests_outcomes"][-1]
                 ) / len(results)
             else:
                 # Use existing pass rate from leaderboard
@@ -105,12 +106,10 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
     if topn:
         valid_entries = valid_entries[:topn]
 
-    # Get all exercise names from a complete run
+    # Get all unique exercise names from all results
     all_exercises = set()
     exercise_solutions = defaultdict(list)
 
-    # Get all unique exercise names from all results
-    all_exercises = set()
     for (dirname, model), results, _ in valid_entries:
         if results:
             for result in results:
@@ -123,7 +122,6 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
         if not results:
             print(f"Could not load results for {dirname}")
             continue
-
         for result in results:
             testcase = result.get("testcase")
             if not testcase:
@@ -138,25 +136,25 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
             if tests_outcomes and tests_outcomes[-1]:
                 exercise_solutions[testcase].append(model)
 
+    # Add exercises that were never solved
+    for exercise in all_exercises:
+        if exercise not in exercise_solutions:
+            exercise_solutions[exercise] = []
+
     # Calculate never solved exercises
-    never_solved = len(all_exercises - set(exercise_solutions.keys()))
+    never_solved = len([e for e in all_exercises if not exercise_solutions[e]])
 
     # Print per-exercise statistics
     print("\nExercise Solution Statistics:")
     print("-" * 40)
 
-    # Add exercises that were never solved
-    for exercise in all_exercises:
-        if exercise not in exercise_solutions:
-            exercise_solutions[exercise] = []
-
     # Create list of (language, exercise) pairs with solution stats
     exercise_stats = []
     total_models = len(valid_entries)
 
     for testcase in all_exercises:
         # Language is already in the testcase string
-        lang = testcase.split("/")[0]  # First part is the language
+        lang = testcase.split("/")[1]  # First part is the language
         models = exercise_solutions[testcase]
         num_solved = len(models)
         percent = (num_solved / total_models) * 100
@@ -292,7 +290,7 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
                 if tests_outcomes and tests_outcomes[-1]:
                     solved_hard += 1
 
-        pct = (solved_hard / len(hard_set)) * 100
+        pct = (solved_hard / len(hard_set)) * 100 if hard_set else 0
         model_hard_stats.append((model, solved_hard, pct))
 
     # Sort by number solved
