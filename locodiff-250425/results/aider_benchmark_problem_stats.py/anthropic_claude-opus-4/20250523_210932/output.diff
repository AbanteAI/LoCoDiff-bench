
index 36481d117..db4f2ef5a 100644
--- a/aider_benchmark_problem_stats.py_expectedoutput.txt (expected):tmp/tmpjieh7_n2_expected.txt	
+++ b/aider_benchmark_problem_stats.py_extracted.txt (actual):tmp/tmp9d1psb8u_actual.txt	
@@ -105,10 +105,6 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
     if topn:
         valid_entries = valid_entries[:topn]
 
-    # Get all exercise names from a complete run
-    all_exercises = set()
-    exercise_solutions = defaultdict(list)
-
     # Get all unique exercise names from all results
     all_exercises = set()
     for (dirname, model), results, _ in valid_entries:
@@ -271,6 +267,26 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
         print(f"{lang:<12} {count:>8} {hard:>9} {total:>7} {pct:>7.1f}%")
     print()
 
+    # Count total problems and unsolved problems by language
+    lang_totals = defaultdict(int)
+    lang_unsolved = defaultdict(int)
+
+    for exercise in all_exercises:
+        lang = exercise.split("/")[1]  # Get language from path
+        lang_totals[lang] += 1
+        if not exercise_solutions[exercise]:  # No models solved this exercise
+            lang_unsolved[lang] += 1
+
+    print("\nUnsolved problems by language:")
+    print(f"{'Language':<12} {'Count':>5} {'Total':>7} {'Percent':>8}")
+    print("-" * 35)
+    for lang in sorted(lang_totals.keys()):
+        count = lang_unsolved[lang]
+        total = lang_totals[lang]
+        pct = (count / total) * 100
+        print(f"{lang:<12} {count:>5} {total:>7} {pct:>7.1f}%")
+    print()
+
     # For each model, compute performance on hard set
     model_hard_stats = []
     for (dirname, model), results, _ in valid_entries:
