
index 36481d117..ec8609186 100644
--- a/aider_benchmark_problem_stats.py_expectedoutput.txt (expected):tmp/tmpul_udrqo_expected.txt	
+++ b/aider_benchmark_problem_stats.py_extracted.txt (actual):tmp/tmppuiin1d5_actual.txt	
@@ -1,4 +1,4 @@
-#!/usr/bin/env python
+#!/usr/bin/env python3
 
 import argparse
 import json
@@ -105,12 +105,9 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
     if topn:
         valid_entries = valid_entries[:topn]
 
-    # Get all exercise names from a complete run
-    all_exercises = set()
-    exercise_solutions = defaultdict(list)
-
     # Get all unique exercise names from all results
     all_exercises = set()
+    exercise_solutions = defaultdict(list)
     for (dirname, model), results, _ in valid_entries:
         if results:
             for result in results:
@@ -141,22 +138,37 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
     # Calculate never solved exercises
     never_solved = len(all_exercises - set(exercise_solutions.keys()))
 
-    # Print per-exercise statistics
-    print("\nExercise Solution Statistics:")
-    print("-" * 40)
-
     # Add exercises that were never solved
     for exercise in all_exercises:
         if exercise not in exercise_solutions:
             exercise_solutions[exercise] = []
 
+    # Count parse errors per exercise
+    parse_error_counts = defaultdict(int)
+    for model_errors in parse_errors_by_model.values():
+        for exercise in model_errors:
+            parse_error_counts[exercise] += 1
+
+    # Find exercises to disqualify based on parse error threshold
+    disqualified_exercises = {
+        exercise for exercise, count in parse_error_counts.items() if count >= PARSE_ERROR_M
+    }
+
+    if disqualified_exercises:
+        print(
+            f"\nDisqualified {len(disqualified_exercises)} exercises with {PARSE_ERROR_M}+ parse"
+            " errors:"
+        )
+        for ex in sorted(disqualified_exercises):
+            print(f"  {ex} ({parse_error_counts[ex]} parse errors)")
+
     # Create list of (language, exercise) pairs with solution stats
     exercise_stats = []
     total_models = len(valid_entries)
 
     for testcase in all_exercises:
         # Language is already in the testcase string
-        lang = testcase.split("/")[0]  # First part is the language
+        lang = testcase.split("/")[1]  # First part is the language
         models = exercise_solutions[testcase]
         num_solved = len(models)
         percent = (num_solved / total_models) * 100
@@ -181,12 +193,8 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
 
     print("\nSummary:")
     solved_at_least_once = len([ex for ex, models in exercise_solutions.items() if models])
-    solved_by_none = never_solved
-    solved_by_all = len(
-        [ex for ex, models in exercise_solutions.items() if len(models) == total_models]
-    )
-
     print(f"Total exercises solved at least once: {solved_at_least_once}")
+    # print out these never solved use lang/exercises/practice/ex ai!
     print(f"Never solved by any model: {solved_by_none}")
     if solved_by_none > 0:
         print("\nExercises never solved by any model:")
@@ -218,35 +226,6 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
         print(f"{i:>6d}  {count:>9d}  {cumsum:>10d}  {revcumsum:>12d}")
         revcumsum -= count  # Decrement the reverse cumulative sum
 
-    # Count parse errors per exercise
-    parse_error_counts = defaultdict(int)
-    for model_errors in parse_errors_by_model.values():
-        for exercise in model_errors:
-            parse_error_counts[exercise] += 1
-
-    # Find exercises to disqualify based on parse error threshold
-    disqualified_exercises = {
-        exercise for exercise, count in parse_error_counts.items() if count >= PARSE_ERROR_M
-    }
-
-    if disqualified_exercises:
-        print(
-            f"\nDisqualified {len(disqualified_exercises)} exercises with {PARSE_ERROR_M}+ parse"
-            " errors:"
-        )
-        for ex in sorted(disqualified_exercises):
-            print(f"  {ex} ({parse_error_counts[ex]} parse errors)")
-
-    # Collect the hard set (exercises solved by HARD_SET_NUM or fewer models)
-    print(f"\nHard Set Analysis (exercises solved by ≤{HARD_SET_NUM} models):")
-    print("-" * 60)
-    hard_set = {
-        ex
-        for ex, models in exercise_solutions.items()
-        if len(models) <= HARD_SET_NUM and ex not in disqualified_exercises
-    }
-    print(f"Total hard set exercises: {len(hard_set)}")
-
     # Count total problems, unsolved problems, and hard set problems by language
     lang_totals = defaultdict(int)
     lang_unsolved = defaultdict(int)
@@ -271,6 +250,16 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
         print(f"{lang:<12} {count:>8} {hard:>9} {total:>7} {pct:>7.1f}%")
     print()
 
+    # Collect the hard set (exercises solved by HARD_SET_NUM or fewer models)
+    print(f"\nHard Set Analysis (exercises solved by ≤{HARD_SET_NUM} models):")
+    print("-" * 60)
+    hard_set = {
+        ex
+        for ex, models in exercise_solutions.items()
+        if len(models) <= HARD_SET_NUM and ex not in disqualified_exercises
+    }
+    print(f"Total hard set exercises: {len(hard_set)}")
+
     # For each model, compute performance on hard set
     model_hard_stats = []
     for (dirname, model), results, _ in valid_entries:
@@ -307,7 +296,7 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
     if copy_hard_set:
         # Create hard set directory
         src_dir = Path("tmp.benchmarks/exercism")
-        dst_dir = Path("tmp.benchmarks/exercism-polyglot")
+        dst_dir = Path("tmp.benchmarks/exercism-polygot")
 
         if dst_dir.exists():
             print(f"\nError: Destination directory {dst_dir} already exists")
