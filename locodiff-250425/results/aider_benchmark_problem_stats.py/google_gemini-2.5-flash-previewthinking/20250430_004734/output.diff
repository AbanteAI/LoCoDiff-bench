--- aider_benchmark_problem_stats.py_expectedoutput.txt (expected)+++ aider_benchmark_problem_stats.py_extracted.txt (actual)@@ -73,8 +73,6 @@     valid_entries = []
     parse_errors_by_model = {}  # Track which exercises had parse errors for each model
 
-    dump(dir_entries)
-
     for dirname, model in dir_entries:
         results_data = load_results(dirname)
 
@@ -85,7 +83,7 @@             if dirs is not None:
                 pass_rate = sum(
                     1 for r in results if r.get("tests_outcomes", []) and r["tests_outcomes"][-1]
-                ) / len(results)
+                ) / max(1, len(results))  # Avoid division by zero
             else:
                 # Use existing pass rate from leaderboard
                 pass_rate = next(
@@ -105,12 +103,10 @@     if topn:
         valid_entries = valid_entries[:topn]
 
-    # Get all exercise names from a complete run
+    # Get all unique exercise names from all results (format: testcase/language)
     all_exercises = set()
     exercise_solutions = defaultdict(list)
 
-    # Get all unique exercise names from all results
-    all_exercises = set()
     for (dirname, model), results, _ in valid_entries:
         if results:
             for result in results:
@@ -132,52 +128,61 @@             if not lang:
                 continue
 
+            # Format key as testcase/language
             testcase = f"{testcase}/{lang}"
+
             # Consider it solved if the last test attempt passed
             tests_outcomes = result.get("tests_outcomes", [])
             if tests_outcomes and tests_outcomes[-1]:
                 exercise_solutions[testcase].append(model)
 
-    # Calculate never solved exercises
-    never_solved = len(all_exercises - set(exercise_solutions.keys()))
+    # Add exercises that were never solved by any included model
+    for exercise in all_exercises:
+        if exercise not in exercise_solutions:
+            exercise_solutions[exercise] = []
+
+    # Calculate never solved exercises (those in all_exercises but with no models in exercise_solutions)
+    never_solved = len([ex for ex, models in exercise_solutions.items() if not models])
 
     # Print per-exercise statistics
     print("\nExercise Solution Statistics:")
     print("-" * 40)
 
-    # Add exercises that were never solved
-    for exercise in all_exercises:
-        if exercise not in exercise_solutions:
-            exercise_solutions[exercise] = []
-
     # Create list of (language, exercise) pairs with solution stats
     exercise_stats = []
     total_models = len(valid_entries)
 
     for testcase in all_exercises:
-        # Language is already in the testcase string
-        lang = testcase.split("/")[0]  # First part is the language
+        # Language is already in the testcase string (format: testcase/language)
+        lang = testcase.split("/")[1]
+        exercise_name = testcase.split("/")[0] # Get just the exercise name
+
         models = exercise_solutions[testcase]
         num_solved = len(models)
         percent = (num_solved / total_models) * 100
-        testcase = testcase.replace("exercises/", "")  # Remove the exercises/ prefix
-        # Remove duplicate language prefix (e.g. javascript/javascript/ -> javascript/)
-        if testcase.startswith(f"{lang}/{lang}/"):
-            testcase = testcase[len(lang) + 1 :]
-        exercise_stats.append((lang, testcase, num_solved, percent))
-
-    # Sort all exercises by solve rate, then by exercise name
-    exercise_stats.sort(
-        key=lambda x: (-x[2], x[1])
-    )  # -x[2] for descending solve rate, x[1] for ascending exercise name
+
+        # Clean up the testcase name for display
+        display_testcase = f"{lang}/{exercise_name}"
+        # Remove duplicate language prefix (e.g. javascript/javascript -> javascript)
+        if display_testcase.startswith(f"{lang}/{lang}/"):
+             display_testcase = display_testcase[len(lang) + 1 :]
+
+        exercise_stats.append((lang, exercise_name, display_testcase, num_solved, percent))
+
+    # Sort all exercises by solve rate, then by display name
+    exercise_stats.sort(key=lambda x: (-x[3], x[2])) # -x[3] is num_solved descending, x[2] is display_testcase ascending
 
     # Calculate max lengths for alignment after cleaning up paths
-    max_name_len = max(len(f"{lang}/{testcase}") for lang, testcase, _, _ in exercise_stats)
+    max_name_len = max((len(display_testcase) for _, _, display_testcase, _, _ in exercise_stats), default=0) # Handle empty exercise set
 
     # Print all exercises sorted by solve rate
     print("\nAll Exercises (sorted by solve rate):")
-    for i, (lang, testcase, num_solved, percent) in enumerate(exercise_stats, 1):
-        print(f"{i:>3}. {testcase:<{max_name_len}} : {num_solved:>3} solved ({percent:>5.1f}%)")
+    print("-" * (max_name_len + 20)) # Simple separator based on max name length
+    for i, (lang, exercise_name, display_testcase, num_solved, percent) in enumerate(exercise_stats, 1):
+        print(
+            f"{i:>3}. {display_testcase:<{max_name_len}} : {num_solved:>3} solved ({percent:>5.1f}%)"
+        )
+
 
     print("\nSummary:")
     solved_at_least_once = len([ex for ex, models in exercise_solutions.items() if models])
@@ -187,13 +192,14 @@     )
 
     print(f"Total exercises solved at least once: {solved_at_least_once}")
+    # print out these never solved use lang/exercises/practice/ex ai!
     print(f"Never solved by any model: {solved_by_none}")
     if solved_by_none > 0:
         print("\nExercises never solved by any model:")
-        unsolved = [ex for ex, models in exercise_solutions.items() if not models]
-        for ex in sorted(unsolved):
-            # Split into language and exercise parts
-            lang, exercise = ex.split("/")
+        unsolved_list = [ex for ex, models in exercise_solutions.items() if not models]
+        for ex in sorted(unsolved_list):
+            # Split into exercise and language parts
+            exercise, lang = ex.split("/")
             # Reconstruct path in desired format
             formatted_path = f"{lang}/exercises/practice/{exercise}"
             print(f"  {formatted_path}")
@@ -292,7 +298,7 @@                 if tests_outcomes and tests_outcomes[-1]:
                     solved_hard += 1
 
-        pct = (solved_hard / len(hard_set)) * 100
+        pct = (solved_hard / len(hard_set)) * 100 if len(hard_set) > 0 else -1
         model_hard_stats.append((model, solved_hard, pct))
 
     # Sort by number solved
@@ -334,22 +340,4 @@                     copied_by_lang[lang] += 1
 
         total_copied = sum(copied_by_lang.values())
-        print(f"\nCopied {total_copied} hard set problems:")
-        for lang in sorted(copied_by_lang):
-            print(f"  {lang}: {copied_by_lang[lang]}")
-
-
-if __name__ == "__main__":
-    parser = argparse.ArgumentParser()
-    parser.add_argument("--topn", type=int, help="Only consider top N models by pass rate")
-    parser.add_argument(
-        "dirs", nargs="*", help="Directories to analyze (optional, defaults to leaderboard entries)"
-    )
-    parser.add_argument(
-        "--copy-hard-set",
-        action="store_true",
-        help="Copy hard set problems to tmp.benchmarks/exercism-polygot",
-    )
-    args = parser.parse_args()
-
-    analyze_exercise_solutions(args.dirs if args.dirs else None, args.topn, args.copy_hard_set)+        print(f"\nCopied {total_copied} hard set problems:")