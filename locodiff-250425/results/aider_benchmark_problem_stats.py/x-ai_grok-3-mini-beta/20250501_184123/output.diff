
index 36481d11..065a6225 100644
--- a/aider_benchmark_problem_stats.py_expectedoutput.txt (expected):tmp/tmpavkz7iur_expected.txt	
+++ b/aider_benchmark_problem_stats.py_extracted.txt (actual):tmp/tmphnsiskix_actual.txt	
@@ -11,7 +11,7 @@ import yaml
 from aider.dump import dump  # noqa
 
 HARD_SET_NUM = 3  # Number of models that defines the hard set threshold
-
+PARSE_ERROR_M = 4  # Threshold for number of parse errors to DQ an exercise
 
 def get_dirs_from_leaderboard():
     # Load the leaderboard data
@@ -19,11 +19,9 @@ def get_dirs_from_leaderboard():
         leaderboard = yaml.safe_load(f)
     return [(entry["dirname"], entry["model"]) for entry in leaderboard]
 
-
 def load_results(dirname):
     """Load all result files from a benchmark directory"""
     dirname = Path(dirname)
-
     benchmark_dir = dirname
     if not benchmark_dir.exists():
         benchmark_dir = Path("tmp.benchmarks") / dirname
@@ -58,10 +56,7 @@ def load_results(dirname):
 
     return all_results, parse_errors
 
-
 def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
-    PARSE_ERROR_M = 4  # Threshold for number of parse errors to DQ an exercise
-
     if dirs is None:
         # Use leaderboard data if no directories specified
         dir_entries = get_dirs_from_leaderboard()
@@ -72,12 +67,10 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
     # Filter out entries that don't load and sort by pass rate
     valid_entries = []
     parse_errors_by_model = {}  # Track which exercises had parse errors for each model
-
     dump(dir_entries)
 
     for dirname, model in dir_entries:
         results_data = load_results(dirname)
-
         if results_data:
             results, model_parse_errors = results_data
             parse_errors_by_model[model] = set(model_parse_errors)
@@ -105,12 +98,9 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
     if topn:
         valid_entries = valid_entries[:topn]
 
-    # Get all exercise names from a complete run
-    all_exercises = set()
-    exercise_solutions = defaultdict(list)
-
     # Get all unique exercise names from all results
     all_exercises = set()
+    exercise_solutions = defaultdict(list)
     for (dirname, model), results, _ in valid_entries:
         if results:
             for result in results:
@@ -121,9 +111,7 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
 
     for (dirname, model), results, _ in valid_entries:
         if not results:
-            print(f"Could not load results for {dirname}")
             continue
-
         for result in results:
             testcase = result.get("testcase")
             if not testcase:
@@ -138,22 +126,12 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
             if tests_outcomes and tests_outcomes[-1]:
                 exercise_solutions[testcase].append(model)
 
-    # Calculate never solved exercises
-    never_solved = len(all_exercises - set(exercise_solutions.keys()))
-
-    # Print per-exercise statistics
     print("\nExercise Solution Statistics:")
     print("-" * 40)
 
-    # Add exercises that were never solved
-    for exercise in all_exercises:
-        if exercise not in exercise_solutions:
-            exercise_solutions[exercise] = []
-
-    # Create list of (language, exercise) pairs with solution stats
+    # Sort by number of models that solved each exercise, then by exercise name
     exercise_stats = []
     total_models = len(valid_entries)
-
     for testcase in all_exercises:
         # Language is already in the testcase string
         lang = testcase.split("/")[0]  # First part is the language
@@ -162,7 +140,7 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
         percent = (num_solved / total_models) * 100
         testcase = testcase.replace("exercises/", "")  # Remove the exercises/ prefix
         # Remove duplicate language prefix (e.g. javascript/javascript/ -> javascript/)
-        if testcase.startswith(f"{lang}/{lang}/"):
+        if testcase.startswith(f"{lang}/"):
             testcase = testcase[len(lang) + 1 :]
         exercise_stats.append((lang, testcase, num_solved, percent))
 
@@ -181,11 +159,6 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
 
     print("\nSummary:")
     solved_at_least_once = len([ex for ex, models in exercise_solutions.items() if models])
-    solved_by_none = never_solved
-    solved_by_all = len(
-        [ex for ex, models in exercise_solutions.items() if len(models) == total_models]
-    )
-
     print(f"Total exercises solved at least once: {solved_at_least_once}")
     print(f"Never solved by any model: {solved_by_none}")
     if solved_by_none > 0:
@@ -276,7 +249,6 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
     for (dirname, model), results, _ in valid_entries:
         if not results:
             continue
-
         solved_hard = 0
         for result in results:
             testcase = result.get("testcase")
@@ -285,13 +257,11 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
             lang = result.get("language")
             if not lang:
                 continue
-
             testcase = f"{testcase}/{lang}"
             if testcase in hard_set:
                 tests_outcomes = result.get("tests_outcomes", [])
                 if tests_outcomes and tests_outcomes[-1]:
                     solved_hard += 1
-
         pct = (solved_hard / len(hard_set)) * 100
         model_hard_stats.append((model, solved_hard, pct))
 
@@ -323,7 +293,6 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
         for lang_dir in src_dir.glob("*/exercises/practice"):
             if not lang_dir.is_dir():
                 continue
-
             lang = lang_dir.parts[-3]  # Get language from path
             for problem_dir in lang_dir.glob("*"):
                 if (problem_dir.name, lang) in hard_set_pairs:
