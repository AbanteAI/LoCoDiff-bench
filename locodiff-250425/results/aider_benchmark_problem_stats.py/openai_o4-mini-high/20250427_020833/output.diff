--- aider_benchmark_problem_stats.py_expectedoutput.txt (expected)+++ aider_benchmark_problem_stats.py_extracted.txt (actual)@@ -12,18 +12,15 @@ 
 HARD_SET_NUM = 3  # Number of models that defines the hard set threshold
 
-
 def get_dirs_from_leaderboard():
     # Load the leaderboard data
     with open("aider/website/_data/polyglot_leaderboard.yml") as f:
         leaderboard = yaml.safe_load(f)
     return [(entry["dirname"], entry["model"]) for entry in leaderboard]
 
-
 def load_results(dirname):
     """Load all result files from a benchmark directory"""
     dirname = Path(dirname)
-
     benchmark_dir = dirname
     if not benchmark_dir.exists():
         benchmark_dir = Path("tmp.benchmarks") / dirname
@@ -44,7 +41,6 @@                 lang = fname.parts[-5]  # Get language from path
                 results["language"] = lang
                 all_results.append(results)
-
         except json.JSONDecodeError:
             error = True
 
@@ -58,10 +54,8 @@ 
     return all_results, parse_errors
 
-
 def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
     PARSE_ERROR_M = 4  # Threshold for number of parse errors to DQ an exercise
-
     if dirs is None:
         # Use leaderboard data if no directories specified
         dir_entries = get_dirs_from_leaderboard()
@@ -69,22 +63,22 @@         # Use provided directories, with dirname as model name
         dir_entries = [(d, d) for d in dirs]
 
+    dump(dir_entries)
+
     # Filter out entries that don't load and sort by pass rate
     valid_entries = []
     parse_errors_by_model = {}  # Track which exercises had parse errors for each model
-
-    dump(dir_entries)
-
     for dirname, model in dir_entries:
         results_data = load_results(dirname)
-
         if results_data:
             results, model_parse_errors = results_data
             parse_errors_by_model[model] = set(model_parse_errors)
+
             # Calculate pass rate for sorting when using custom dirs
             if dirs is not None:
                 pass_rate = sum(
-                    1 for r in results if r.get("tests_outcomes", []) and r["tests_outcomes"][-1]
+                    1 for r in results
+                    if r.get("tests_outcomes", []) and r["tests_outcomes"][-1]
                 ) / len(results)
             else:
                 # Use existing pass rate from leaderboard
@@ -105,12 +99,11 @@     if topn:
         valid_entries = valid_entries[:topn]
 
-    # Get all exercise names from a complete run
+    # Prepare exercise solutions mapping and collect all exercises
+    exercise_solutions = defaultdict(list)
     all_exercises = set()
-    exercise_solutions = defaultdict(list)
 
     # Get all unique exercise names from all results
-    all_exercises = set()
     for (dirname, model), results, _ in valid_entries:
         if results:
             for result in results:
@@ -119,6 +112,7 @@                 except KeyError:
                     print(f"Warning: Missing testcase in {dirname}", json.dumps(result, indent=4))
 
+    # Collect solved exercises per model
     for (dirname, model), results, _ in valid_entries:
         if not results:
             print(f"Could not load results for {dirname}")
@@ -132,44 +126,28 @@             if not lang:
                 continue
 
-            testcase = f"{testcase}/{lang}"
-            # Consider it solved if the last test attempt passed
+            key = f"{testcase}/{lang}"
             tests_outcomes = result.get("tests_outcomes", [])
             if tests_outcomes and tests_outcomes[-1]:
-                exercise_solutions[testcase].append(model)
-
-    # Calculate never solved exercises
-    never_solved = len(all_exercises - set(exercise_solutions.keys()))
-
-    # Print per-exercise statistics
-    print("\nExercise Solution Statistics:")
-    print("-" * 40)
+                exercise_solutions[key].append(model)
 
     # Add exercises that were never solved
     for exercise in all_exercises:
         if exercise not in exercise_solutions:
             exercise_solutions[exercise] = []
 
-    # Create list of (language, exercise) pairs with solution stats
+    # Sort all exercises by solve rate, then by exercise name
     exercise_stats = []
     total_models = len(valid_entries)
-
-    for testcase in all_exercises:
-        # Language is already in the testcase string
-        lang = testcase.split("/")[0]  # First part is the language
-        models = exercise_solutions[testcase]
-        num_solved = len(models)
+    for exercise in all_exercises:
+        parts = exercise.split("/")
+        testcase = parts[0]
+        lang = parts[1]
+        num_solved = len(exercise_solutions[exercise])
         percent = (num_solved / total_models) * 100
-        testcase = testcase.replace("exercises/", "")  # Remove the exercises/ prefix
-        # Remove duplicate language prefix (e.g. javascript/javascript/ -> javascript/)
-        if testcase.startswith(f"{lang}/{lang}/"):
-            testcase = testcase[len(lang) + 1 :]
         exercise_stats.append((lang, testcase, num_solved, percent))
 
-    # Sort all exercises by solve rate, then by exercise name
-    exercise_stats.sort(
-        key=lambda x: (-x[2], x[1])
-    )  # -x[2] for descending solve rate, x[1] for ascending exercise name
+    exercise_stats.sort(key=lambda x: (-x[2], x[1]))
 
     # Calculate max lengths for alignment after cleaning up paths
     max_name_len = max(len(f"{lang}/{testcase}") for lang, testcase, _, _ in exercise_stats)
@@ -179,12 +157,11 @@     for i, (lang, testcase, num_solved, percent) in enumerate(exercise_stats, 1):
         print(f"{i:>3}. {testcase:<{max_name_len}} : {num_solved:>3} solved ({percent:>5.1f}%)")
 
+    # Print summary
     print("\nSummary:")
     solved_at_least_once = len([ex for ex, models in exercise_solutions.items() if models])
-    solved_by_none = never_solved
-    solved_by_all = len(
-        [ex for ex, models in exercise_solutions.items() if len(models) == total_models]
-    )
+    solved_by_none = len([ex for ex, models in exercise_solutions.items() if not models])
+    solved_by_all = len([ex for ex, models in exercise_solutions.items() if len(models) == total_models])
 
     print(f"Total exercises solved at least once: {solved_at_least_once}")
     print(f"Never solved by any model: {solved_by_none}")
@@ -192,18 +169,16 @@         print("\nExercises never solved by any model:")
         unsolved = [ex for ex, models in exercise_solutions.items() if not models]
         for ex in sorted(unsolved):
-            # Split into language and exercise parts
             lang, exercise = ex.split("/")
-            # Reconstruct path in desired format
             formatted_path = f"{lang}/exercises/practice/{exercise}"
             print(f"  {formatted_path}")
     print(f"\nSolved by all models: {solved_by_all}")
     print(
-        f"Total exercises: {len(all_exercises)} = {solved_by_none} (none) + {solved_by_all} (all) +"
-        f" {len(all_exercises) - solved_by_none - solved_by_all} (some)"
+        f"Total exercises: {len(all_exercises)} = {solved_by_none} (none) + {solved_by_all} (all) + "
+        f"{len(all_exercises) - solved_by_none - solved_by_all} (some)"
     )
 
-    # Distribution table of how many models solved each exercise
+    # Distribution of solutions
     print("\nDistribution of solutions:")
     print("Models  Exercises  Cumulative  RevCumulative")
     print("-" * 50)
@@ -216,7 +191,7 @@     for i, count in enumerate(counts):
         cumsum += count
         print(f"{i:>6d}  {count:>9d}  {cumsum:>10d}  {revcumsum:>12d}")
-        revcumsum -= count  # Decrement the reverse cumulative sum
+        revcumsum -= count
 
     # Count parse errors per exercise
     parse_error_counts = defaultdict(int)
@@ -226,7 +201,9 @@ 
     # Find exercises to disqualify based on parse error threshold
     disqualified_exercises = {
-        exercise for exercise, count in parse_error_counts.items() if count >= PARSE_ERROR_M
+        exercise
+        for exercise, count in parse_error_counts.items()
+        if count >= PARSE_ERROR_M
     }
 
     if disqualified_exercises:
@@ -237,7 +214,7 @@         for ex in sorted(disqualified_exercises):
             print(f"  {ex} ({parse_error_counts[ex]} parse errors)")
 
-    # Collect the hard set (exercises solved by HARD_SET_NUM or fewer models)
+    # Hard Set Analysis
     print(f"\nHard Set Analysis (exercises solved by â‰¤{HARD_SET_NUM} models):")
     print("-" * 60)
     hard_set = {
@@ -247,17 +224,16 @@     }
     print(f"Total hard set exercises: {len(hard_set)}")
 
-    # Count total problems, unsolved problems, and hard set problems by language
+    # Unsolved and hard set problems by language
     lang_totals = defaultdict(int)
     lang_unsolved = defaultdict(int)
     lang_hard_set = defaultdict(int)
-
     for exercise in all_exercises:
         lang = exercise.split("/")[1]  # Get language from path
         lang_totals[lang] += 1
-        if not exercise_solutions[exercise]:  # No models solved this exercise
+        if not exercise_solutions[exercise]:
             lang_unsolved[lang] += 1
-        if exercise in hard_set:  # Exercise is in the hard set
+        if exercise in hard_set:
             lang_hard_set[lang] += 1
 
     print("\nUnsolved and hard set problems by language:")
@@ -271,7 +247,7 @@         print(f"{lang:<12} {count:>8} {hard:>9} {total:>7} {pct:>7.1f}%")
     print()
 
-    # For each model, compute performance on hard set
+    # Model performance on hard set
     model_hard_stats = []
     for (dirname, model), results, _ in valid_entries:
         if not results:
@@ -286,13 +262,13 @@             if not lang:
                 continue
 
-            testcase = f"{testcase}/{lang}"
-            if testcase in hard_set:
+            exercise = f"{testcase}/{lang}"
+            if exercise in hard_set:
                 tests_outcomes = result.get("tests_outcomes", [])
                 if tests_outcomes and tests_outcomes[-1]:
                     solved_hard += 1
 
-        pct = (solved_hard / len(hard_set)) * 100
+        pct = (solved_hard / len(hard_set)) * 100 if hard_set else 0
         model_hard_stats.append((model, solved_hard, pct))
 
     # Sort by number solved
@@ -307,7 +283,7 @@     if copy_hard_set:
         # Create hard set directory
         src_dir = Path("tmp.benchmarks/exercism")
-        dst_dir = Path("tmp.benchmarks/exercism-polyglot")
+        dst_dir = Path("tmp.benchmarks/exercism-polygot")
 
         if dst_dir.exists():
             print(f"\nError: Destination directory {dst_dir} already exists")
@@ -316,7 +292,7 @@         print(f"\nCopying hard set problems to {dst_dir}...")
 
         # Create a set of (exercise, language) pairs from hard_set
-        hard_set_pairs = {tuple(exercise.split("/")) for exercise in hard_set}
+        hard_set_pairs = {tuple(ex.split("/")) for ex in hard_set}
 
         # Copy each hard set problem's directory
         copied_by_lang = defaultdict(int)
@@ -337,7 +313,6 @@         print(f"\nCopied {total_copied} hard set problems:")
         for lang in sorted(copied_by_lang):
             print(f"  {lang}: {copied_by_lang[lang]}")
-
 
 if __name__ == "__main__":
     parser = argparse.ArgumentParser()
