
index 36481d11..85e8fd31 100644
--- a/aider_benchmark_problem_stats.py_expectedoutput.txt (expected):tmp/tmp84a38cd4_expected.txt	
+++ b/aider_benchmark_problem_stats.py_extracted.txt (actual):tmp/tmpmi3vewp7_actual.txt	
@@ -36,6 +36,7 @@ def load_results(dirname):
     # Look in language subdirectories under exercises/practice
     for fname in benchmark_dir.glob("*/exercises/practice/*/.aider.results.json"):
         error = False
+        results = None
         try:
             results = json.loads(fname.read_text())
             error = "testcase" not in results
@@ -51,8 +52,9 @@ def load_results(dirname):
         if error:
             # Track the parse error for this exercise/model combination
             lang = fname.parts[-5]
-            exercise = f"{fname.parts[-2]}/{lang}"  # Use directory name as testcase
-            parse_errors.append(exercise)
+            exercise = f"{fname.parts[-2]}"  # Use directory name as testcase
+            # Store as exercise/language to match the solved set format
+            parse_errors.append(f"{exercise}/{lang}")
             print(f"Bad results file {fname}")
             continue
 
@@ -69,12 +71,12 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
         # Use provided directories, with dirname as model name
         dir_entries = [(d, d) for d in dirs]
 
+    dump(dir_entries)
+
     # Filter out entries that don't load and sort by pass rate
     valid_entries = []
     parse_errors_by_model = {}  # Track which exercises had parse errors for each model
 
-    dump(dir_entries)
-
     for dirname, model in dir_entries:
         results_data = load_results(dirname)
 
@@ -105,16 +107,15 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
     if topn:
         valid_entries = valid_entries[:topn]
 
-    # Get all exercise names from a complete run
+    # Get all unique exercise names (exercise/language format) from all results
     all_exercises = set()
     exercise_solutions = defaultdict(list)
 
-    # Get all unique exercise names from all results
-    all_exercises = set()
     for (dirname, model), results, _ in valid_entries:
         if results:
             for result in results:
                 try:
+                    # Store as exercise/language
                     all_exercises.add(result["testcase"] + "/" + result["language"])
                 except KeyError:
                     print(f"Warning: Missing testcase in {dirname}", json.dumps(result, indent=4))
@@ -132,6 +133,7 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
             if not lang:
                 continue
 
+            # Store as exercise/language
             testcase = f"{testcase}/{lang}"
             # Consider it solved if the last test attempt passed
             tests_outcomes = result.get("tests_outcomes", [])
@@ -145,39 +147,32 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
     print("\nExercise Solution Statistics:")
     print("-" * 40)
 
-    # Add exercises that were never solved
-    for exercise in all_exercises:
-        if exercise not in exercise_solutions:
-            exercise_solutions[exercise] = []
-
     # Create list of (language, exercise) pairs with solution stats
     exercise_stats = []
     total_models = len(valid_entries)
 
     for testcase in all_exercises:
-        # Language is already in the testcase string
-        lang = testcase.split("/")[0]  # First part is the language
+        # Language/Exercise are already in the testcase string
+        lang = testcase.split("/")[1]  # Second part is the language
+        exercise_name = testcase.split("/")[0]  # First part is the exercise name
         models = exercise_solutions[testcase]
         num_solved = len(models)
         percent = (num_solved / total_models) * 100
-        testcase = testcase.replace("exercises/", "")  # Remove the exercises/ prefix
-        # Remove duplicate language prefix (e.g. javascript/javascript/ -> javascript/)
-        if testcase.startswith(f"{lang}/{lang}/"):
-            testcase = testcase[len(lang) + 1 :]
-        exercise_stats.append((lang, testcase, num_solved, percent))
+        exercise_stats.append((lang, exercise_name, num_solved, percent))
 
     # Sort all exercises by solve rate, then by exercise name
-    exercise_stats.sort(
-        key=lambda x: (-x[2], x[1])
-    )  # -x[2] for descending solve rate, x[1] for ascending exercise name
+    # -x[2] for descending solve rate, x[1] for ascending exercise name
+    exercise_stats.sort(key=lambda x: (-x[2], x[1]))
 
     # Calculate max lengths for alignment after cleaning up paths
-    max_name_len = max(len(f"{lang}/{testcase}") for lang, testcase, _, _ in exercise_stats)
+    max_name_len = max(len(f"{ex}/{lang}") for lang, ex, _, _ in exercise_stats)
 
     # Print all exercises sorted by solve rate
     print("\nAll Exercises (sorted by solve rate):")
-    for i, (lang, testcase, num_solved, percent) in enumerate(exercise_stats, 1):
-        print(f"{i:>3}. {testcase:<{max_name_len}} : {num_solved:>3} solved ({percent:>5.1f}%)")
+    for i, (lang, exercise_name, num_solved, percent) in enumerate(exercise_stats, 1):
+        print(
+            f"{i:>3}. {exercise_name}/{lang:<{max_name_len - len(exercise_name) - 1}} : {num_solved:>3} solved ({percent:>5.1f}%)"
+        )
 
     print("\nSummary:")
     solved_at_least_once = len([ex for ex, models in exercise_solutions.items() if models])
@@ -190,12 +185,12 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
     print(f"Never solved by any model: {solved_by_none}")
     if solved_by_none > 0:
         print("\nExercises never solved by any model:")
-        unsolved = [ex for ex, models in exercise_solutions.items() if not models]
-        for ex in sorted(unsolved):
+        unsolved_list = [ex for ex, models in exercise_solutions.items() if not models]
+        for ex in sorted(unsolved_list):
             # Split into language and exercise parts
-            lang, exercise = ex.split("/")
+            exercise_name, lang = ex.split("/")
             # Reconstruct path in desired format
-            formatted_path = f"{lang}/exercises/practice/{exercise}"
+            formatted_path = f"{lang}/exercises/practice/{exercise_name}"
             print(f"  {formatted_path}")
     print(f"\nSolved by all models: {solved_by_all}")
     print(
@@ -235,9 +230,12 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
             " errors:"
         )
         for ex in sorted(disqualified_exercises):
-            print(f"  {ex} ({parse_error_counts[ex]} parse errors)")
+            lang = ex.split("/")[1]
+            exercise_name = ex.split("/")[0]
+            formatted_path = f"{lang}/exercises/practice/{exercise_name}"
+            print(f"  {formatted_path} ({parse_error_counts[ex]} parse errors)")
 
-    # Collect the hard set (exercises solved by HARD_SET_NUM or fewer models)
+    # Collect the hard set (exercises solved by HARD_SET_NUM or fewer models), excluding disqualified
     print(f"\nHard Set Analysis (exercises solved by â‰¤{HARD_SET_NUM} models):")
     print("-" * 60)
     hard_set = {
@@ -253,6 +251,7 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
     lang_hard_set = defaultdict(int)
 
     for exercise in all_exercises:
+        # lang/exercise
         lang = exercise.split("/")[1]  # Get language from path
         lang_totals[lang] += 1
         if not exercise_solutions[exercise]:  # No models solved this exercise
@@ -261,48 +260,52 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
             lang_hard_set[lang] += 1
 
     print("\nUnsolved and hard set problems by language:")
-    print(f"{'Language':<12} {'Unsolved':>8} {'Hard Set':>9} {'Total':>7} {'%hardUnsolved':>8}")
+    print(f"{'Language':<12} {'Unsolved':>8} {'Hard Set':>9} {'Total':>7} {'%Unsolved':>8}")
     print("-" * 47)
     for lang in sorted(lang_totals.keys()):
         count = lang_unsolved[lang]
         hard = lang_hard_set[lang]
         total = lang_totals[lang]
-        pct = (count / hard) * 100 if hard else -1
+        pct = (count / total) * 100
         print(f"{lang:<12} {count:>8} {hard:>9} {total:>7} {pct:>7.1f}%")
     print()
 
     # For each model, compute performance on hard set
     model_hard_stats = []
-    for (dirname, model), results, _ in valid_entries:
-        if not results:
-            continue
-
-        solved_hard = 0
-        for result in results:
-            testcase = result.get("testcase")
-            if not testcase:
-                continue
-            lang = result.get("language")
-            if not lang:
+    if len(hard_set) > 0:
+        for (dirname, model), results, _ in valid_entries:
+            if not results:
                 continue
 
-            testcase = f"{testcase}/{lang}"
-            if testcase in hard_set:
-                tests_outcomes = result.get("tests_outcomes", [])
-                if tests_outcomes and tests_outcomes[-1]:
-                    solved_hard += 1
-
-        pct = (solved_hard / len(hard_set)) * 100
-        model_hard_stats.append((model, solved_hard, pct))
-
-    # Sort by number solved
-    model_hard_stats.sort(key=lambda x: x[1], reverse=True)
+            solved_hard = 0
+            for result in results:
+                testcase = result.get("testcase")
+                if not testcase:
+                    continue
+                lang = result.get("language")
+                if not lang:
+                    continue
+
+                testcase = f"{testcase}/{lang}"
+                if testcase in hard_set:
+                    tests_outcomes = result.get("tests_outcomes", [])
+                    if tests_outcomes and tests_outcomes[-1]:
+                        solved_hard += 1
+
+            pct = (solved_hard / len(hard_set)) * 100
+            model_hard_stats.append((model, solved_hard, pct))
+
+        # Sort by number solved
+        model_hard_stats.sort(key=lambda x: x[1], reverse=True)
+
+        print("\nModel performance on hard set:")
+        print(f"{'Model':<55} {'Solved':<8} {'Percent':>7}")
+        print("-" * 50)
+        for model, solved, pct in model_hard_stats:
+            print(f"{model:<55} {solved:>6d}   {pct:>6.1f}%")
+    else:
+        print("\nNo hard set problems to analyze.")
 
-    print("\nModel performance on hard set:")
-    print(f"{'Model':<55} {'Solved':<8} {'Percent':>7}")
-    print("-" * 50)
-    for model, solved, pct in model_hard_stats:
-        print(f"{model:<55} {solved:>6d}   {pct:>6.1f}%")
 
     if copy_hard_set:
         # Create hard set directory
