```python
#!/usr/bin/env python
"""
Analyze benchmark results for Exercism “polyglot” runs.

This script scans the benchmark result JSON blobs produced by aider, tallies
which models solved which Exercism practice exercises, and prints a variety of
stats.  It can also copy the “hard set” (poorly-solved) exercises into a new
directory for further study.

The script intentionally keeps lots of debugging and exploratory output that is
useful when iterating on benchmarking.  Accordingly, the code style is a bit
looser than production quality.
"""

import argparse
import json
import shutil
from collections import defaultdict
from pathlib import Path

import yaml
from aider.dump import dump  # noqa

HARD_SET_NUM = 3  # Number of models (≤) that defines the hard-set threshold


def get_dirs_from_leaderboard():
    """Return (dirname, model) tuples from the polyglot leaderboard."""
    with open("aider/website/_data/polyglot_leaderboard.yml") as f:
        leaderboard = yaml.safe_load(f)
    return [(entry["dirname"], entry["model"]) for entry in leaderboard]


def load_results(dirname):
    """
    Load all .aider.results.json blobs for a benchmark directory.

    Returns a tuple: (results_list, parse_error_exercises)
    – results_list            : list of dicts for successfully parsed results
    – parse_error_exercises   : list of exercise strings that failed to parse
    """
    dirname = Path(dirname)

    # Allow callers to pass either the full path or just the leaf “benchmark id”
    benchmark_dir = dirname
    if not benchmark_dir.exists():
        benchmark_dir = Path("tmp.benchmarks") / dirname
        if not benchmark_dir.exists():
            return None

    all_results = []
    parse_errors = []

    # Look in language sub-dirs: */exercises/practice/*/.aider.results.json
    for fname in benchmark_dir.glob("*/exercises/practice/*/.aider.results.json"):
        error = False
        try:
            results = json.loads(fname.read_text())
            error = "testcase" not in results
            if not error:
                lang = fname.parts[-5]  # language component of the path
                results["language"] = lang
                all_results.append(results)
        except json.JSONDecodeError:
            error = True

        if error:
            # Track which exercise failed for later disqualification
            lang = fname.parts[-5]
            exercise = f"{fname.parts[-2]}/{lang}"
            parse_errors.append(exercise)
            print(f"Bad results file {fname}")

    return all_results, parse_errors


def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
    PARSE_ERROR_M = 4  # Disqualify exercises with ≥M parse errors

    # Build list of (dirname, model) entries
    if dirs is None:
        dir_entries = get_dirs_from_leaderboard()
    else:
        dir_entries = [(d, d) for d in dirs]  # Use dir name as “model” label

    valid_entries = []  # [( (dirname, model), results, pass_rate ), …]
    parse_errors_by_model = {}

    dump(dir_entries)

    for dirname, model in dir_entries:
        results_data = load_results(dirname)
        if results_data:
            results, model_parse_errors = results_data
            parse_errors_by_model[model] = set(model_parse_errors)

            # Compute pass rate for custom dirs; otherwise pull from leaderboard
            if dirs is not None:
                solved = sum(
                    1
                    for r in results
                    if r.get("tests_outcomes", []) and r["tests_outcomes"][-1]
                )
                pass_rate = solved / len(results) if results else 0
            else:
                pass_rate = next(
                    (
                        entry["pass_rate_2"]
                        for entry in yaml.safe_load(
                            open("aider/website/_data/polyglot_leaderboard.yml")
                        )
                        if entry["dirname"] == dirname
                    ),
                    0,
                )

            valid_entries.append(((dirname, model), results, float(pass_rate)))

    # Sort by pass rate and truncate to topn if requested
    valid_entries.sort(key=lambda x: x[2], reverse=True)
    if topn:
        valid_entries = valid_entries[: topn]

    # Gather all exercise names (exercise/language)
    all_exercises = set()
    exercise_solutions = defaultdict(list)  # exercise → [models]

    for (dirname, model), results, _ in valid_entries:
        if results:
            for result in results:
                try:
                    all_exercises.add(f'{result["testcase"]}/{result["language"]}')
                except KeyError:
                    print(
                        f"Warning: Missing testcase in {dirname}",
                        json.dumps(result, indent=4),
                    )

    # Populate per-exercise solutions
    for (dirname, model), results, _ in valid_entries:
        if not results:
            print(f"Could not load results for {dirname}")
            continue
        for result in results:
            testcase = result.get("testcase")
            lang = result.get("language")
            if not testcase or not lang:
                continue
            testcase_combined = f"{testcase}/{lang}"
            tests_outcomes = result.get("tests_outcomes", [])
            if tests_outcomes and tests_outcomes[-1]:
                exercise_solutions[testcase_combined].append(model)

    # Ensure every exercise key exists (even if unsolved)
    for exercise in all_exercises:
        exercise_solutions.setdefault(exercise, [])

    # Per-exercise solve stats -------------------------------------------------
    total_models = len(valid_entries)

    exercise_stats = []
    for exercise in all_exercises:
        lang = exercise.split("/")[0]  # already “exercise/lang”
        models = exercise_solutions[exercise]
        num_solved = len(models)
        percent = (num_solved / total_models) * 100 if total_models else 0
        cleaned = exercise.replace("exercises/", "")
        if cleaned.startswith(f"{lang}/{lang}/"):
            cleaned = cleaned[len(lang) + 1 :]
        exercise_stats.append((lang, cleaned, num_solved, percent))

    # Sort by solve rate (desc), then name (asc)
    exercise_stats.sort(key=lambda x: (-x[2], x[1]))
    max_name_len = max(len(f"{lang}/{ex}") for lang, ex, _, _ in exercise_stats)

    print("\nAll Exercises (sorted by solve rate):")
    for i, (_, testcase, num_solved, percent) in enumerate(exercise_stats, 1):
        print(f"{i:>3}. {testcase:<{max_name_len}} : {num_solved:>3} solved ({percent:>5.1f}%)")

    # Summary -----------------------------------------------------------------
    solved_by_none = len([ex for ex, models in exercise_solutions.items() if not models])
    solved_by_all = len([ex for ex, models in exercise_solutions.items() if len(models) == total_models])
    solved_at_least_once = len(all_exercises) - solved_by_none
    never_solved = solved_by_none

    print("\nSummary:")
    print(f"Total exercises solved at least once: {solved_at_least_once}")
    print(f"Never solved by any model: {never_solved}")
    if never_solved:
        print("\nExercises never solved by any model:")
        for ex in sorted(ex for ex, models in exercise_solutions.items() if not models):
            lang, exercise = ex.split("/")
            print(f"  {lang}/exercises/practice/{exercise}")
    print(f"\nSolved by all models: {solved_by_all}")
    print(
        f"Total exercises: {len(all_exercises)} = {never_solved} (none) + "
        f"{solved_by_all} (all) + {len(all_exercises) - never_solved - solved_by_all} (some)"
    )

    # Distribution table ------------------------------------------------------
    print("\nDistribution of solutions:")
    print("Models  Exercises  Cumulative  RevCumulative")
    print("-" * 50)
    counts = [0] * (total_models + 1)
    for models in exercise_solutions.values():
        counts[len(models)] += 1

    cumsum = 0
    revcumsum = sum(counts)
    for i, count in enumerate(counts):
        cumsum += count
        print(f"{i:>6d}  {count:>9d}  {cumsum:>10d}  {revcumsum:>12d}")
        revcumsum -= count

    # Disqualify exercises with many parse errors ----------------------------
    parse_error_counts = defaultdict(int)
    for model_errors in parse_errors_by_model.values():
        for exercise in model_errors:
            parse_error_counts[exercise] += 1

    disqualified_exercises = {
        ex for ex, cnt in parse_error_counts.items() if cnt >= PARSE_ERROR_M
    }
    if disqualified_exercises:
        print(
            f"\nDisqualified {len(disqualified_exercises)} exercises with "
            f"{PARSE_ERROR_M}+ parse errors:"
        )
        for ex in sorted(disqualified_exercises):
            print(f"  {ex} ({parse_error_counts[ex]} parse errors)")

    # Hard-set (poorly solved) analysis --------------------------------------
    print(f"\nHard Set Analysis (exercises solved by ≤{HARD_SET_NUM} models):")
    print("-" * 60)
    hard_set = {
        ex
        for ex, models in exercise_solutions.items()
        if len(models) <= HARD_SET_NUM and ex not in disqualified_exercises
    }
    print(f"Total hard set exercises: {len(hard_set)}")

    # Per-language unsolved & hard-set counts
    lang_totals = defaultdict(int)
    lang_unsolved = defaultdict(int)
    lang_hard_set = defaultdict(int)

    for exercise in all_exercises:
        _, lang = exercise.split("/")
        lang_totals[lang] += 1
        if not exercise_solutions[exercise]:
            lang_unsolved[lang] += 1
        if exercise in hard_set:
            lang_hard_set[lang] += 1

    print("\nUnsolved and hard set problems by language:")
    print(f"{'Language':<12} {'Unsolved':>8} {'Hard Set':>9} {'Total':>7} {'%hardUnsolved':>8}")
    print("-" * 47)
    for lang in sorted(lang_totals):
        count = lang_unsolved[lang]
        hard = lang_hard_set[lang]
        total = lang_totals[lang]
        pct = (count / hard) * 100 if hard else -1
        print(f"{lang:<12} {count:>8} {hard:>9} {total:>7} {pct:>7.1f}%")
    print()

    # Per-model performance on hard set
    model_hard_stats = []
    for (dirname, model), results, _ in valid_entries:
        if not results:
            continue
        solved_hard = 0
        for result in results:
            testcase = result.get("testcase")
            lang = result.get("language")
            if not testcase or not lang:
                continue
            combined = f"{testcase}/{lang}"
            if combined in hard_set:
                if result.get("tests_outcomes", []) and result["tests_outcomes"][-1]:
                    solved_hard += 1
        pct = (solved_hard / len(hard_set)) * 100 if hard_set else 0
        model_hard_stats.append((model, solved_hard, pct))

    model_hard_stats.sort(key=lambda x: x[1], reverse=True)

    print("\nModel performance on hard set:")
    print(f"{'Model':<55} {'Solved':<8} {'Percent':>7}")
    print("-" * 50)
    for model, solved, pct in model_hard_stats:
        print(f"{model:<55} {solved:>6d}   {pct:>6.1f}%")

    # Optionally copy hard-set problems for manual inspection ---------------
    if copy_hard_set:
        src_dir = Path("tmp.benchmarks/exercism")
        dst_dir = Path("tmp.benchmarks/exercism-polyglot")

        if dst_dir.exists():
            print(f"\nError: destination directory {dst_dir} already exists")
            return

        print(f"\nCopying hard set problems to {dst_dir}...")
        hard_set_pairs = {tuple(exercise.split("/")) for exercise in hard_set}

        copied_by_lang = defaultdict(int)
        for lang_dir in src_dir.glob("*/exercises/practice"):
            if not lang_dir.is_dir():
                continue
            lang = lang_dir.parts[-3]
            for problem_dir in lang_dir.glob("*"):
                if (problem_dir.name, lang) in hard_set_pairs:
                    rel_path = problem_dir.relative_to(src_dir)
                    dst_path = dst_dir / rel_path
                    dst_path.parent.mkdir(parents=True, exist_ok=True)
                    shutil.copytree(problem_dir, dst_path)
                    copied_by_lang[lang] += 1

        total_copied = sum(copied_by_lang.values())
        print(f"\nCopied {total_copied} hard set problems:")
        for lang in sorted(copied_by_lang):
            print(f"  {lang}: {copied_by_lang[lang]}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--topn", type=int, help="Only consider top N models by pass rate")
    parser.add_argument(
        "dirs",
        nargs="*",
        help="Benchmark directories to analyze "
        "(default = all entries from the polyglot leaderboard)",
    )
    parser.add_argument(
        "--copy-hard-set",
        action="store_true",
        help="Copy hard set problems to tmp.benchmarks/exercism-polyglot",
    )
    args = parser.parse_args()

    analyze_exercise_solutions(
        args.dirs if args.dirs else None,
        args.topn,
        args.copy_hard_set,
    )
```