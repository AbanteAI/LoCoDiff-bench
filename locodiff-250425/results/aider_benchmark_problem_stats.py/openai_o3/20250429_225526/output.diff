
index 36481d11..7018af71 100644
--- a/aider_benchmark_problem_stats.py_expectedoutput.txt (expected):tmp/tmpfxog99r1_expected.txt	
+++ b/aider_benchmark_problem_stats.py_extracted.txt (actual):tmp/tmp9sa2ekf4_actual.txt	
@@ -1,4 +1,16 @@
 #!/usr/bin/env python
+"""
+Analyze benchmark results for Exercism “polyglot” runs.
+
+This script scans the benchmark result JSON blobs produced by aider, tallies
+which models solved which Exercism practice exercises, and prints a variety of
+stats.  It can also copy the “hard set” (poorly-solved) exercises into a new
+directory for further study.
+
+The script intentionally keeps lots of debugging and exploratory output that is
+useful when iterating on benchmarking.  Accordingly, the code style is a bit
+looser than production quality.
+"""
 
 import argparse
 import json
@@ -7,23 +19,29 @@ from collections import defaultdict
 from pathlib import Path
 
 import yaml
-
 from aider.dump import dump  # noqa
 
-HARD_SET_NUM = 3  # Number of models that defines the hard set threshold
+HARD_SET_NUM = 3  # Number of models (≤) that defines the hard-set threshold
 
 
 def get_dirs_from_leaderboard():
-    # Load the leaderboard data
+    """Return (dirname, model) tuples from the polyglot leaderboard."""
     with open("aider/website/_data/aider_benchmark_problem_stats.py_expectedoutput.txt (expected):
         leaderboard = yaml.safe_load(f)
     return [(entry["dirname"], entry["model"]) for entry in leaderboard]
 
 
 def load_results(dirname):
-    """Load all result files from a benchmark directory"""
+    """
+    Load all .aider.results.json blobs for a benchmark directory.
+
+    Returns a tuple: (results_list, parse_error_exercises)
+    – results_list            : list of dicts for successfully parsed results
+    – parse_error_exercises   : list of exercise strings that failed to parse
+    """
     dirname = Path(dirname)
 
+    # Allow callers to pass either the full path or just the leaf “benchmark id”
     benchmark_dir = dirname
     if not benchmark_dir.exists():
         benchmark_dir = Path("tmp.benchmarks") / dirname
@@ -31,63 +49,60 @@ def load_results(dirname):
             return None
 
     all_results = []
-    parse_errors = []  # Track which exercises had parse errors for this model
+    parse_errors = []
 
-    # Look in language subdirectories under exercises/practice
+    # Look in language sub-dirs: */exercises/practice/*/.aider.results.json
     for fname in benchmark_dir.glob("*/exercises/practice/*/.aider.results.json"):
         error = False
         try:
             results = json.loads(fname.read_text())
             error = "testcase" not in results
             if not error:
-                # Add language info to results
-                lang = fname.parts[-5]  # Get language from path
+                lang = fname.parts[-5]  # language component of the path
                 results["language"] = lang
                 all_results.append(results)
-
         except json.JSONDecodeError:
             error = True
 
         if error:
-            # Track the parse error for this exercise/model combination
+            # Track which exercise failed for later disqualification
             lang = fname.parts[-5]
-            exercise = f"{fname.parts[-2]}/{lang}"  # Use directory name as testcase
+            exercise = f"{fname.parts[-2]}/{lang}"
             parse_errors.append(exercise)
             print(f"Bad results file {fname}")
-            continue
 
     return all_results, parse_errors
 
 
 def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
-    PARSE_ERROR_M = 4  # Threshold for number of parse errors to DQ an exercise
+    PARSE_ERROR_M = 4  # Disqualify exercises with ≥M parse errors
 
+    # Build list of (dirname, model) entries
     if dirs is None:
-        # Use leaderboard data if no directories specified
         dir_entries = get_dirs_from_leaderboard()
     else:
-        # Use provided directories, with dirname as model name
-        dir_entries = [(d, d) for d in dirs]
+        dir_entries = [(d, d) for d in dirs]  # Use dir name as “model” label
 
-    # Filter out entries that don't load and sort by pass rate
-    valid_entries = []
-    parse_errors_by_model = {}  # Track which exercises had parse errors for each model
+    valid_entries = []  # [( (dirname, model), results, pass_rate ), …]
+    parse_errors_by_model = {}
 
     dump(dir_entries)
 
     for dirname, model in dir_entries:
         results_data = load_results(dirname)
-
         if results_data:
             results, model_parse_errors = results_data
             parse_errors_by_model[model] = set(model_parse_errors)
-            # Calculate pass rate for sorting when using custom dirs
+
+            # Compute pass rate for custom dirs; otherwise pull from leaderboard
             if dirs is not None:
-                pass_rate = sum(
-                    1 for r in results if r.get("tests_outcomes", []) and r["tests_outcomes"][-1]
-                ) / len(results)
+                solved = sum(
+                    1
+                    for r in results
+                    if r.get("tests_outcomes", []) and r["tests_outcomes"][-1]
+                )
+                pass_rate = solved / len(results) if results else 0
             else:
-                # Use existing pass rate from leaderboard
                 pass_rate = next(
                     (
                         entry["pass_rate_2"]
@@ -98,146 +113,123 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
                     ),
                     0,
                 )
+
             valid_entries.append(((dirname, model), results, float(pass_rate)))
 
-    # Sort by pass rate and take top N if specified
+    # Sort by pass rate and truncate to topn if requested
     valid_entries.sort(key=lambda x: x[2], reverse=True)
     if topn:
-        valid_entries = valid_entries[:topn]
+        valid_entries = valid_entries[: topn]
 
-    # Get all exercise names from a complete run
+    # Gather all exercise names (exercise/language)
     all_exercises = set()
-    exercise_solutions = defaultdict(list)
+    exercise_solutions = defaultdict(list)  # exercise → [models]
 
-    # Get all unique exercise names from all results
-    all_exercises = set()
     for (dirname, model), results, _ in valid_entries:
         if results:
             for result in results:
                 try:
-                    all_exercises.add(result["testcase"] + "/" + result["language"])
+                    all_exercises.add(f'{result["testcase"]}/{result["language"]}')
                 except KeyError:
-                    print(f"Warning: Missing testcase in {dirname}", json.dumps(result, indent=4))
+                    print(
+                        f"Warning: Missing testcase in {dirname}",
+                        json.dumps(result, indent=4),
+                    )
 
+    # Populate per-exercise solutions
     for (dirname, model), results, _ in valid_entries:
         if not results:
             print(f"Could not load results for {dirname}")
             continue
-
         for result in results:
             testcase = result.get("testcase")
-            if not testcase:
-                continue
             lang = result.get("language")
-            if not lang:
+            if not testcase or not lang:
                 continue
-
-            testcase = f"{testcase}/{lang}"
-            # Consider it solved if the last test attempt passed
+            testcase_combined = f"{testcase}/{lang}"
             tests_outcomes = result.get("tests_outcomes", [])
             if tests_outcomes and tests_outcomes[-1]:
-                exercise_solutions[testcase].append(model)
-
-    # Calculate never solved exercises
-    never_solved = len(all_exercises - set(exercise_solutions.keys()))
-
-    # Print per-exercise statistics
-    print("\nExercise Solution Statistics:")
-    print("-" * 40)
+                exercise_solutions[testcase_combined].append(model)
 
-    # Add exercises that were never solved
+    # Ensure every exercise key exists (even if unsolved)
     for exercise in all_exercises:
-        if exercise not in exercise_solutions:
-            exercise_solutions[exercise] = []
+        exercise_solutions.setdefault(exercise, [])
 
-    # Create list of (language, exercise) pairs with solution stats
-    exercise_stats = []
+    # Per-exercise solve stats -------------------------------------------------
     total_models = len(valid_entries)
 
-    for testcase in all_exercises:
-        # Language is already in the testcase string
-        lang = testcase.split("/")[0]  # First part is the language
-        models = exercise_solutions[testcase]
+    exercise_stats = []
+    for exercise in all_exercises:
+        lang = exercise.split("/")[0]  # already “exercise/lang”
+        models = exercise_solutions[exercise]
         num_solved = len(models)
-        percent = (num_solved / total_models) * 100
-        testcase = testcase.replace("exercises/", "")  # Remove the exercises/ prefix
-        # Remove duplicate language prefix (e.g. javascript/javascript/ -> javascript/)
-        if testcase.startswith(f"{lang}/{lang}/"):
-            testcase = testcase[len(lang) + 1 :]
-        exercise_stats.append((lang, testcase, num_solved, percent))
-
-    # Sort all exercises by solve rate, then by exercise name
-    exercise_stats.sort(
-        key=lambda x: (-x[2], x[1])
-    )  # -x[2] for descending solve rate, x[1] for ascending exercise name
-
-    # Calculate max lengths for alignment after cleaning up paths
-    max_name_len = max(len(f"{lang}/{testcase}") for lang, testcase, _, _ in exercise_stats)
-
-    # Print all exercises sorted by solve rate
+        percent = (num_solved / total_models) * 100 if total_models else 0
+        cleaned = exercise.replace("exercises/", "")
+        if cleaned.startswith(f"{lang}/{lang}/"):
+            cleaned = cleaned[len(lang) + 1 :]
+        exercise_stats.append((lang, cleaned, num_solved, percent))
+
+    # Sort by solve rate (desc), then name (asc)
+    exercise_stats.sort(key=lambda x: (-x[2], x[1]))
+    max_name_len = max(len(f"{lang}/{ex}") for lang, ex, _, _ in exercise_stats)
+
     print("\nAll Exercises (sorted by solve rate):")
-    for i, (lang, testcase, num_solved, percent) in enumerate(exercise_stats, 1):
+    for i, (_, testcase, num_solved, percent) in enumerate(exercise_stats, 1):
         print(f"{i:>3}. {testcase:<{max_name_len}} : {num_solved:>3} solved ({percent:>5.1f}%)")
 
-    print("\nSummary:")
-    solved_at_least_once = len([ex for ex, models in exercise_solutions.items() if models])
-    solved_by_none = never_solved
-    solved_by_all = len(
-        [ex for ex, models in exercise_solutions.items() if len(models) == total_models]
-    )
+    # Summary -----------------------------------------------------------------
+    solved_by_none = len([ex for ex, models in exercise_solutions.items() if not models])
+    solved_by_all = len([ex for ex, models in exercise_solutions.items() if len(models) == total_models])
+    solved_at_least_once = len(all_exercises) - solved_by_none
+    never_solved = solved_by_none
 
+    print("\nSummary:")
     print(f"Total exercises solved at least once: {solved_at_least_once}")
-    print(f"Never solved by any model: {solved_by_none}")
-    if solved_by_none > 0:
+    print(f"Never solved by any model: {never_solved}")
+    if never_solved:
         print("\nExercises never solved by any model:")
-        unsolved = [ex for ex, models in exercise_solutions.items() if not models]
-        for ex in sorted(unsolved):
-            # Split into language and exercise parts
+        for ex in sorted(ex for ex, models in exercise_solutions.items() if not models):
             lang, exercise = ex.split("/")
-            # Reconstruct path in desired format
-            formatted_path = f"{lang}/exercises/practice/{exercise}"
-            print(f"  {formatted_path}")
+            print(f"  {lang}/exercises/practice/{exercise}")
     print(f"\nSolved by all models: {solved_by_all}")
     print(
-        f"Total exercises: {len(all_exercises)} = {solved_by_none} (none) + {solved_by_all} (all) +"
-        f" {len(all_exercises) - solved_by_none - solved_by_all} (some)"
+        f"Total exercises: {len(all_exercises)} = {never_solved} (none) + "
+        f"{solved_by_all} (all) + {len(all_exercises) - never_solved - solved_by_all} (some)"
     )
 
-    # Distribution table of how many models solved each exercise
+    # Distribution table ------------------------------------------------------
     print("\nDistribution of solutions:")
     print("Models  Exercises  Cumulative  RevCumulative")
     print("-" * 50)
     counts = [0] * (total_models + 1)
-    for ex, models in exercise_solutions.items():
+    for models in exercise_solutions.values():
         counts[len(models)] += 1
 
     cumsum = 0
-    revcumsum = sum(counts)  # Start with total number of exercises
+    revcumsum = sum(counts)
     for i, count in enumerate(counts):
         cumsum += count
         print(f"{i:>6d}  {count:>9d}  {cumsum:>10d}  {revcumsum:>12d}")
-        revcumsum -= count  # Decrement the reverse cumulative sum
+        revcumsum -= count
 
-    # Count parse errors per exercise
+    # Disqualify exercises with many parse errors ----------------------------
     parse_error_counts = defaultdict(int)
     for model_errors in parse_errors_by_model.values():
         for exercise in model_errors:
             parse_error_counts[exercise] += 1
 
-    # Find exercises to disqualify based on parse error threshold
     disqualified_exercises = {
-        exercise for exercise, count in parse_error_counts.items() if count >= PARSE_ERROR_M
+        ex for ex, cnt in parse_error_counts.items() if cnt >= PARSE_ERROR_M
     }
-
     if disqualified_exercises:
         print(
-            f"\nDisqualified {len(disqualified_exercises)} exercises with {PARSE_ERROR_M}+ parse"
-            " errors:"
+            f"\nDisqualified {len(disqualified_exercises)} exercises with "
+            f"{PARSE_ERROR_M}+ parse errors:"
         )
         for ex in sorted(disqualified_exercises):
             print(f"  {ex} ({parse_error_counts[ex]} parse errors)")
 
-    # Collect the hard set (exercises solved by HARD_SET_NUM or fewer models)
+    # Hard-set (poorly solved) analysis --------------------------------------
     print(f"\nHard Set Analysis (exercises solved by ≤{HARD_SET_NUM} models):")
     print("-" * 60)
     hard_set = {
@@ -247,23 +239,23 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
     }
     print(f"Total hard set exercises: {len(hard_set)}")
 
-    # Count total problems, unsolved problems, and hard set problems by language
+    # Per-language unsolved & hard-set counts
     lang_totals = defaultdict(int)
     lang_unsolved = defaultdict(int)
     lang_hard_set = defaultdict(int)
 
     for exercise in all_exercises:
-        lang = exercise.split("/")[1]  # Get language from path
+        _, lang = exercise.split("/")
         lang_totals[lang] += 1
-        if not exercise_solutions[exercise]:  # No models solved this exercise
+        if not exercise_solutions[exercise]:
             lang_unsolved[lang] += 1
-        if exercise in hard_set:  # Exercise is in the hard set
+        if exercise in hard_set:
             lang_hard_set[lang] += 1
 
     print("\nUnsolved and hard set problems by language:")
     print(f"{'Language':<12} {'Unsolved':>8} {'Hard Set':>9} {'Total':>7} {'%hardUnsolved':>8}")
     print("-" * 47)
-    for lang in sorted(lang_totals.keys()):
+    for lang in sorted(lang_totals):
         count = lang_unsolved[lang]
         hard = lang_hard_set[lang]
         total = lang_totals[lang]
@@ -271,31 +263,24 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
         print(f"{lang:<12} {count:>8} {hard:>9} {total:>7} {pct:>7.1f}%")
     print()
 
-    # For each model, compute performance on hard set
+    # Per-model performance on hard set
     model_hard_stats = []
     for (dirname, model), results, _ in valid_entries:
         if not results:
             continue
-
         solved_hard = 0
         for result in results:
             testcase = result.get("testcase")
-            if not testcase:
-                continue
             lang = result.get("language")
-            if not lang:
+            if not testcase or not lang:
                 continue
-
-            testcase = f"{testcase}/{lang}"
-            if testcase in hard_set:
-                tests_outcomes = result.get("tests_outcomes", [])
-                if tests_outcomes and tests_outcomes[-1]:
+            combined = f"{testcase}/{lang}"
+            if combined in hard_set:
+                if result.get("tests_outcomes", []) and result["tests_outcomes"][-1]:
                     solved_hard += 1
-
-        pct = (solved_hard / len(hard_set)) * 100
+        pct = (solved_hard / len(hard_set)) * 100 if hard_set else 0
         model_hard_stats.append((model, solved_hard, pct))
 
-    # Sort by number solved
     model_hard_stats.sort(key=lambda x: x[1], reverse=True)
 
     print("\nModel performance on hard set:")
@@ -304,27 +289,23 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
     for model, solved, pct in model_hard_stats:
         print(f"{model:<55} {solved:>6d}   {pct:>6.1f}%")
 
+    # Optionally copy hard-set problems for manual inspection ---------------
     if copy_hard_set:
-        # Create hard set directory
         src_dir = Path("tmp.benchmarks/exercism")
         dst_dir = Path("tmp.benchmarks/exercism-polyglot")
 
         if dst_dir.exists():
-            print(f"\nError: Destination directory {dst_dir} already exists")
+            print(f"\nError: destination directory {dst_dir} already exists")
             return
 
         print(f"\nCopying hard set problems to {dst_dir}...")
-
-        # Create a set of (exercise, language) pairs from hard_set
         hard_set_pairs = {tuple(exercise.split("/")) for exercise in hard_set}
 
-        # Copy each hard set problem's directory
         copied_by_lang = defaultdict(int)
         for lang_dir in src_dir.glob("*/exercises/practice"):
             if not lang_dir.is_dir():
                 continue
-
-            lang = lang_dir.parts[-3]  # Get language from path
+            lang = lang_dir.parts[-3]
             for problem_dir in lang_dir.glob("*"):
                 if (problem_dir.name, lang) in hard_set_pairs:
                     rel_path = problem_dir.relative_to(src_dir)
@@ -343,13 +324,20 @@ if __name__ == "__main__":
     parser = argparse.ArgumentParser()
     parser.add_argument("--topn", type=int, help="Only consider top N models by pass rate")
     parser.add_argument(
-        "dirs", nargs="*", help="Directories to analyze (optional, defaults to leaderboard entries)"
+        "dirs",
+        nargs="*",
+        help="Benchmark directories to analyze "
+        "(default = all entries from the polyglot leaderboard)",
     )
     parser.add_argument(
         "--copy-hard-set",
         action="store_true",
-        help="Copy hard set problems to tmp.benchmarks/exercism-polygot",
+        help="Copy hard set problems to tmp.benchmarks/exercism-polyglot",
     )
     args = parser.parse_args()
 
-    analyze_exercise_solutions(args.dirs if args.dirs else None, args.topn, args.copy_hard_set)
\ No newline at end of file
+    analyze_exercise_solutions(
+        args.dirs if args.dirs else None,
+        args.topn,
+        args.copy_hard_set,
+    )
\ No newline at end of file
