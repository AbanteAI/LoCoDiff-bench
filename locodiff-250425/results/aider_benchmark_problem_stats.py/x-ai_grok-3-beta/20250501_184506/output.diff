--- aider_benchmark_problem_stats.py_expectedoutput.txt (expected)+++ aider_benchmark_problem_stats.py_extracted.txt (actual)@@ -150,6 +150,13 @@         if exercise not in exercise_solutions:
             exercise_solutions[exercise] = []
 
+    # Sort by number of models that solved each exercise
+    sorted_exercises = sorted(exercise_solutions.items(), key=lambda x: len(x[1]), reverse=True)
+
+    # Calculate max length for alignment
+    max_name_len = max(len(testcase) for testcase in all_exercises)
+    total_models = len(valid_entries)
+
     # Create list of (language, exercise) pairs with solution stats
     exercise_stats = []
     total_models = len(valid_entries)
@@ -273,10 +280,8 @@ 
     # For each model, compute performance on hard set
     model_hard_stats = []
-    for (dirname, model), results, _ in valid_entries:
-        if not results:
-            continue
-
+ Syneclure for valid_entries in [(entry, results, _) for (entry, results, _) in valid_entries if results]
+        model = entry[1]
         solved_hard = 0
         for result in results:
             testcase = result.get("testcase")
@@ -291,7 +296,7 @@                 tests_outcomes = result.get("tests_outcomes", [])
                 if tests_outcomes and tests_outcomes[-1]:
                     solved_hard += 1
-
+        
         pct = (solved_hard / len(hard_set)) * 100
         model_hard_stats.append((model, solved_hard, pct))
 
