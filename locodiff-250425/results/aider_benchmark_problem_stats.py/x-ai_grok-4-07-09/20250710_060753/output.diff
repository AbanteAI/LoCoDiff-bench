
index 36481d117..8d5d17ad2 100644
--- a/aider_benchmark_problem_stats.py_expectedoutput.txt (expected):tmp/tmpv07ztp_x_expected.txt	
+++ b/aider_benchmark_problem_stats.py_extracted.txt (actual):tmp/tmpo76ljvtv_actual.txt	
@@ -73,11 +73,8 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
     valid_entries = []
     parse_errors_by_model = {}  # Track which exercises had parse errors for each model
 
-    dump(dir_entries)
-
     for dirname, model in dir_entries:
         results_data = load_results(dirname)
-
         if results_data:
             results, model_parse_errors = results_data
             parse_errors_by_model[model] = set(model_parse_errors)
@@ -117,7 +114,7 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
                 try:
                     all_exercises.add(result["testcase"] + "/" + result["language"])
                 except KeyError:
-                    print(f"Warning: Missing testcase in {dirname}", json.dumps(result, indent=4))
+                    print(f"Warning: Missing testcase in {dirname}")
 
     for (dirname, model), results, _ in valid_entries:
         if not results:
@@ -138,13 +135,6 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
             if tests_outcomes and tests_outcomes[-1]:
                 exercise_solutions[testcase].append(model)
 
-    # Calculate never solved exercises
-    never_solved = len(all_exercises - set(exercise_solutions.keys()))
-
-    # Print per-exercise statistics
-    print("\nExercise Solution Statistics:")
-    print("-" * 40)
-
     # Add exercises that were never solved
     for exercise in all_exercises:
         if exercise not in exercise_solutions:
@@ -156,7 +146,7 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
 
     for testcase in all_exercises:
         # Language is already in the testcase string
-        lang = testcase.split("/")[0]  # First part is the language
+        lang = testcase.split("/")[1]  # First part is the language
         models = exercise_solutions[testcase]
         num_solved = len(models)
         percent = (num_solved / total_models) * 100
@@ -177,15 +167,16 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
     # Print all exercises sorted by solve rate
     print("\nAll Exercises (sorted by solve rate):")
     for i, (lang, testcase, num_solved, percent) in enumerate(exercise_stats, 1):
-        print(f"{i:>3}. {testcase:<{max_name_len}} : {num_solved:>3} solved ({percent:>5.1f}%)")
+        print(
+            f"{i:>3}. {lang}/{testcase:<{max_name_len}} : {num_solved:>3} solved ({percent:>5.1f}%)"
+        )
 
     print("\nSummary:")
     solved_at_least_once = len([ex for ex, models in exercise_solutions.items() if models])
-    solved_by_none = never_solved
+    solved_by_none = len([ex for ex, models in exercise_solutions.items() if not models])
     solved_by_all = len(
         [ex for ex, models in exercise_solutions.items() if len(models) == total_models]
     )
-
     print(f"Total exercises solved at least once: {solved_at_least_once}")
     print(f"Never solved by any model: {solved_by_none}")
     if solved_by_none > 0:
@@ -197,7 +188,7 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
             # Reconstruct path in desired format
             formatted_path = f"{lang}/exercises/practice/{exercise}"
             print(f"  {formatted_path}")
-    print(f"\nSolved by all models: {solved_by_all}")
+    print(f"Solved by all models: {solved_by_all}")
     print(
         f"Total exercises: {len(all_exercises)} = {solved_by_none} (none) + {solved_by_all} (all) +"
         f" {len(all_exercises) - solved_by_none - solved_by_all} (some)"
@@ -231,8 +222,7 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
 
     if disqualified_exercises:
         print(
-            f"\nDisqualified {len(disqualified_exercises)} exercises with {PARSE_ERROR_M}+ parse"
-            " errors:"
+            f"\nDisqualified {len(disqualified_exercises)} exercises with {PARSE_ERROR_M}+ parse errors:"
         )
         for ex in sorted(disqualified_exercises):
             print(f"  {ex} ({parse_error_counts[ex]} parse errors)")
@@ -348,7 +338,7 @@ if __name__ == "__main__":
     parser.add_argument(
         "--copy-hard-set",
         action="store_true",
-        help="Copy hard set problems to tmp.benchmarks/exercism-polygot",
+        help="Copy hard set problems to tmp.benchmarks/exercism-polyglot",
     )
     args = parser.parse_args()
 
