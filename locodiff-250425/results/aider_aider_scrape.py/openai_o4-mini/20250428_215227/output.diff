
index c315012c..a8582a45 100644
--- a/aider_aider_scrape.py_expectedoutput.txt (expected):tmp/tmpfit_84w3_expected.txt	
+++ b/aider_aider_scrape.py_extracted.txt (actual):tmp/tmpm8kpw8u4_actual.txt	
@@ -10,14 +10,10 @@ from aider.dump import dump  # noqa: F401
 
 aider_user_agent = f"Aider/{__version__} +{urls.website}"
 
-# Playwright is nice because it has a simple way to install dependencies on most
-# platforms.
-
 
 def install_playwright(io):
     try:
         from playwright.sync_api import sync_playwright
-
         has_pip = True
     except ImportError:
         has_pip = False
@@ -45,9 +41,8 @@ def install_playwright(io):
     text = f"""For the best web scraping, install Playwright:
 
 {cmds}
-See {urls.enable_playwright} for more info.
-"""
-
+ See {urls.enable_playwright} for more info.
+ """
     io.tool_output(text)
     if not io.confirm_ask("Install playwright?", default="y"):
         return
@@ -69,9 +64,7 @@ See {urls.enable_playwright} for more info.
 class Scraper:
     pandoc_available = None
     playwright_available = None
-    playwright_instructions_shown = False
 
-    # Public API...
     def __init__(self, print_error=None, playwright_available=None, verify_ssl=True):
         """
         `print_error` - a function to call to print error/debug info.
@@ -92,7 +85,6 @@ class Scraper:
 
         `url` - the URL to scrape.
         """
-
         if self.playwright_available:
             content, mime_type = self.scrape_with_playwright(url)
         else:
@@ -111,25 +103,6 @@ class Scraper:
 
         return content
 
-    def looks_like_html(self, content):
-        """
-        Check if the content looks like HTML.
-        """
-        if isinstance(content, str):
-            # Check for common HTML tags
-            html_patterns = [
-                r"<!DOCTYPE\s+html",
-                r"<html",
-                r"<head",
-                r"<body",
-                r"<div",
-                r"<p>",
-                r"<a\s+href=",
-            ]
-            return any(re.search(pattern, content, re.IGNORECASE) for pattern in html_patterns)
-        return False
-
-    # Internals...
     def scrape_with_playwright(self, url):
         import playwright  # noqa: F401
         from playwright.sync_api import Error as PlaywrightError
@@ -176,10 +149,10 @@ class Scraper:
                     self.print_error(f"Error retrieving page content: {str(e)}")
                     content = None
                     mime_type = None
-            finally:
-                browser.close()
+                finally:
+                    browser.close()
 
-        return content, mime_type
+            return content, mime_type
 
     def scrape_with_httpx(self, url):
         import httpx
@@ -234,25 +207,44 @@ class Scraper:
 
         md = re.sub(r"</div>", "      ", md)
         md = re.sub(r"<div>", "     ", md)
-
         md = re.sub(r"\n\s*\n", "\n\n", md)
 
         return md
 
+    def looks_like_html(self, content):
+        """
+        Check if the content looks like HTML.
+        """
+        if isinstance(content, str):
+            html_patterns = [
+                r"<!DOCTYPE\s+html",
+                r"<html",
+                r"<head",
+                r"<body",
+                r"<div",
+                r"<p>",
+                r"<a\s+href=",
+            ]
+            return any(re.search(pattern, content, re.IGNORECASE) for pattern in html_patterns)
+        return False
+
 
 def slimdown_html(soup):
+    # Remove all <svg> tags
     for svg in soup.find_all("svg"):
         svg.decompose()
 
-    if soup.img:
-        soup.img.decompose()
+    # Remove all <img> tags
+    for img in soup.find_all("img"):
+        img.decompose()
 
+    # Remove all elements with data: URLs
     for tag in soup.find_all(href=lambda x: x and x.startswith("data:")):
         tag.decompose()
-
     for tag in soup.find_all(src=lambda x: x and x.startswith("data:")):
         tag.decompose()
 
+    # Remove all attributes except href
     for tag in soup.find_all(True):
         for attr in list(tag.attrs):
             if attr != "href":
