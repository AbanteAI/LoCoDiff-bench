
index c315012c..a6e9f5ab 100644
--- a/aider_aider_scrape.py_expectedoutput.txt (expected):tmp/tmpwa42vt3g_expected.txt	
+++ b/aider_aider_scrape.py_extracted.txt (actual):tmp/tmplkdu5ou8_actual.txt	
@@ -10,9 +10,6 @@ from aider.dump import dump  # noqa: F401
 
 aider_user_agent = f"Aider/{__version__} +{urls.website}"
 
-# Playwright is nice because it has a simple way to install dependencies on most
-# platforms.
-
 
 def install_playwright(io):
     try:
@@ -176,8 +173,13 @@ class Scraper:
                     self.print_error(f"Error retrieving page content: {str(e)}")
                     content = None
                     mime_type = None
-            finally:
-                browser.close()
+                finally:
+                    browser.close()
+
+            except PlaywrightError as e:
+                self.print_error(f"Playwright error: {str(e)}")
+                content = None
+                mime_type = None
 
         return content, mime_type
 
@@ -221,7 +223,7 @@ class Scraper:
         from bs4 import BeautifulSoup
 
         soup = BeautifulSoup(page_source, "html.parser")
-        soup = slimdown_html(soup)
+        soup = self.slimdown_html(soup)
         page_source = str(soup)
 
         if not self.pandoc_available:
@@ -239,26 +241,25 @@ class Scraper:
 
         return md
 
+    def slimdown_html(self, soup):
+        for svg in soup.find_all("svg"):
+            svg.decompose()
 
-def slimdown_html(soup):
-    for svg in soup.find_all("svg"):
-        svg.decompose()
-
-    if soup.img:
-        soup.img.decompose()
+        if soup.img:
+            soup.img.decompose()
 
-    for tag in soup.find_all(href=lambda x: x and x.startswith("data:")):
-        tag.decompose()
+        for tag in soup.find_all(href=lambda x: x and x.startswith("data:")):
+            tag.decompose()
 
-    for tag in soup.find_all(src=lambda x: x and x.startswith("data:")):
-        tag.decompose()
+        for tag in soup.find_all(src=lambda x: x and x.startswith("data:")):
+            tag.decompose()
 
-    for tag in soup.find_all(True):
-        for attr in list(tag.attrs):
-            if attr != "href":
-                tag.attrs.pop(attr, None)
+        for tag in soup.find_all(True):
+            for attr in list(tag.attrs):
+                if attr != "href":
+                    tag.attrs.pop(attr, None)
 
-    return soup
+        return soup
 
 
 def main(url):
