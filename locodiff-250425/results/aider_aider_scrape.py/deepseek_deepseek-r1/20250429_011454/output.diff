--- aider_aider_scrape.py_expectedoutput.txt (expected)+++ aider_aider_scrape.py_extracted.txt (actual)@@ -9,9 +9,6 @@ from aider.dump import dump  # noqa: F401
 
 aider_user_agent = f"Aider/{__version__} +{urls.website}"
-
-# Playwright is nice because it has a simple way to install dependencies on most
-# platforms.
 
 
 def install_playwright(io):
@@ -176,8 +173,13 @@                     self.print_error(f"Error retrieving page content: {str(e)}")
                     content = None
                     mime_type = None
-            finally:
-                browser.close()
+                finally:
+                    browser.close()
+
+            except PlaywrightError as e:
+                self.print_error(f"Playwright error: {str(e)}")
+                content = None
+                mime_type = None
 
         return content, mime_type
 
@@ -221,7 +223,7 @@         from bs4 import BeautifulSoup
 
         soup = BeautifulSoup(page_source, "html.parser")
-        soup = slimdown_html(soup)
+        soup = self.slimdown_html(soup)
         page_source = str(soup)
 
         if not self.pandoc_available:
@@ -239,26 +241,25 @@ 
         return md
 
-
-def slimdown_html(soup):
-    for svg in soup.find_all("svg"):
-        svg.decompose()
-
-    if soup.img:
-        soup.img.decompose()
-
-    for tag in soup.find_all(href=lambda x: x and x.startswith("data:")):
-        tag.decompose()
-
-    for tag in soup.find_all(src=lambda x: x and x.startswith("data:")):
-        tag.decompose()
-
-    for tag in soup.find_all(True):
-        for attr in list(tag.attrs):
-            if attr != "href":
-                tag.attrs.pop(attr, None)
-
-    return soup
+    def slimdown_html(self, soup):
+        for svg in soup.find_all("svg"):
+            svg.decompose()
+
+        if soup.img:
+            soup.img.decompose()
+
+        for tag in soup.find_all(href=lambda x: x and x.startswith("data:")):
+            tag.decompose()
+
+        for tag in soup.find_all(src=lambda x: x and x.startswith("data:")):
+            tag.decompose()
+
+        for tag in soup.find_all(True):
+            for attr in list(tag.attrs):
+                if attr != "href":
+                    tag.attrs.pop(attr, None)
+
+        return soup
 
 
 def main(url):
