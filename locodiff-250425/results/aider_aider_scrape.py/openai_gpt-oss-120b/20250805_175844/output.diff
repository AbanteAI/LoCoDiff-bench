
index c315012c0..86efe55c1 100644
--- a/aider_aider_scrape.py_expectedoutput.txt (expected):tmp/tmpqyz39owd_expected.txt	
+++ b/aider_aider_scrape.py_extracted.txt (actual):tmp/tmpwd4sxhqi_actual.txt	
@@ -2,18 +2,13 @@
 
 import re
 import sys
+import re
 
 import pypandoc
 
 from aider import __version__, urls, utils
 from aider.dump import dump  # noqa: F401
 
-aider_user_agent = f"Aider/{__version__} +{urls.website}"
-
-# Playwright is nice because it has a simple way to install dependencies on most
-# platforms.
-
-
 def install_playwright(io):
     try:
         from playwright.sync_api import sync_playwright
@@ -47,9 +42,8 @@ def install_playwright(io):
 {cmds}
 See {urls.enable_playwright} for more info.
 """
-
     io.tool_output(text)
-    if not io.confirm_ask("Install playwright?", default="y"):
+    if not io.confirm_ask("Install Playwright?", default="y"):
         return
 
     if not has_pip:
@@ -71,7 +65,6 @@ class Scraper:
     playwright_available = None
     playwright_instructions_shown = False
 
-    # Public API...
     def __init__(self, print_error=None, playwright_available=None, verify_ssl=True):
         """
         `print_error` - a function to call to print error/debug info.
@@ -92,7 +85,6 @@ class Scraper:
 
         `url` - the URL to scrape.
         """
-
         if self.playwright_available:
             content, mime_type = self.scrape_with_playwright(url)
         else:
@@ -102,7 +94,6 @@ class Scraper:
             self.print_error(f"Failed to retrieve content from {url}")
             return None
 
-        # Check if the content is HTML based on MIME type or content
         if (mime_type and mime_type.startswith("text/html")) or (
             mime_type is None and self.looks_like_html(content)
         ):
@@ -116,7 +107,6 @@ class Scraper:
         Check if the content looks like HTML.
         """
         if isinstance(content, str):
-            # Check for common HTML tags
             html_patterns = [
                 r"<!DOCTYPE\s+html",
                 r"<html",
@@ -129,12 +119,13 @@ class Scraper:
             return any(re.search(pattern, content, re.IGNORECASE) for pattern in html_patterns)
         return False
 
-    # Internals...
+    # Internals -----------------------------------------------------------
+
     def scrape_with_playwright(self, url):
         import playwright  # noqa: F401
+        from playwright.sync_api import sync_playwright
         from playwright.sync_api import Error as PlaywrightError
         from playwright.sync_api import TimeoutError as PlaywrightTimeoutError
-        from playwright.sync_api import sync_playwright
 
         with sync_playwright() as p:
             try:
@@ -145,58 +136,63 @@ class Scraper:
                 return None, None
 
             try:
-                context = browser.new_context(ignore_https_errors=not self.verify_ssl)
+                context = browser.new_context()
                 page = context.new_page()
+            except Exception as e:
+                self.print_error(str(e))
+                return None, None
 
-                user_agent = page.evaluate("navigator.userAgent")
-                user_agent = user_agent.replace("Headless", "")
-                user_agent = user_agent.replace("headless", "")
-                user_agent += " " + aider_user_agent
+            # Determine user agent
+            user_agent = page.evaluate("navigator.userAgent")
+            user_agent = user_agent.replace("Headless", "")
+            user_agent = user_agent.replace("headless", "")
+            user_agent = f"{user_agent} {aider_user_agent}"
 
-                page.set_extra_http_headers({"User-Agent": user_agent})
+            page.set_extra_http_headers({"User-Agent": user_agent})
 
+            try:
+                response = page.goto(url, wait_until="networkidle", timeout=5000)
+            except PlaywrightTimeoutError:
+                print(f"Page didn't quiesce, scraping content anyway: {url}")
                 response = None
-                try:
-                    response = page.goto(url, wait_until="networkidle", timeout=5000)
-                except PlaywrightTimeoutError:
-                    print(f"Page didn't quiesce, scraping content anyway: {url}")
-                    response = None
-                except PlaywrightError as e:
-                    self.print_error(f"Error navigating to {url}: {str(e)}")
-                    return None, None
-
-                try:
-                    content = page.content()
-                    mime_type = None
-                    if response:
-                        content_type = response.header_value("content-type")
-                        if content_type:
-                            mime_type = content_type.split(";")[0]
-                except PlaywrightError as e:
-                    self.print_error(f"Error retrieving page content: {str(e)}")
-                    content = None
-                    mime_type = None
+            except PlaywrightError as e:
+                self.print_error(f"Error navigating to {url}: {str(e)}")
+                return None, None
+
+            try:
+                content = page.content()
+                mime_type = None
+                if response:
+                    content_type = response.header_value("content-type")
+                    if content_type:
+                        mime_type = content_type.split(";")[0]
+            except PlaywrightError as e:
+                self.print_error(f"Error retrieving page content: {str(e)}")
+                content = None
+                mime_type = None
             finally:
                 browser.close()
 
-        return content, mime_type
+            return content, mime_type
 
     def scrape_with_httpx(self, url):
         import httpx
-
         headers = {"User-Agent": f"Mozilla./5.0 ({aider_user_agent})"}
         try:
             with httpx.Client(
-                headers=headers, verify=self.verify_ssl, follow_redirects=True
+                headers=headers,
+                verify=self.verify_ssl,
+                follow_redirects=True,
             ) as client:
                 response = client.get(url)
                 response.raise_for_status()
-                return response.text, response.headers.get("content-type", "").split(";")[0]
-        except httpx.HTTPError as http_err:
-            self.print_error(f"HTTP error occurred: {http_err}")
-        except Exception as err:
-            self.print_error(f"An error occurred: {err}")
-        return None, None
+                content_type = response.headers.get("content-type", "").split(";")[0]
+                return response.text, content_type
+            except httpx.HTTPError as http_err:
+                self.print_error(f"HTTP error occurred: {http_err}")
+            except Exception as err:
+                self.print_error(f"An error occurred: {err}")
+            return None, None
 
     def try_pandoc(self):
         if self.pandoc_available:
@@ -234,41 +230,44 @@ class Scraper:
 
         md = re.sub(r"</div>", "      ", md)
         md = re.sub(r"<div>", "     ", md)
-
         md = re.sub(r"\n\s*\n", "\n\n", md)
-
         return md
 
 
 def slimdown_html(soup):
-    for svg in soup.find_all("svg"):
-        svg.decompose()
-
-    if soup.img:
-        soup.img.decompose()
+    # Remove all <img> tags
+    for img in soup.find_all("img"):
+        img.decompose()
 
+    # Remove all elements with data: URLs
     for tag in soup.find_all(href=lambda x: x and x.startswith("data:")):
         tag.decompose()
-
     for tag in soup.find_all(src=lambda x: x and x.startswith("data:")):
         tag.decompose()
 
+    # Remove per-element CSS styles, preserving href attributes
     for tag in soup.find_all(True):
         for attr in list(tag.attrs):
             if attr != "href":
                 tag.attrs.pop(attr, None)
 
+    # Remove internal anchor elements
+    for anchor in soup.find_all("a", href=True):
+        if anchor["href"].startswith("#"):
+            anchor.decompose()
+
     return soup
 
 
 def main(url):
     scraper = Scraper()
     content = scraper.scrape(url)
-    print(content)
+    if content is not None:
+        print(content)
 
 
 if __name__ == "__main__":
     if len(sys.argv) < 2:
-        print("Usage: python playw.py <URL>")
+        print("Usage: python scrape.py <URL>")
         sys.exit(1)
     main(sys.argv[1])
\ No newline at end of file
