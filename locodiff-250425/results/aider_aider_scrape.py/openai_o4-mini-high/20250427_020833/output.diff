
index c315012c..cd056e05 100644
--- a/aider_aider_scrape.py_expectedoutput.txt (expected):tmp/tmpe4o0fm61_expected.txt	
+++ b/aider_aider_scrape.py_extracted.txt (actual):tmp/tmpm96_7q13_actual.txt	
@@ -10,9 +10,6 @@ from aider.dump import dump  # noqa: F401
 
 aider_user_agent = f"Aider/{__version__} +{urls.website}"
 
-# Playwright is nice because it has a simple way to install dependencies on most
-# platforms.
-
 
 def install_playwright(io):
     try:
@@ -26,7 +23,8 @@ def install_playwright(io):
         with sync_playwright() as p:
             p.chromium.launch()
             has_chromium = True
-    except Exception:
+    except Exception as err:
+        dump(err)
         has_chromium = False
 
     if has_pip and has_chromium:
@@ -98,6 +96,7 @@ class Scraper:
         else:
             content, mime_type = self.scrape_with_httpx(url)
 
+        dump(content)
         if not content:
             self.print_error(f"Failed to retrieve content from {url}")
             return None
@@ -111,25 +110,6 @@ class Scraper:
 
         return content
 
-    def looks_like_html(self, content):
-        """
-        Check if the content looks like HTML.
-        """
-        if isinstance(content, str):
-            # Check for common HTML tags
-            html_patterns = [
-                r"<!DOCTYPE\s+html",
-                r"<html",
-                r"<head",
-                r"<body",
-                r"<div",
-                r"<p>",
-                r"<a\s+href=",
-            ]
-            return any(re.search(pattern, content, re.IGNORECASE) for pattern in html_patterns)
-        return False
-
-    # Internals...
     def scrape_with_playwright(self, url):
         import playwright  # noqa: F401
         from playwright.sync_api import Error as PlaywrightError
@@ -239,6 +219,24 @@ class Scraper:
 
         return md
 
+    def looks_like_html(self, content):
+        """
+        Check if the content looks like HTML.
+        """
+        if isinstance(content, str):
+            # Check for common HTML tags
+            html_patterns = [
+                r"<!DOCTYPE\s+html",
+                r"<html",
+                r"<head",
+                r"<body",
+                r"<div",
+                r"<p>",
+                r"<a\s+href=",
+            ]
+            return any(re.search(pattern, content, re.IGNORECASE) for pattern in html_patterns)
+        return False
+
 
 def slimdown_html(soup):
     for svg in soup.find_all("svg"):
@@ -249,14 +247,11 @@ def slimdown_html(soup):
 
     for tag in soup.find_all(href=lambda x: x and x.startswith("data:")):
         tag.decompose()
-
     for tag in soup.find_all(src=lambda x: x and x.startswith("data:")):
         tag.decompose()
 
     for tag in soup.find_all(True):
-        for attr in list(tag.attrs):
-            if attr != "href":
-                tag.attrs.pop(attr, None)
+        tag.attrs.clear()
 
     return soup
 
