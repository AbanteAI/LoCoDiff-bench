--- aider_aider_scrape.py_expectedoutput.txt (expected)+++ aider_aider_scrape.py_extracted.txt (actual)@@ -9,9 +9,6 @@ from aider.dump import dump  # noqa: F401
 
 aider_user_agent = f"Aider/{__version__} +{urls.website}"
-
-# Playwright is nice because it has a simple way to install dependencies on most
-# platforms.
 
 
 def install_playwright(io):
@@ -26,7 +23,8 @@         with sync_playwright() as p:
             p.chromium.launch()
             has_chromium = True
-    except Exception:
+    except Exception as err:
+        dump(err)
         has_chromium = False
 
     if has_pip and has_chromium:
@@ -98,6 +96,7 @@         else:
             content, mime_type = self.scrape_with_httpx(url)
 
+        dump(content)
         if not content:
             self.print_error(f"Failed to retrieve content from {url}")
             return None
@@ -110,6 +109,115 @@             content = self.html_to_markdown(content)
 
         return content
+
+    def scrape_with_playwright(self, url):
+        import playwright  # noqa: F401
+        from playwright.sync_api import Error as PlaywrightError
+        from playwright.sync_api import TimeoutError as PlaywrightTimeoutError
+        from playwright.sync_api import sync_playwright
+
+        with sync_playwright() as p:
+            try:
+                browser = p.chromium.launch()
+            except Exception as e:
+                self.playwright_available = False
+                self.print_error(str(e))
+                return None, None
+
+            try:
+                context = browser.new_context(ignore_https_errors=not self.verify_ssl)
+                page = context.new_page()
+
+                user_agent = page.evaluate("navigator.userAgent")
+                user_agent = user_agent.replace("Headless", "")
+                user_agent = user_agent.replace("headless", "")
+                user_agent += " " + aider_user_agent
+
+                page.set_extra_http_headers({"User-Agent": user_agent})
+
+                response = None
+                try:
+                    response = page.goto(url, wait_until="networkidle", timeout=5000)
+                except PlaywrightTimeoutError:
+                    print(f"Page didn't quiesce, scraping content anyway: {url}")
+                    response = None
+                except PlaywrightError as e:
+                    self.print_error(f"Error navigating to {url}: {str(e)}")
+                    return None, None
+
+                try:
+                    content = page.content()
+                    mime_type = None
+                    if response:
+                        content_type = response.header_value("content-type")
+                        if content_type:
+                            mime_type = content_type.split(";")[0]
+                except PlaywrightError as e:
+                    self.print_error(f"Error retrieving page content: {str(e)}")
+                    content = None
+                    mime_type = None
+            finally:
+                browser.close()
+
+        return content, mime_type
+
+    def scrape_with_httpx(self, url):
+        import httpx
+
+        headers = {"User-Agent": f"Mozilla./5.0 ({aider_user_agent})"}
+        try:
+            with httpx.Client(
+                headers=headers, verify=self.verify_ssl, follow_redirects=True
+            ) as client:
+                response = client.get(url)
+                response.raise_for_status()
+                return response.text, response.headers.get("content-type", "").split(";")[0]
+        except httpx.HTTPError as http_err:
+            self.print_error(f"HTTP error occurred: {http_err}")
+        except Exception as err:
+            self.print_error(f"An error occurred: {err}")
+        return None, None
+
+    def try_pandoc(self):
+        if self.pandoc_available:
+            return
+
+        try:
+            pypandoc.get_pandoc_version()
+            self.pandoc_available = True
+            return
+        except OSError:
+            pass
+
+        try:
+            pypandoc.download_pandoc(delete_installer=True)
+        except Exception as err:
+            self.print_error(f"Unable to install pandoc: {err}")
+            return
+
+        self.pandoc_available = True
+
+    def html_to_markdown(self, page_source):
+        from bs4 import BeautifulSoup
+
+        soup = BeautifulSoup(page_source, "html.parser")
+        soup = slimdown_html(soup)
+        page_source = str(soup)
+
+        if not self.pandoc_available:
+            return page_source
+
+        try:
+            md = pypandoc.convert_text(page_source, "markdown", format="html")
+        except OSError:
+            return page_source
+
+        md = re.sub(r"</div>", "      ", md)
+        md = re.sub(r"<div>", "     ", md)
+
+        md = re.sub(r"\n\s*\n", "\n\n", md)
+
+        return md
 
     def looks_like_html(self, content):
         """
@@ -129,116 +237,6 @@             return any(re.search(pattern, content, re.IGNORECASE) for pattern in html_patterns)
         return False
 
-    # Internals...
-    def scrape_with_playwright(self, url):
-        import playwright  # noqa: F401
-        from playwright.sync_api import Error as PlaywrightError
-        from playwright.sync_api import TimeoutError as PlaywrightTimeoutError
-        from playwright.sync_api import sync_playwright
-
-        with sync_playwright() as p:
-            try:
-                browser = p.chromium.launch()
-            except Exception as e:
-                self.playwright_available = False
-                self.print_error(str(e))
-                return None, None
-
-            try:
-                context = browser.new_context(ignore_https_errors=not self.verify_ssl)
-                page = context.new_page()
-
-                user_agent = page.evaluate("navigator.userAgent")
-                user_agent = user_agent.replace("Headless", "")
-                user_agent = user_agent.replace("headless", "")
-                user_agent += " " + aider_user_agent
-
-                page.set_extra_http_headers({"User-Agent": user_agent})
-
-                response = None
-                try:
-                    response = page.goto(url, wait_until="networkidle", timeout=5000)
-                except PlaywrightTimeoutError:
-                    print(f"Page didn't quiesce, scraping content anyway: {url}")
-                    response = None
-                except PlaywrightError as e:
-                    self.print_error(f"Error navigating to {url}: {str(e)}")
-                    return None, None
-
-                try:
-                    content = page.content()
-                    mime_type = None
-                    if response:
-                        content_type = response.header_value("content-type")
-                        if content_type:
-                            mime_type = content_type.split(";")[0]
-                except PlaywrightError as e:
-                    self.print_error(f"Error retrieving page content: {str(e)}")
-                    content = None
-                    mime_type = None
-            finally:
-                browser.close()
-
-        return content, mime_type
-
-    def scrape_with_httpx(self, url):
-        import httpx
-
-        headers = {"User-Agent": f"Mozilla./5.0 ({aider_user_agent})"}
-        try:
-            with httpx.Client(
-                headers=headers, verify=self.verify_ssl, follow_redirects=True
-            ) as client:
-                response = client.get(url)
-                response.raise_for_status()
-                return response.text, response.headers.get("content-type", "").split(";")[0]
-        except httpx.HTTPError as http_err:
-            self.print_error(f"HTTP error occurred: {http_err}")
-        except Exception as err:
-            self.print_error(f"An error occurred: {err}")
-        return None, None
-
-    def try_pandoc(self):
-        if self.pandoc_available:
-            return
-
-        try:
-            pypandoc.get_pandoc_version()
-            self.pandoc_available = True
-            return
-        except OSError:
-            pass
-
-        try:
-            pypandoc.download_pandoc(delete_installer=True)
-        except Exception as err:
-            self.print_error(f"Unable to install pandoc: {err}")
-            return
-
-        self.pandoc_available = True
-
-    def html_to_markdown(self, page_source):
-        from bs4 import BeautifulSoup
-
-        soup = BeautifulSoup(page_source, "html.parser")
-        soup = slimdown_html(soup)
-        page_source = str(soup)
-
-        if not self.pandoc_available:
-            return page_source
-
-        try:
-            md = pypandoc.convert_text(page_source, "markdown", format="html")
-        except OSError:
-            return page_source
-
-        md = re.sub(r"</div>", "      ", md)
-        md = re.sub(r"<div>", "     ", md)
-
-        md = re.sub(r"\n\s*\n", "\n\n", md)
-
-        return md
-
 
 def slimdown_html(soup):
     for svg in soup.find_all("svg"):
@@ -249,14 +247,11 @@ 
     for tag in soup.find_all(href=lambda x: x and x.startswith("data:")):
         tag.decompose()
-
     for tag in soup.find_all(src=lambda x: x and x.startswith("data:")):
         tag.decompose()
 
     for tag in soup.find_all(True):
-        for attr in list(tag.attrs):
-            if attr != "href":
-                tag.attrs.pop(attr, None)
+        tag.attrs.clear()
 
     return soup
 
