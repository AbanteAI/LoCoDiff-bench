
index c315012c..fd96e754 100644
--- a/aider_aider_scrape.py_expectedoutput.txt (expected):tmp/tmpzjdji05x_expected.txt	
+++ b/aider_aider_scrape.py_extracted.txt (actual):tmp/tmpwrfi_sj7_actual.txt	
@@ -4,15 +4,14 @@ import re
 import sys
 
 import pypandoc
+from playwright.sync_api import Error as PlaywrightError
+from playwright.sync_api import TimeoutError as PlaywrightTimeoutError
 
 from aider import __version__, urls, utils
 from aider.dump import dump  # noqa: F401
 
 aider_user_agent = f"Aider/{__version__} +{urls.website}"
 
-# Playwright is nice because it has a simple way to install dependencies on most
-# platforms.
-
 
 def install_playwright(io):
     try:
@@ -167,11 +166,9 @@ class Scraper:
 
                 try:
                     content = page.content()
-                    mime_type = None
-                    if response:
-                        content_type = response.header_value("content-type")
-                        if content_type:
-                            mime_type = content_type.split(";")[0]
+                    mime_type = (
+                        response.header_value("content-type").split(";")[0] if response else None
+                    )
                 except PlaywrightError as e:
                     self.print_error(f"Error retrieving page content: {str(e)}")
                     content = None
@@ -261,6 +258,24 @@ def slimdown_html(soup):
     return soup
 
 
+# Adapted from AutoGPT, MIT License
+#
+# https://github.com/Significant-Gravitas/AutoGPT/blob/aider_aider_scrape.py_extracted.txt (actual): str) -> str:
+    soup = BeautifulSoup(page_source, "html.parser")
+
+    for script in soup(["script", "style"]):
+        script.extract()
+
+    text = soup.get_text()
+    lines = (line.strip() for line in text.splitlines())
+    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
+    text = "\n".join(chunk for chunk in chunks if chunk)
+    return text
+
+
 def main(url):
     scraper = Scraper()
     content = scraper.scrape(url)
