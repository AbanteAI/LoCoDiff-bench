--- aider_aider_scrape.py_expectedoutput.txt (expected)+++ aider_aider_scrape.py_extracted.txt (actual)@@ -4,14 +4,13 @@ import sys
 
 import pypandoc
+from playwright.sync_api import Error as PlaywrightError
+from playwright.sync_api import TimeoutError as PlaywrightTimeoutError
 
 from aider import __version__, urls, utils
 from aider.dump import dump  # noqa: F401
 
 aider_user_agent = f"Aider/{__version__} +{urls.website}"
-
-# Playwright is nice because it has a simple way to install dependencies on most
-# platforms.
 
 
 def install_playwright(io):
@@ -167,11 +166,9 @@ 
                 try:
                     content = page.content()
-                    mime_type = None
-                    if response:
-                        content_type = response.header_value("content-type")
-                        if content_type:
-                            mime_type = content_type.split(";")[0]
+                    mime_type = (
+                        response.header_value("content-type").split(";")[0] if response else None
+                    )
                 except PlaywrightError as e:
                     self.print_error(f"Error retrieving page content: {str(e)}")
                     content = None
@@ -261,6 +258,24 @@     return soup
 
 
+# Adapted from AutoGPT, MIT License
+#
+# https://github.com/Significant-Gravitas/AutoGPT/blob/fe0923ba6c9abb42ac4df79da580e8a4391e0418/autogpts/autogpt/autogpt/commands/web_selenium.py#L173
+
+
+def html_to_text(page_source: str) -> str:
+    soup = BeautifulSoup(page_source, "html.parser")
+
+    for script in soup(["script", "style"]):
+        script.extract()
+
+    text = soup.get_text()
+    lines = (line.strip() for line in text.splitlines())
+    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
+    text = "\n".join(chunk for chunk in chunks if chunk)
+    return text
+
+
 def main(url):
     scraper = Scraper()
     content = scraper.scrape(url)
