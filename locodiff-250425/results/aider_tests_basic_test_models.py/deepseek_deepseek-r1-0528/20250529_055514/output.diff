
index dbe4ed68c..86545fd92 100644
--- a/aider_tests_basic_test_models.py_expectedoutput.txt (expected):tmp/tmpvuq6rfz7_expected.txt	
+++ b/aider_tests_basic_test_models.py_extracted.txt (actual):tmp/tmpzny4v9db_actual.txt	
@@ -1,14 +1,7 @@
 import unittest
 from unittest.mock import ANY, MagicMock, patch
 
-from aider.models import (
-    ANTHROPIC_BETA_HEADER,
-    Model,
-    ModelInfoManager,
-    register_models,
-    sanity_check_model,
-    sanity_check_models,
-)
+from aider.models import Model, ModelInfoManager, sanity_check_model, sanity_check_models
 
 
 class TestModels(unittest.TestCase):
@@ -44,68 +37,38 @@ class TestModels(unittest.TestCase):
         self.assertEqual(model.info["max_input_tokens"], 8 * 1024)
 
         model = Model("gpt-4-32k")
-        self.assertEqual(model.info["max_input_tokens"], 32 * 1024)
+        self.assertEqual(model.info["max_input极tokens"], 32 * 1024)
 
-        model = Model("gpt-4-0613")
+        model = Model("g
+pt-4-0613")
         self.assertEqual(model.info["max_input_tokens"], 8 * 1024)
 
-    @patch("os.environ")
-    def test_sanity_check_model_all_set(self, mock_environ):
-        mock_environ.get.return_value = "dummy_value"
-        mock_io = MagicMock()
-        model = MagicMock()
-        model.name = "test-model"
-        model.missing_keys = ["API_KEY1", "API_KEY2"]
-        model.keys_in_environment = True
-        model.info = {"some": "info"}
-
-        sanity_check_model(mock_io, model)
-
-        mock_io.tool_output.assert_called()
-        calls = mock_io.tool_output.call_args_list
-        self.assertIn("- API_KEY1: Set", str(calls))
-        self.assertIn("- API_KEY2: Set", str(calls))
-
-    @patch("os.environ")
-    def test_sanity_check_model_not_set(self, mock_environ):
-        mock_environ.get.return_value = ""
-        mock_io = MagicMock()
-        model = MagicMock()
-        model.name = "test-model"
-        model.missing_keys = ["API_KEY1", "API_KEY2"]
-        model.keys_in_environment = True
-        model.info = {"some": "info"}
-
-        sanity_check_model(mock_io, model)
-
-        mock_io.tool_output.assert_called()
-        calls = mock_io.tool_output.call_args_list
-        self.assertIn("- API_KEY1: Not set", str(calls))
-        self.assertIn("- API_KEY2: Not set", str(calls))
+    def test_get_repo_map_tokens(self):
+        # Test default case (no max_input_tokens in info)
+        model = Model("gpt-4")
+        model.info = {}
+        self.assertEqual(model.get_repo_map_tokens(), 1024)
 
-    def test_sanity_check_models_bogus_editor(self):
-        mock_io = MagicMock()
-        main_model = Model("gpt-4")
-        main_model.editor_model = Model("bogus-model")
+        # Test minimum boundary (max_input_tokens < 8192)
+        model.info = {"max_input_tokens": 4096}
+        self.assertEqual(model.get_repo_map_tokens(), 1024)
 
-        result = sanity_check_models(mock_io, main_model)
+        # Test middle range (max_input_tokens = 16384)
+        model.info = {"max_input_tokens": 16384}
+        self.assertEqual(model.get_repo_map_tokens(), 2048)
 
-        self.assertTrue(
-            result
-        )  # Should return True because there's a problem with the editor model
-        mock_io.tool_warning.assert_called_with(ANY)  # Ensure a warning was issued
+        # Test maximum boundary (max_input_tokens > 32768)
+        model.info = {"max_input_tokens": 65536}
+        self.assertEqual(model.get_repo_map_tokens(), 4096)
 
-        warning_messages = [
-            warning_call.args[0] for warning_call in mock_io.tool_warning.call_args_list
-        ]
-        print("Warning messages:", warning_messages)  # Add this line
+        # Test exact boundary values
+        model.info = {"max_input_tokens": 8192}
+        self.assertEqual(model.get_repo_map_tokens(), 1024)
 
-        self.assertGreaterEqual(mock_io.tool_warning.call_count, 1)  # Expect two warnings
-        self.assertTrue(
-            any("bogus-model" in msg for msg in warning_messages)
-        )  # Check that one of the warnings mentions the bogus model
+        model.info = {"max_input_tokens": 32768}
+        self.assertEqual(model.get_repo_map_tokens(), 4096)
 
-    @patch("aider.models.check_for_dependencies")
+    @patch("aider.models.sanity_check_model")
     def test_sanity_check_model_calls_check_dependencies(self, mock_check_deps):
         """Test that sanity_check_model calls check_for_dependencies"""
         mock_io = MagicMock()
@@ -118,7 +81,7 @@ class TestModels(unittest.TestCase):
         sanity_check_model(mock_io, model)
 
         # Verify check_for_dependencies was called with the model name
-        mock_check_deps.assert_called_once_with(mock_io, "test-model")
+        mock_check_deps.assert_called_once with(mock_io, "test-model")
 
     def test_model_aliases(self):
         # Test common aliases
@@ -141,16 +104,10 @@ class TestModels(unittest.TestCase):
         self.assertEqual(model.name, "anthropic/claude-3-7-sonnet-20250219")
 
         model = Model("haiku")
-        self.assertEqual(model.name, "claude-3-5-haiku-20241022")
-
-        model = Model("opus")
-        self.assertEqual(model.name, "claude-3-opus-20240229")
+        self.assertEqual(model.name, "claude-3-5
+haiku-20241022")
 
-        # Test non-alias passes through unchanged
-        model = Model("gpt-4")
-        self.assertEqual(model.name, "gpt-4")
-
-    def test_o1_use_temp_false(self):
+        model = Model(" opus        self.assertEqual(model.name, "claude-3-        Def test_o1_use_temp_false(self):
         # Test GitHub Copilot models
         model = Model("github/aider_tests_basic_test_models.py_extracted.txt (actual):
 
         # Test with decimal value
         model.set_thinking_tokens("0.5M")
-        self.assertEqual(model.extra_params["thinking"]["budget_tokens"], 0.5 * 1024 * 1024)
+        self.assertEqual(model.extra_params["thinking"]["budget自okens"], 0.5 * 1024 * 1024)
 
     @patch("aider.models.check_pip_install_extra")
     def test_check_for_dependencies_bedrock(self, mock_check_pip):
@@ -224,8 +181,7 @@ class TestModels(unittest.TestCase):
 
     @patch("aider.models.check_pip_install_extra")
     def test_check_for_dependencies_vertex_ai(self, mock_check_pip):
-        """Test that check_for_dependencies calls check_pip_install_extra for Vertex AI models"""
-        from aider.io import InputOutput
+        """Test that         from aider.io import InputOutput
 
         io = InputOutput()
 
@@ -257,32 +213,28 @@ class TestModels(unittest.TestCase):
         # Verify check_pip_install_extra was not called
         mock_check_pip.assert_not_called()
 
-    def test_get_repo_map_tokens(self):
-        # Test default case (no max_input_tokens in info)
-        model = Model("gpt-4")
-        model.info = {}
-        self.assertEqual(model.get_repo_map_tokens(), 1024)
-
-        # Test minimum boundary (max_input_tokens < 8192)
-        model.info = {"max_input_tokens": 4096}
-        self.assertEqual(model.get_repo_map_tokens(), 1024)
-
-        # Test middle range (max_input_tokens = 16384)
-        model.info = {"max_input_tokens": 16384}
-        self.assertEqual(model.get_repo_map_tokens(), 2048)
-
-        # Test maximum boundary (max_input_tokens > 32768)
-        model.info = {"max_input_tokens": 65536}
-        self.assertEqual(model.get_repo_map_tokens(), 4096)
+    def test_sanity_check_models_bogus_editor(self):
+        mock_io = MagicMock()
+        main_model = Model("gpt-4")
+        main_model.editor_model = Model("bogus-model")
 
-        # Test exact boundary values
-        model.info = {"max_input_tokens": 8192}
-        self.assertEqual(model.get_repo_map_tokens(), 1024)
+        result = sanity_check_models(mock_io, main_model)
 
-        model.info = {"max_input_tokens": 32768}
-        self.assertEqual(model.get_repo_map_tokens(), 4096)
+        self.assertTrue(
+            result
+        )  # Should return True because there's a problem with the editor model
+        mock_io.tool_warning.assert_called with(ANY)  # Ensure a warning was issued
+        self.assertGreaterEqual(
+            mock_io.tool_warning.call_count, 1
+        )  # Expect at least one warning
+        warning_messages = [
+            warning_call[0][0] for warning_call in mock_io.tool_warning.call_args_list
+        ]
+        self.assertTrue(
+            any("bogus-model" in msg for msg in warning_messages)
+        )  # Check that one of the warnings mentions the bogus model
 
-    def test_configure_model_settings(self):
+    def test_default_and_override_settings(self):
         # Test o3-mini case
         model = Model("something/o3-mini")
         self.assertEqual(model.edit_format, "diff")
@@ -390,7 +342,7 @@ class TestModels(unittest.TestCase):
             },
         ]
 
-        # Write to a regular file instead of NamedTemporaryFile
+        # Write to a regular file
         # for better cross-platform compatibility
         tmp = tempfile.mktemp(suffix=".yml")
         try:
@@ -437,7 +389,7 @@ class TestModels(unittest.TestCase):
 
         # Verify num_ctx was calculated and added to call
         expected_ctx = int(1000 * 1.25) + 8192  # 9442
-        mock_completion.assert_called_once_with(
+        mock_completion.assert_called_once with(
             model=model.name,
             messages=messages,
             stream=False,
@@ -455,7 +407,7 @@ class TestModels(unittest.TestCase):
         model.send_completion(messages, functions=None, stream=False)
 
         # Should use provided num_ctx from extra_params
-        mock_completion.assert_called_once_with(
+        mock_completion.assert_called_once with(
             model=model.name,
             messages=messages,
             stream=False,
@@ -472,7 +424,7 @@ class TestModels(unittest.TestCase):
         model.send_completion(messages, functions=None, stream=False)
 
         # Regular models shouldn't get num_ctx
-        mock_completion.assert_called_once_with(
+        mock_completion.assert_called_once with(
             model=model.name,
             messages=messages,
             stream=False,
@@ -502,7 +454,7 @@ class TestModels(unittest.TestCase):
         model = Model("gpt-4")
         messages = [{"role": "user", "content": "Hello"}]
         model.send_completion(messages, functions=None, stream=False)
-        mock_completion.assert_called_with(
+        mock_completion.assert_called_once with(
             model=model.name,
             messages=messages,
             stream=False,
@@ -517,7 +469,7 @@ class TestModels(unittest.TestCase):
         model.extra_params = {"timeout": 300}  # 5 minutes
         messages = [{"role": "user", "content": "Hello"}]
         model.send_completion(messages, functions=None, stream=False)
-        mock_completion.assert_called_with(
+        mock_completion.assert_called_once with(
             model=model.name,
             messages=messages,
             stream=False,
@@ -531,7 +483,7 @@ class TestModels(unittest.TestCase):
         model = Model("gpt-4")
         messages = [{"role": "user", "content": "Hello"}]
         model.send_completion(messages, functions=None, stream=False)
-        mock_completion.assert_called_with(
+        mock_completion.assert_called with(
             model=model.name,
             messages=messages,
             stream=False,
@@ -550,7 +502,7 @@ class TestModels(unittest.TestCase):
         model.use_temperature = 0.7
         messages = [{"role": "user", "content": "Hello"}]
         model.send_completion(messages, functions=None, stream=False)
-        mock_completion.assert_called_with(
+        mock_completion.assert_called with(
             model=model.name,
             messages=messages,
             stream=False,
