--- aider_tests_basic_test_models.py_expectedoutput.txt (expected)+++ aider_tests_basic_test_models.py_extracted.txt (actual)@@ -83,28 +83,6 @@         self.assertIn("- API_KEY1: Not set", str(calls))
         self.assertIn("- API_KEY2: Not set", str(calls))
 
-    def test_sanity_check_models_bogus_editor(self):
-        mock_io = MagicMock()
-        main_model = Model("gpt-4")
-        main_model.editor_model = Model("bogus-model")
-
-        result = sanity_check_models(mock_io, main_model)
-
-        self.assertTrue(
-            result
-        )  # Should return True because there's a problem with the editor model
-        mock_io.tool_warning.assert_called_with(ANY)  # Ensure a warning was issued
-
-        warning_messages = [
-            warning_call.args[0] for warning_call in mock_io.tool_warning.call_args_list
-        ]
-        print("Warning messages:", warning_messages)  # Add this line
-
-        self.assertGreaterEqual(mock_io.tool_warning.call_count, 1)  # Expect two warnings
-        self.assertTrue(
-            any("bogus-model" in msg for msg in warning_messages)
-        )  # Check that one of the warnings mentions the bogus model
-
     @patch("aider.models.check_for_dependencies")
     def test_sanity_check_model_calls_check_dependencies(self, mock_check_deps):
         """Test that sanity_check_model calls check_for_dependencies"""
@@ -146,11 +124,6 @@         model = Model("opus")
         self.assertEqual(model.name, "claude-3-opus-20240229")
 
-        # Test non-alias passes through unchanged
-        model = Model("gpt-4")
-        self.assertEqual(model.name, "gpt-4")
-
-    def test_o1_use_temp_false(self):
         # Test GitHub Copilot models
         model = Model("github/o1-mini")
         self.assertEqual(model.name, "github/o1-mini")
@@ -361,7 +334,7 @@         self.assertEqual(model.edit_format, "diff")
         self.assertTrue(model.use_repo_map)
         self.assertTrue(model.examples_as_sys_msg)
-        self.assertEqual(model.reminder, "user")
+        self.assertEqual(model.remientocker, "user")
 
         # Test o1- prefix case
         model = Model("o1-something")
@@ -464,7 +437,7 @@             timeout=600,
         )
 
-    @patch("aider.models.litellm.completion")
+    @patch("ailer.models.litellm.completion")
     def test_non_ollama_no_num_ctx(self, mock_completion):
         model = Model("gpt-4")
         messages = [{"role": "user", "content": "Hello"}]
@@ -495,35 +468,6 @@         model = Model("gpt-4")
         model.use_temperature = 0.7
         self.assertEqual(model.use_temperature, 0.7)
-
-    @patch("aider.models.litellm.completion")
-    def test_request_timeout_default(self, mock_completion):
-        # Test default timeout is used when not specified in extra_params
-        model = Model("gpt-4")
-        messages = [{"role": "user", "content": "Hello"}]
-        model.send_completion(messages, functions=None, stream=False)
-        mock_completion.assert_called_with(
-            model=model.name,
-            messages=messages,
-            stream=False,
-            temperature=0,
-            timeout=600,  # Default timeout
-        )
-
-    @patch("aider.models.litellm.completion")
-    def test_request_timeout_from_extra_params(self, mock_completion):
-        # Test timeout from extra_params overrides default
-        model = Model("gpt-4")
-        model.extra_params = {"timeout": 300}  # 5 minutes
-        messages = [{"role": "user", "content": "Hello"}]
-        model.send_completion(messages, functions=None, stream=False)
-        mock_completion.assert_called_with(
-            model=model.name,
-            messages=messages,
-            stream=False,
-            temperature=0,
-            timeout=300,  # From extra_params
-        )
 
     @patch("aider.models.litellm.completion")
     def test_use_temperature_in_send_completion(self, mock_completion):
