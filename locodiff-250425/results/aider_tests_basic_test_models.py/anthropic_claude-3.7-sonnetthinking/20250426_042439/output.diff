
index dbe4ed68..f3a0e5ec 100644
--- a/aider_tests_basic_test_models.py_expectedoutput.txt (expected):tmp/tmpjgolqftr_expected.txt	
+++ b/aider_tests_basic_test_models.py_extracted.txt (actual):tmp/tmpbltfls4c_actual.txt	
@@ -83,28 +83,6 @@ class TestModels(unittest.TestCase):
         self.assertIn("- API_KEY1: Not set", str(calls))
         self.assertIn("- API_KEY2: Not set", str(calls))
 
-    def test_sanity_check_models_bogus_editor(self):
-        mock_io = MagicMock()
-        main_model = Model("gpt-4")
-        main_model.editor_model = Model("bogus-model")
-
-        result = sanity_check_models(mock_io, main_model)
-
-        self.assertTrue(
-            result
-        )  # Should return True because there's a problem with the editor model
-        mock_io.tool_warning.assert_called_with(ANY)  # Ensure a warning was issued
-
-        warning_messages = [
-            warning_call.args[0] for warning_call in mock_io.tool_warning.call_args_list
-        ]
-        print("Warning messages:", warning_messages)  # Add this line
-
-        self.assertGreaterEqual(mock_io.tool_warning.call_count, 1)  # Expect two warnings
-        self.assertTrue(
-            any("bogus-model" in msg for msg in warning_messages)
-        )  # Check that one of the warnings mentions the bogus model
-
     @patch("aider.models.check_for_dependencies")
     def test_sanity_check_model_calls_check_dependencies(self, mock_check_deps):
         """Test that sanity_check_model calls check_for_dependencies"""
@@ -425,6 +403,28 @@ class TestModels(unittest.TestCase):
             except OSError:
                 pass
 
+    def test_sanity_check_models_bogus_editor(self):
+        mock_io = MagicMock()
+        main_model = Model("gpt-4")
+        main_model.editor_model = Model("bogus-model")
+
+        result = sanity_check_models(mock_io, main_model)
+
+        self.assertTrue(
+            result
+        )  # Should return True because there's a problem with the editor model
+        mock_io.tool_warning.assert_called_with(ANY)  # Ensure a warning was issued
+
+        warning_messages = [
+            warning_call.args[0] for warning_call in mock_io.tool_warning.call_args_list
+        ]
+        print("Warning messages:", warning_messages)  # Add this line
+
+        self.assertGreaterEqual(mock_io.tool_warning.call_count, 1)  # Expect two warnings
+        self.assertTrue(
+            any("bogus-model" in msg for msg in warning_messages)
+        )  # Check that one of the warnings mentions the bogus model
+
     @patch("aider.models.litellm.completion")
     @patch.object(Model, "token_count")
     def test_ollama_num_ctx_set_when_missing(self, mock_token_count, mock_completion):
