
index dbe4ed68c..9776be4a2 100644
--- a/aider_tests_basic_test_models.py_expectedoutput.txt (expected):tmp/tmpco5rno75_expected.txt	
+++ b/aider_tests_basic_test_models.py_extracted.txt (actual):tmp/tmpwu0_u6o7_actual.txt	
@@ -49,62 +49,6 @@ class TestModels(unittest.TestCase):
         model = Model("gpt-4-0613")
         self.assertEqual(model.info["max_input_tokens"], 8 * 1024)
 
-    @patch("os.environ")
-    def test_sanity_check_model_all_set(self, mock_environ):
-        mock_environ.get.return_value = "dummy_value"
-        mock_io = MagicMock()
-        model = MagicMock()
-        model.name = "test-model"
-        model.missing_keys = ["API_KEY1", "API_KEY2"]
-        model.keys_in_environment = True
-        model.info = {"some": "info"}
-
-        sanity_check_model(mock_io, model)
-
-        mock_io.tool_output.assert_called()
-        calls = mock_io.tool_output.call_args_list
-        self.assertIn("- API_KEY1: Set", str(calls))
-        self.assertIn("- API_KEY2: Set", str(calls))
-
-    @patch("os.environ")
-    def test_sanity_check_model_not_set(self, mock_environ):
-        mock_environ.get.return_value = ""
-        mock_io = MagicMock()
-        model = MagicMock()
-        model.name = "test-model"
-        model.missing_keys = ["API_KEY1", "API_KEY2"]
-        model.keys_in_environment = True
-        model.info = {"some": "info"}
-
-        sanity_check_model(mock_io, model)
-
-        mock_io.tool_output.assert_called()
-        calls = mock_io.tool_output.call_args_list
-        self.assertIn("- API_KEY1: Not set", str(calls))
-        self.assertIn("- API_KEY2: Not set", str(calls))
-
-    def test_sanity_check_models_bogus_editor(self):
-        mock_io = MagicMock()
-        main_model = Model("gpt-4")
-        main_model.editor_model = Model("bogus-model")
-
-        result = sanity_check_models(mock_io, main_model)
-
-        self.assertTrue(
-            result
-        )  # Should return True because there's a problem with the editor model
-        mock_io.tool_warning.assert_called_with(ANY)  # Ensure a warning was issued
-
-        warning_messages = [
-            warning_call.args[0] for warning_call in mock_io.tool_warning.call_args_list
-        ]
-        print("Warning messages:", warning_messages)  # Add this line
-
-        self.assertGreaterEqual(mock_io.tool_warning.call_count, 1)  # Expect two warnings
-        self.assertTrue(
-            any("bogus-model" in msg for msg in warning_messages)
-        )  # Check that one of the warnings mentions the bogus model
-
     @patch("aider.models.check_for_dependencies")
     def test_sanity_check_model_calls_check_dependencies(self, mock_check_deps):
         """Test that sanity_check_model calls check_for_dependencies"""
@@ -160,103 +104,6 @@ class TestModels(unittest.TestCase):
         self.assertEqual(model.name, "github/aider_tests_basic_test_models.py_extracted.txt (actual):
-        # Create a model instance to test the parse_token_value method
-        model = Model("gpt-4")
-
-        # Test integer inputs
-        self.assertEqual(model.parse_token_value(8096), 8096)
-        self.assertEqual(model.parse_token_value(1000), 1000)
-
-        # Test string inputs
-        self.assertEqual(model.parse_token_value("8096"), 8096)
-
-        # Test k/K suffix (kilobytes)
-        self.assertEqual(model.parse_token_value("8k"), 8 * 1024)
-        self.assertEqual(model.parse_token_value("8K"), 8 * 1024)
-        self.assertEqual(model.parse_token_value("10.5k"), 10.5 * 1024)
-        self.assertEqual(model.parse_token_value("0.5K"), 0.5 * 1024)
-
-        # Test m/M suffix (megabytes)
-        self.assertEqual(model.parse_token_value("1m"), 1 * 1024 * 1024)
-        self.assertEqual(model.parse_token_value("1M"), 1 * 1024 * 1024)
-        self.assertEqual(model.parse_token_value("0.5M"), 0.5 * 1024 * 1024)
-
-        # Test with spaces
-        self.assertEqual(model.parse_token_value(" 8k "), 8 * 1024)
-
-        # Test conversion from other types
-        self.assertEqual(model.parse_token_value(8.0), 8)
-
-    def test_set_thinking_tokens(self):
-        # Test that set_thinking_tokens correctly sets the tokens with different formats
-        model = Model("gpt-4")
-
-        # Test with integer
-        model.set_thinking_tokens(8096)
-        self.assertEqual(model.extra_params["thinking"]["budget_tokens"], 8096)
-        self.assertFalse(model.use_temperature)
-
-        # Test with string
-        model.set_thinking_tokens("10k")
-        self.assertEqual(model.extra_params["thinking"]["budget_tokens"], 10 * 1024)
-
-        # Test with decimal value
-        model.set_thinking_tokens("0.5M")
-        self.assertEqual(model.extra_params["thinking"]["budget_tokens"], 0.5 * 1024 * 1024)
-
-    @patch("aider.models.check_pip_install_extra")
-    def test_check_for_dependencies_bedrock(self, mock_check_pip):
-        """Test that check_for_dependencies calls check_pip_install_extra for Bedrock models"""
-        from aider.io import InputOutput
-
-        io = InputOutput()
-
-        # Test with a Bedrock model
-        from aider.models import check_for_dependencies
-
-        check_for_dependencies(io, "bedrock/anthropic.claude-3-sonnet-20240229-v1:0")
-
-        # Verify check_pip_install_extra was called with correct arguments
-        mock_check_pip.assert_called_once_with(
-            io, "boto3", "AWS Bedrock models require the boto3 package.", ["boto3"]
-        )
-
-    @patch("aider.models.check_pip_install_extra")
-    def test_check_for_dependencies_vertex_ai(self, mock_check_pip):
-        """Test that check_for_dependencies calls check_pip_install_extra for Vertex AI models"""
-        from aider.io import InputOutput
-
-        io = InputOutput()
-
-        # Test with a Vertex AI model
-        from aider.models import check_for_dependencies
-
-        check_for_dependencies(io, "vertex_ai/gemini-1.5-pro")
-
-        # Verify check_pip_install_extra was called with correct arguments
-        mock_check_pip.assert_called_once_with(
-            io,
-            "google.cloud.aiplatform",
-            "Google Vertex AI models require the google-cloud-aiplatform package.",
-            ["google-cloud-aiplatform"],
-        )
-
-    @patch("aider.models.check_pip_install_extra")
-    def test_check_for_dependencies_other_model(self, mock_check_pip):
-        """Test that check_for_dependencies doesn't call check_pip_install_extra for other models"""
-        from aider.io import InputOutput
-
-        io = InputOutput()
-
-        # Test with a non-Bedrock, non-Vertex AI model
-        from aider.models import check_for_dependencies
-
-        check_for_dependencies(io, "gpt-4")
-
-        # Verify check_pip_install_extra was not called
-        mock_check_pip.assert_not_called()
-
     def test_get_repo_map_tokens(self):
         # Test default case (no max_input_tokens in info)
         model = Model("gpt-4")
@@ -282,6 +129,62 @@ class TestModels(unittest.TestCase):
         model.info = {"max_input_tokens": 32768}
         self.assertEqual(model.get_repo_map_tokens(), 4096)
 
+    @patch("aider.models.litellm.completion")
+    @patch.object(Model, "token_count")
+    def test_ollama_num_ctx_set_when_missing(self, mock_token_count, mock_completion):
+        mock_token_count.return_value = 1000
+
+        model = Model("ollama/aider_tests_basic_test_models.py_expectedoutput.txt (expected): "user", "content": "Hello"}]
+
+        model.send_completion(messages, functions=None, stream=False)
+
+        # Verify num_ctx was calculated and added to call
+        expected_ctx = int(1000 * 1.25) + 8192  # 9442
+        mock_completion.assert_called_once_with(
+            model=model.name,
+            messages=messages,
+            stream=False,
+            temperature=0,
+            num_ctx=expected_ctx,
+            timeout=600,
+        )
+
+    @patch("aider.models.litellm.completion")
+    def test_ollama_uses_existing_num_ctx(self, mock_completion):
+        model = Model("ollama/aider_tests_basic_test_models.py_expectedoutput.txt (expected): 4096}
+
+        messages = [{"role": "user", "content": "Hello"}]
+        model.send_completion(messages, functions=None, stream=False)
+
+        # Should use provided num_ctx from extra_params
+        mock_completion.assert_called_once_with(
+            model=model.name,
+            messages=messages,
+            stream=False,
+            temperature=0,
+            num_ctx=4096,
+            timeout=600,
+        )
+
+    @patch("aider.models.litellm.completion")
+    def test_non_ollama_no_num_ctx(self, mock_completion):
+        model = Model("gpt-4")
+        messages = [{"role": "user", "content": "Hello"}]
+
+        model.send_completion(messages, functions=None, stream=False)
+
+        # Regular models shouldn't get num_ctx
+        mock_completion.assert_called_once_with(
+            model=model.name,
+            messages=messages,
+            stream=False,
+            temperature=0,
+            timeout=600,
+        )
+        self.assertNotIn("num_ctx", mock_completion.call_args.kwargs)
+
     def test_configure_model_settings(self):
         # Test o3-mini case
         model = Model("something/o3-mini")
@@ -402,8 +305,6 @@ class TestModels(unittest.TestCase):
 
             # Test that defaults are applied when no exact match
             model = Model("claude-3-5-sonnet-20240620")
-            # Test that both the override and existing headers are present
-            model = Model("claude-3-5-sonnet-20240620")
             self.assertEqual(model.extra_params["extra_headers"]["Foo"], "bar")
             self.assertEqual(
                 model.extra_params["extra_headers"]["anthropic-beta"],
@@ -425,61 +326,58 @@ class TestModels(unittest.TestCase):
             except OSError:
                 pass
 
-    @patch("aider.models.litellm.completion")
-    @patch.object(Model, "token_count")
-    def test_ollama_num_ctx_set_when_missing(self, mock_token_count, mock_completion):
-        mock_token_count.return_value = 1000
-
-        model = Model("ollama/aider_tests_basic_test_models.py_expectedoutput.txt (expected): "user", "content": "Hello"}]
+    @patch("os.environ")
+    def test_sanity_check_model_all_set(self, mock_environ):
+        mock_environ.get.return_value = "dummy_value"
+        mock_io = MagicMock()
+        model = MagicMock()
+        model.name = "test-model"
+        model.missing_keys = ["API_KEY1", "API_KEY2"]
+        model.keys_in_environment = True
+        model.info = {"some": "info"}
 
-        model.send_completion(messages, functions=None, stream=False)
+        sanity_check_model(mock_io, model)
 
-        # Verify num_ctx was calculated and added to call
-        expected_ctx = int(1000 * 1.25) + 8192  # 9442
-        mock_completion.assert_called_once_with(
-            model=model.name,
-            messages=messages,
-            stream=False,
-            temperature=0,
-            num_ctx=expected_ctx,
-            timeout=600,
-        )
+        mock_io.tool_output.assert_called()
+        calls = mock_io.tool_output.call_args_list
+        self.assertIn("- API_KEY1: Set", str(calls))
+        self.assertIn("- API_KEY2: Set", str(calls))
 
-    @patch("aider.models.litellm.completion")
-    def test_ollama_uses_existing_num_ctx(self, mock_completion):
-        model = Model("ollama/aider_tests_basic_test_models.py_expectedoutput.txt (expected): 4096}
+    @patch("os.environ")
+    def test_sanity_check_model_not_set(self, mock_environ):
+        mock_environ.get.return_value = ""
+        mock_io = MagicMock()
+        model = MagicMock()
+        model.name = "test-model"
+        model.missing_keys = ["API_KEY1", "API_KEY2"]
+        model.keys_in_environment = True
+        model.info = {"some": "info"}
 
-        messages = [{"role": "user", "content": "Hello"}]
-        model.send_completion(messages, functions=None, stream=False)
+        sanity_check_model(mock_io, model)
 
-        # Should use provided num_ctx from extra_params
-        mock_completion.assert_called_once_with(
-            model=model.name,
-            messages=messages,
-            stream=False,
-            temperature=0,
-            num_ctx=4096,
-            timeout=600,
-        )
+        mock_io.tool_output.assert_called()
+        calls = mock_io.tool_output.call_args_list
+        self.assertIn("- API_KEY1: Not set", str(calls))
+        self.assertIn("- API_KEY2: Not set", str(calls))
 
-    @patch("aider.models.litellm.completion")
-    def test_non_ollama_no_num_ctx(self, mock_completion):
-        model = Model("gpt-4")
-        messages = [{"role": "user", "content": "Hello"}]
+    def test_sanity_check_models_bogus_editor(self):
+        mock_io = MagicMock()
+        main_model = Model("gpt-4")
+        main_model.editor_model = Model("bogus-model")
 
-        model.send_completion(messages, functions=None, stream=False)
+        result = sanity_check_models(mock_io, main_model)
 
-        # Regular models shouldn't get num_ctx
-        mock_completion.assert_called_once_with(
-            model=model.name,
-            messages=messages,
-            stream=False,
-            temperature=0,
-            timeout=600,
-        )
-        self.assertNotIn("num_ctx", mock_completion.call_args.kwargs)
+        self.assertTrue(
+            result
+        )  # Should return True because there's a problem with the editor model
+        mock_io.tool_warning.assert_called_with(ANY)  # Ensure a warning was issued
+        # ai print the args that tool_warning was called with!
+        self.assertGreaterEqual(mock_io.tool_warning.call_count, 2)  # Expect two warnings
+        warning_messages = [call.args[0] for call in mock_io.tool_warning.call_args_list]
+        print("Warning messages:", warning_messages)  # Add this line
+        self.assertTrue(
+            any("bogus-model" in msg for msg in warning_messages)
+        )  # Check that one of the warnings mentions the bogus model
 
     def test_use_temperature_settings(self):
         # Test use_temperature=True (default) uses temperature=0
@@ -558,6 +456,103 @@ class TestModels(unittest.TestCase):
             timeout=600,
         )
 
+    def test_parse_token_value(self):
+        # Create a model instance to test the parse_token_value method
+        model = Model("gpt-4")
+
+        # Test integer inputs
+        self.assertEqual(model.parse_token_value(8096), 8096)
+        self.assertEqual(model.parse_token_value(1000), 1000)
+
+        # Test string inputs
+        self.assertEqual(model.parse_token_value("8096"), 8096)
+
+        # Test k/K suffix (kilobytes)
+        self.assertEqual(model.parse_token_value("8k"), 8 * 1024)
+        self.assertEqual(model.parse_token_value("8K"), 8 * 1024)
+        self.assertEqual(model.parse_token_value("10.5k"), 10.5 * 1024)
+        self.assertEqual(model.parse_token_value("0.5K"), 0.5 * 1024)
+
+        # Test m/M suffix (megabytes)
+        self.assertEqual(model.parse_token_value("1m"), 1 * 1024 * 1024)
+        self.assertEqual(model.parse_token_value("1M"), 1 * 1024 * 1024)
+        self.assertEqual(model.parse_token_value("0.5M"), 0.5 * 1024 * 1024)
+
+        # Test with spaces
+        self.assertEqual(model.parse_token_value(" 8k "), 8 * 1024)
+
+        # Test conversion from other types
+        self.assertEqual(model.parse_token_value(8.0), 8)
+
+    def test_set_thinking_tokens(self):
+        # Test that set_thinking_tokens correctly sets the tokens with different formats
+        model = Model("gpt-4")
+
+        # Test with integer
+        model.set_thinking_tokens(8096)
+        self.assertEqual(model.extra_params["thinking"]["budget_tokens"], 8096)
+        self.assertFalse(model.use_temperature)
+
+        # Test with string
+        model.set_thinking_tokens("10k")
+        self.assertEqual(model.extra_params["thinking"]["budget_tokens"], 10 * 1024)
+
+        # Test with decimal value
+        model.set_thinking_tokens("0.5M")
+        self.assertEqual(model.extra_params["thinking"]["budget_tokens"], 0.5 * 1024 * 1024)
+
+    @patch("aider.models.check_pip_install_extra")
+    def test_check_for_dependencies_bedrock(self, mock_check_pip):
+        """Test that check_for_dependencies calls check_pip_install_extra for Bedrock models"""
+        from aider.io import InputOutput
+
+        io = InputOutput()
+
+        # Test with a Bedrock model
+        from aider.models import check_for_dependencies
+
+        check_for_dependencies(io, "bedrock/anthropic.claude-3-sonnet-20240229-v1:0")
+
+        # Verify check_pip_install_extra was called with correct arguments
+        mock_check_pip.assert_called_once_with(
+            io, "boto3", "AWS Bedrock models require the boto3 package.", ["boto3"]
+        )
+
+    @patch("aider.models.check_pip_install_extra")
+    def test_check_for_dependencies_vertex_ai(self, mock_check_pip):
+        """Test that check_for_dependencies calls check_pip_install_extra for Vertex AI models"""
+        from aider.io import InputOutput
+
+        io = InputOutput()
+
+        # Test with a Vertex AI model
+        from aider.models import check_for_dependencies
+
+        check_for_dependencies(io, "vertex_ai/gemini-1.5-pro")
+
+        # Verify check_pip_install_extra was called with correct arguments
+        mock_check_pip.assert_called_once_with(
+            io,
+            "google.cloud.aiplatform",
+            "Google Vertex AI models require the google-cloud-aiplatform package.",
+            ["google-cloud-aiplatform"],
+        )
+
+    @patch("aider.models.check_pip_install_extra")
+    def test_check_for_dependencies_other_model(self, mock_check_pip):
+        """Test that check_for_dependencies doesn't call check_pip_install_extra for other models"""
+        from aider.io import InputOutput
+
+        io = InputOutput()
+
+        # Test with a non-Bedrock, non-Vertex AI model
+        from aider.models import check_for_dependencies
+
+        check_for_dependencies(io, "gpt-4")
+
+        # Verify check_pip_install_extra was not called
+        mock_check_pip.assert_not_called()
+
 
 if __name__ == "__main__":
     unittest.main()
\ No newline at end of file
