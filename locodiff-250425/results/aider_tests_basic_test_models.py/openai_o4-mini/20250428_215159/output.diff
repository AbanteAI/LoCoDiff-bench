
index dbe4ed68..7449f367 100644
--- a/aider_tests_basic_test_models.py_expectedoutput.txt (expected):tmp/tmpej1e0ux5_expected.txt	
+++ b/aider_tests_basic_test_models.py_extracted.txt (actual):tmp/tmpjedy8u7b_actual.txt	
@@ -10,7 +10,6 @@ from aider.models import (
     sanity_check_models,
 )
 
-
 class TestModels(unittest.TestCase):
     def setUp(self):
         """Reset MODEL_SETTINGS before each test"""
@@ -49,6 +48,20 @@ class TestModels(unittest.TestCase):
         model = Model("gpt-4-0613")
         self.assertEqual(model.info["max_input_tokens"], 8 * 1024)
 
+    @patch("aider.models.check_for_dependencies")
+    def test_sanity_check_model_calls_check_dependencies(self, mock_check_deps):
+        """Test that sanity_check_model calls check_for_dependencies"""
+        mock_io = MagicMock()
+        model = MagicMock()
+        model.name = "test-model"
+        model.missing_keys = []
+        model.keys_in_environment = True
+        model.info = {"some": "info"}
+
+        sanity_check_model(mock_io, model)
+
+        mock_check_deps.assert_called_once_with(mock_io, "test-model")
+
     @patch("os.environ")
     def test_sanity_check_model_all_set(self, mock_environ):
         mock_environ.get.return_value = "dummy_value"
@@ -94,31 +107,15 @@ class TestModels(unittest.TestCase):
             result
         )  # Should return True because there's a problem with the editor model
         mock_io.tool_warning.assert_called_with(ANY)  # Ensure a warning was issued
-
         warning_messages = [
             warning_call.args[0] for warning_call in mock_io.tool_warning.call_args_list
         ]
         print("Warning messages:", warning_messages)  # Add this line
 
-        self.assertGreaterEqual(mock_io.tool_warning.call_count, 1)  # Expect two warnings
+        self.assertGreaterEqual(mock_io.tool_warning.call_count, 1)
         self.assertTrue(
             any("bogus-model" in msg for msg in warning_messages)
-        )  # Check that one of the warnings mentions the bogus model
-
-    @patch("aider.models.check_for_dependencies")
-    def test_sanity_check_model_calls_check_dependencies(self, mock_check_deps):
-        """Test that sanity_check_model calls check_for_dependencies"""
-        mock_io = MagicMock()
-        model = MagicMock()
-        model.name = "test-model"
-        model.missing_keys = []
-        model.keys_in_environment = True
-        model.info = {"some": "info"}
-
-        sanity_check_model(mock_io, model)
-
-        # Verify check_for_dependencies was called with the model name
-        mock_check_deps.assert_called_once_with(mock_io, "test-model")
+        )
 
     def test_model_aliases(self):
         # Test common aliases
@@ -146,20 +143,6 @@ class TestModels(unittest.TestCase):
         model = Model("opus")
         self.assertEqual(model.name, "claude-3-opus-20240229")
 
-        # Test non-alias passes through unchanged
-        model = Model("gpt-4")
-        self.assertEqual(model.name, "gpt-4")
-
-    def test_o1_use_temp_false(self):
-        # Test GitHub Copilot models
-        model = Model("github/aider_tests_basic_test_models.py_extracted.txt (actual):
         # Create a model instance to test the parse_token_value method
         model = Model("gpt-4")
@@ -205,58 +188,6 @@ class TestModels(unittest.TestCase):
         model.set_thinking_tokens("0.5M")
         self.assertEqual(model.extra_params["thinking"]["budget_tokens"], 0.5 * 1024 * 1024)
 
-    @patch("aider.models.check_pip_install_extra")
-    def test_check_for_dependencies_bedrock(self, mock_check_pip):
-        """Test that check_for_dependencies calls check_pip_install_extra for Bedrock models"""
-        from aider.io import InputOutput
-
-        io = InputOutput()
-
-        # Test with a Bedrock model
-        from aider.models import check_for_dependencies
-
-        check_for_dependencies(io, "bedrock/anthropic.claude-3-sonnet-20240229-v1:0")
-
-        # Verify check_pip_install_extra was called with correct arguments
-        mock_check_pip.assert_called_once_with(
-            io, "boto3", "AWS Bedrock models require the boto3 package.", ["boto3"]
-        )
-
-    @patch("aider.models.check_pip_install_extra")
-    def test_check_for_dependencies_vertex_ai(self, mock_check_pip):
-        """Test that check_for_dependencies calls check_pip_install_extra for Vertex AI models"""
-        from aider.io import InputOutput
-
-        io = InputOutput()
-
-        # Test with a Vertex AI model
-        from aider.models import check_for_dependencies
-
-        check_for_dependencies(io, "vertex_ai/gemini-1.5-pro")
-
-        # Verify check_pip_install_extra was called with correct arguments
-        mock_check_pip.assert_called_once_with(
-            io,
-            "google.cloud.aiplatform",
-            "Google Vertex AI models require the google-cloud-aiplatform package.",
-            ["google-cloud-aiplatform"],
-        )
-
-    @patch("aider.models.check_pip_install_extra")
-    def test_check_for_dependencies_other_model(self, mock_check_pip):
-        """Test that check_for_dependencies doesn't call check_pip_install_extra for other models"""
-        from aider.io import InputOutput
-
-        io = InputOutput()
-
-        # Test with a non-Bedrock, non-Vertex AI model
-        from aider.models import check_for_dependencies
-
-        check_for_dependencies(io, "gpt-4")
-
-        # Verify check_pip_install_extra was not called
-        mock_check_pip.assert_not_called()
-
     def test_get_repo_map_tokens(self):
         # Test default case (no max_input_tokens in info)
         model = Model("gpt-4")
@@ -324,21 +255,6 @@ class TestModels(unittest.TestCase):
         self.assertFalse(model.use_temperature)
         self.assertEqual(model.reasoning_tag, "think")
 
-        # Test provider/deepseek-r1 case
-        model = Model("someprovider/deepseek-r1")
-        self.assertEqual(model.edit_format, "diff")
-        self.assertTrue(model.use_repo_map)
-        self.assertTrue(model.examples_as_sys_msg)
-        self.assertFalse(model.use_temperature)
-        self.assertEqual(model.reasoning_tag, "think")
-
-        # Test provider/deepseek-v3 case
-        model = Model("anotherprovider/deepseek-v3")
-        self.assertEqual(model.edit_format, "diff")
-        self.assertTrue(model.use_repo_map)
-        self.assertEqual(model.reminder, "sys")
-        self.assertTrue(model.examples_as_sys_msg)
-
         # Test llama3 70b case
         model = Model("llama3-70b")
         self.assertEqual(model.edit_format, "diff")
@@ -374,12 +290,25 @@ class TestModels(unittest.TestCase):
         self.assertEqual(model.editor_edit_format, "editor-diff")
         self.assertTrue(model.use_repo_map)
 
+        # Test provider/deepseek-r1 case
+        model = Model("someprovider/deepseek-r1")
+        self.assertEqual(model.edit_format, "diff")
+        self.assertTrue(model.use_repo_map)
+        self.assertTrue(model.examples_as_sys_msg)
+        self.assertFalse(model.use_temperature)
+        self.assertEqual(model.reasoning_tag, "think")
+
+        # Test provider/deepseek-v3 case
+        model = Model("anotherprovider/deepseek-v3")
+        self.assertEqual(model.edit_format, "diff")
+        self.assertTrue(model.use_repo_map)
+        self.assertEqual(model.reminder, "sys")
+        self.assertTrue(model.examples_as_sys_msg)
+
     def test_aider_extra_model_settings(self):
         import tempfile
-
         import yaml
 
-        # Create temporary YAML file with test settings
         test_settings = [
             {
                 "name": "aider/extra_params",
@@ -387,22 +316,16 @@ class TestModels(unittest.TestCase):
                     "extra_headers": {"Foo": "bar"},
                     "some_param": "some value",
                 },
-            },
+            }
         ]
 
-        # Write to a regular file instead of NamedTemporaryFile
-        # for better cross-platform compatibility
         tmp = tempfile.mktemp(suffix=".yml")
         try:
             with open(tmp, "w") as f:
                 yaml.dump(test_settings, f)
 
-            # Register the test settings
             register_models([tmp])
 
-            # Test that defaults are applied when no exact match
-            model = Model("claude-3-5-sonnet-20240620")
-            # Test that both the override and existing headers are present
             model = Model("claude-3-5-sonnet-20240620")
             self.assertEqual(model.extra_params["extra_headers"]["Foo"], "bar")
             self.assertEqual(
@@ -412,14 +335,11 @@ class TestModels(unittest.TestCase):
             self.assertEqual(model.extra_params["some_param"], "some value")
             self.assertEqual(model.extra_params["max_tokens"], 8192)
 
-            # Test that exact match overrides defaults but not overrides
             model = Model("gpt-4")
             self.assertEqual(model.extra_params["extra_headers"]["Foo"], "bar")
             self.assertEqual(model.extra_params["some_param"], "some value")
         finally:
-            # Clean up the temporary file
             import os
-
             try:
                 os.unlink(tmp)
             except OSError:
@@ -481,21 +401,6 @@ class TestModels(unittest.TestCase):
         )
         self.assertNotIn("num_ctx", mock_completion.call_args.kwargs)
 
-    def test_use_temperature_settings(self):
-        # Test use_temperature=True (default) uses temperature=0
-        model = Model("gpt-4")
-        self.assertTrue(model.use_temperature)
-        self.assertEqual(model.use_temperature, True)
-
-        # Test use_temperature=False doesn't pass temperature
-        model = Model("github/aider_tests_basic_test_models.py_extracted.txt (actual):
         # Test default timeout is used when not specified in extra_params
@@ -525,6 +430,21 @@ class TestModels(unittest.TestCase):
             timeout=300,  # From extra_params
         )
 
+    def test_use_temperature_settings(self):
+        # Test use_temperature=True (default) uses temperature=0
+        model = Model("gpt-4")
+        self.assertTrue(model.use_temperature)
+        self.assertEqual(model.use_temperature, True)
+
+        # Test use_temperature=False doesn't pass temperature
+        model = Model("github/aider_tests_basic_test_models.py_extracted.txt (actual):
         # Test use_temperature=True sends temperature=0
@@ -558,6 +478,46 @@ class TestModels(unittest.TestCase):
             timeout=600,
         )
 
+    @patch("aider.models.check_pip_install_extra")
+    def test_check_for_dependencies_bedrock(self, mock_check_pip):
+        """Test that check_for_dependencies calls check_pip_install_extra for Bedrock models"""
+        from aider.io import InputOutput
+        io = InputOutput()
+
+        from aider.models import check_for_dependencies
+        check_for_dependencies(io, "bedrock/anthropic.claude-3-sonnet-20240229-v1:0")
+
+        mock_check_pip.assert_called_once_with(
+            io, "boto3", "AWS Bedrock models require the boto3 package.", ["boto3"]
+        )
+
+    @patch("aider.models.check_pip_install_extra")
+    def test_check_for_dependencies_vertex_ai(self, mock_check_pip):
+        """Test that check_for_dependencies calls check_pip_install_extra for Vertex AI models"""
+        from aider.io import InputOutput
+        io = InputOutput()
+
+        from aider.models import check_for_dependencies
+        check_for_dependencies(io, "vertex_ai/gemini-1.5-pro")
+
+        mock_check_pip.assert_called_once_with(
+            io,
+            "google.cloud.aiplatform",
+            "Google Vertex AI models require the google-cloud-aiplatform package.",
+            ["google-cloud-aiplatform"],
+        )
+
+    @patch("aider.models.check_pip_install_extra")
+    def test_check_for_dependencies_other_model(self, mock_check_pip):
+        """Test that check_for_dependencies doesn't call check_pip_install_extra for other models"""
+        from aider.io import InputOutput
+        io = InputOutput()
+
+        from aider.models import check_for_dependencies
+        check_for_dependencies(io, "gpt-4")
+
+        mock_check_pip.assert_not_called()
+
 
 if __name__ == "__main__":
     unittest.main()
\ No newline at end of file
