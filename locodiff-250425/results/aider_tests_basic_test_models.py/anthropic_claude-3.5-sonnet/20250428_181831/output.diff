--- aider_tests_basic_test_models.py_expectedoutput.txt (expected)+++ aider_tests_basic_test_models.py_extracted.txt (actual)@@ -2,7 +2,7 @@ from unittest.mock import ANY, MagicMock, patch
 
 from aider.models import (
-    ANTHROPIC_BETA_HEADER,
+    MODEL_SETTINGS,
     Model,
     ModelInfoManager,
     register_models,
@@ -49,40 +49,6 @@         model = Model("gpt-4-0613")
         self.assertEqual(model.info["max_input_tokens"], 8 * 1024)
 
-    @patch("os.environ")
-    def test_sanity_check_model_all_set(self, mock_environ):
-        mock_environ.get.return_value = "dummy_value"
-        mock_io = MagicMock()
-        model = MagicMock()
-        model.name = "test-model"
-        model.missing_keys = ["API_KEY1", "API_KEY2"]
-        model.keys_in_environment = True
-        model.info = {"some": "info"}
-
-        sanity_check_model(mock_io, model)
-
-        mock_io.tool_output.assert_called()
-        calls = mock_io.tool_output.call_args_list
-        self.assertIn("- API_KEY1: Set", str(calls))
-        self.assertIn("- API_KEY2: Set", str(calls))
-
-    @patch("os.environ")
-    def test_sanity_check_model_not_set(self, mock_environ):
-        mock_environ.get.return_value = ""
-        mock_io = MagicMock()
-        model = MagicMock()
-        model.name = "test-model"
-        model.missing_keys = ["API_KEY1", "API_KEY2"]
-        model.keys_in_environment = True
-        model.info = {"some": "info"}
-
-        sanity_check_model(mock_io, model)
-
-        mock_io.tool_output.assert_called()
-        calls = mock_io.tool_output.call_args_list
-        self.assertIn("- API_KEY1: Not set", str(calls))
-        self.assertIn("- API_KEY2: Not set", str(calls))
-
     def test_sanity_check_models_bogus_editor(self):
         mock_io = MagicMock()
         main_model = Model("gpt-4")
@@ -93,8 +59,7 @@         self.assertTrue(
             result
         )  # Should return True because there's a problem with the editor model
-        mock_io.tool_warning.assert_called_with(ANY)  # Ensure a warning was issued
-
+        mock_io.tool_warning.assert_called()
         warning_messages = [
             warning_call.args[0] for warning_call in mock_io.tool_warning.call_args_list
         ]
@@ -481,22 +446,6 @@         )
         self.assertNotIn("num_ctx", mock_completion.call_args.kwargs)
 
-    def test_use_temperature_settings(self):
-        # Test use_temperature=True (default) uses temperature=0
-        model = Model("gpt-4")
-        self.assertTrue(model.use_temperature)
-        self.assertEqual(model.use_temperature, True)
-
-        # Test use_temperature=False doesn't pass temperature
-        model = Model("github/o1-mini")
-        self.assertFalse(model.use_temperature)
-
-        # Test use_temperature as float value
-        model = Model("gpt-4")
-        model.use_temperature = 0.7
-        self.assertEqual(model.use_temperature, 0.7)
-
-    @patch("aider.models.litellm.completion")
     def test_request_timeout_default(self, mock_completion):
         # Test default timeout is used when not specified in extra_params
         model = Model("gpt-4")
@@ -525,6 +474,21 @@             timeout=300,  # From extra_params
         )
 
+    def test_use_temperature_settings(self):
+        # Test use_temperature=True (default) uses temperature=0
+        model = Model("gpt-4")
+        self.assertTrue(model.use_temperature)
+        self.assertEqual(model.use_temperature, True)
+
+        # Test use_temperature=False doesn't pass temperature
+        model = Model("github/o1-mini")
+        self.assertFalse(model.use_temperature)
+
+        # Test use_temperature as float value
+        model = Model("gpt-4")
+        model.use_temperature = 0.7
+        self.assertEqual(model.use_temperature, 0.7)
+
     @patch("aider.models.litellm.completion")
     def test_use_temperature_in_send_completion(self, mock_completion):
         # Test use_temperature=True sends temperature=0
@@ -539,7 +503,7 @@             timeout=600,
         )
 
-        # Test use_temperature=False doesn't send temperature
+        # Test use_temperature=False doesn't pass temperature
         model = Model("github/o1-mini")
         messages = [{"role": "user", "content": "Hello"}]
         model.send_completion(messages, functions=None, stream=False)
