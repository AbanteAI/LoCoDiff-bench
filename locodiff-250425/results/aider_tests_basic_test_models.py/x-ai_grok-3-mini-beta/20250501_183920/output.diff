
index dbe4ed68..4850c21f 100644
--- a/aider_tests_basic_test_models.py_expectedoutput.txt (expected):tmp/tmp6jln7a9r_expected.txt	
+++ b/aider_tests_basic_test_models.py_extracted.txt (actual):tmp/tmpl0gxqn03_actual.txt	
@@ -2,9 +2,10 @@ import unittest
 from unittest.mock import ANY, MagicMock, patch
 
 from aider.models import (
-    ANTHROPIC_BETA_HEADER,
+    MODEL_SETTINGS,
     Model,
     ModelInfoManager,
+    check_for_dependencies,
     register_models,
     sanity_check_model,
     sanity_check_models,
@@ -49,62 +50,6 @@ class TestModels(unittest.TestCase):
         model = Model("gpt-4-0613")
         self.assertEqual(model.info["max_input_tokens"], 8 * 1024)
 
-    @patch("os.environ")
-    def test_sanity_check_model_all_set(self, mock_environ):
-        mock_environ.get.return_value = "dummy_value"
-        mock_io = MagicMock()
-        model = MagicMock()
-        model.name = "test-model"
-        model.missing_keys = ["API_KEY1", "API_KEY2"]
-        model.keys_in_environment = True
-        model.info = {"some": "info"}
-
-        sanity_check_model(mock_io, model)
-
-        mock_io.tool_output.assert_called()
-        calls = mock_io.tool_output.call_args_list
-        self.assertIn("- API_KEY1: Set", str(calls))
-        self.assertIn("- API_KEY2: Set", str(calls))
-
-    @patch("os.environ")
-    def test_sanity_check_model_not_set(self, mock_environ):
-        mock_environ.get.return_value = ""
-        mock_io = MagicMock()
-        model = MagicMock()
-        model.name = "test-model"
-        model.missing_keys = ["API_KEY1", "API_KEY2"]
-        model.keys_in_environment = True
-        model.info = {"some": "info"}
-
-        sanity_check_model(mock_io, model)
-
-        mock_io.tool_output.assert_called()
-        calls = mock_io.tool_output.call_args_list
-        self.assertIn("- API_KEY1: Not set", str(calls))
-        self.assertIn("- API_KEY2: Not set", str(calls))
-
-    def test_sanity_check_models_bogus_editor(self):
-        mock_io = MagicMock()
-        main_model = Model("gpt-4")
-        main_model.editor_model = Model("bogus-model")
-
-        result = sanity_check_models(mock_io, main_model)
-
-        self.assertTrue(
-            result
-        )  # Should return True because there's a problem with the editor model
-        mock_io.tool_warning.assert_called_with(ANY)  # Ensure a warning was issued
-
-        warning_messages = [
-            warning_call.args[0] for warning_call in mock_io.tool_warning.call_args_list
-        ]
-        print("Warning messages:", warning_messages)  # Add this line
-
-        self.assertGreaterEqual(mock_io.tool_warning.call_count, 1)  # Expect two warnings
-        self.assertTrue(
-            any("bogus-model" in msg for msg in warning_messages)
-        )  # Check that one of the warnings mentions the bogus model
-
     @patch("aider.models.check_for_dependencies")
     def test_sanity_check_model_calls_check_dependencies(self, mock_check_deps):
         """Test that sanity_check_model calls check_for_dependencies"""
@@ -146,10 +91,6 @@ class TestModels(unittest.TestCase):
         model = Model("opus")
         self.assertEqual(model.name, "claude-3-opus-20240229")
 
-        # Test non-alias passes through unchanged
-        model = Model("gpt-4")
-        self.assertEqual(model.name, "gpt-4")
-
     def test_o1_use_temp_false(self):
         # Test GitHub Copilot models
         model = Model("github/aider_tests_basic_test_models.py_extracted.txt (actual):
         model = Model("deepseek-v3")
         self.assertEqual(model.edit_format, "diff")
         self.assertTrue(model.use_repo_map)
-        self.assertEqual(model.reminder, "sys")
+        self.assertEqual(model.reasoning_tag, "sys")
         self.assertTrue(model.examples_as_sys_msg)
 
         # Test deepseek reasoner case
@@ -336,7 +277,7 @@ class TestModels(unittest.TestCase):
         model = Model("anotherprovider/deepseek-v3")
         self.assertEqual(model.edit_format, "diff")
         self.assertTrue(model.use_repo_map)
-        self.assertEqual(model.reminder, "sys")
+        self.assertEqual(model.reasoning_tag, "sys")
         self.assertTrue(model.examples_as_sys_msg)
 
         # Test llama3 70b case
@@ -354,14 +295,14 @@ class TestModels(unittest.TestCase):
 
         # Test gpt-3.5 case
         model = Model("gpt-3.5")
-        self.assertEqual(model.reminder, "sys")
+        self.assertEqual(model.reasoning_tag, "sys")
 
         # Test 3.5-sonnet case
         model = Model("claude-3.5-sonnet")
         self.assertEqual(model.edit_format, "diff")
         self.assertTrue(model.use_repo_map)
         self.assertTrue(model.examples_as_sys_msg)
-        self.assertEqual(model.reminder, "user")
+        self.assertEqual(model.reasoning_tag, "user")
 
         # Test o1- prefix case
         model = Model("o1-something")
@@ -401,9 +342,8 @@ class TestModels(unittest.TestCase):
             register_models([tmp])
 
             # Test that defaults are applied when no exact match
-            model = Model("claude-3-5-sonnet-20240620")
+            model = Model("claude-3-5-sonnet-20241022")
             # Test that both the override and existing headers are present
-            model = Model("claude-3-5-sonnet-20240620")
             self.assertEqual(model.extra_params["extra_headers"]["Foo"], "bar")
             self.assertEqual(
                 model.extra_params["extra_headers"]["anthropic-beta"],
@@ -426,8 +366,7 @@ class TestModels(unittest.TestCase):
                 pass
 
     @patch("aider.models.litellm.completion")
-    @patch.object(Model, "token_count")
-    def test_ollama_num_ctx_set_when_missing(self, mock_token_count, mock_completion):
+    def test_ollama_num_ctx_set_when_missing(self, mock_completion):
         mock_token_count.return_value = 1000
 
         model = Model("ollama/aider_tests_basic_test_models.py_expectedoutput.txt (expected):
         self.assertEqual(model.use_temperature, 0.7)
 
     @patch("aider.models.litellm.completion")
-    def test_request_timeout_default(self, mock_completion):
-        # Test default timeout is used when not specified in extra_params
+    def test_use_temperature_in_send_completion(self, mock_completion):
+        # Test use_temperature=True sends temperature=0
         model = Model("gpt-4")
         messages = [{"role": "user", "content": "Hello"}]
         model.send_completion(messages, functions=None, stream=False)
@@ -507,27 +446,31 @@ class TestModels(unittest.TestCase):
             messages=messages,
             stream=False,
             temperature=0,
-            timeout=600,  # Default timeout
+            timeout=600,
         )
 
-    @patch("aider.models.litellm.completion")
-    def test_request_timeout_from_extra_params(self, mock_completion):
-        # Test timeout from extra_params overrides default
+        # Test use_temperature=False doesn't send temperature
+        model = Model("github/aider_tests_basic_test_models.py_extracted.txt (actual): "user", "content": "Hello"}]
+        model.send_completion(messages, functions=None, stream=False)
+        self.assertNotIn("temperature", mock_completion.call_args.kwargs)
+
+        # Test use_temperature as float sends that value
         model = Model("gpt-4")
-        model.extra_params = {"timeout": 300}  # 5 minutes
+        model.use_temperature = 0.7
         messages = [{"role": "user", "content": "Hello"}]
         model.send_completion(messages, functions=None, stream=False)
         mock_completion.assert_called_with(
             model=model.name,
             messages=messages,
             stream=False,
-            temperature=0,
-            timeout=300,  # From extra_params
+            temperature=0.7,
+            timeout=600,
         )
 
     @patch("aider.models.litellm.completion")
-    def test_use_temperature_in_send_completion(self, mock_completion):
-        # Test use_temperature=True sends temperature=0
+    def test_request_timeout_default(self, mock_completion):
+        # Test default timeout is used when not specified in extra_params
         model = Model("gpt-4")
         messages = [{"role": "user", "content": "Hello"}]
         model.send_completion(messages, functions=None, stream=False)
@@ -536,28 +479,20 @@ class TestModels(unittest.TestCase):
             messages=messages,
             stream=False,
             temperature=0,
-            timeout=600,
+            timeout=600,  # Default timeout
         )
 
-        # Test use_temperature=False doesn't send temperature
-        model = Model("github/aider_tests_basic_test_models.py_extracted.txt (actual): "user", "content": "Hello"}]
-        model.send_completion(messages, functions=None, stream=False)
-        self.assertNotIn("temperature", mock_completion.call_args.kwargs)
-
-        # Test use_temperature as float sends that value
+    @patch("aider.models.litellm.completion")
+    def test_request_timeout_from_extra_params(self, mock_completion):
+        # Test timeout from extra_params overrides default
         model = Model("gpt-4")
-        model.use_temperature = 0.7
+        model.extra_params = {"timeout": 300}  # 5 minutes
         messages = [{"role": "user", "content": "Hello"}]
         model.send_completion(messages, functions=None, stream=False)
         mock_completion.assert_called_with(
             model=model.name,
             messages=messages,
             stream=False,
-            temperature=0.7,
-            timeout=600,
-        )
-
-
-if __name__ == "__main__":
-    unittest.main()
\ No newline at end of file
+            temperature=0,
+            timeout=300,  # From extra_params
+        )
\ No newline at end of file
