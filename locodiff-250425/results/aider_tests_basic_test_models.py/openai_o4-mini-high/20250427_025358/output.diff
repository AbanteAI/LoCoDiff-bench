
index dbe4ed68..38054d87 100644
--- a/aider_tests_basic_test_models.py_expectedoutput.txt (expected):tmp/tmpk2x61omw_expected.txt	
+++ b/aider_tests_basic_test_models.py_extracted.txt (actual):tmp/tmpaxk3cymh_actual.txt	
@@ -25,6 +25,21 @@ class TestModels(unittest.TestCase):
         MODEL_SETTINGS.clear()
         MODEL_SETTINGS.extend(self._original_settings)
 
+    @patch("aider.models.check_for_dependencies")
+    def test_sanity_check_model_calls_check_dependencies(self, mock_check_deps):
+        """Test that sanity_check_model calls check_for_dependencies"""
+        mock_io = MagicMock()
+        model = MagicMock()
+        model.name = "test-model"
+        model.missing_keys = []
+        model.keys_in_environment = True
+        model.info = {"some": "info"}
+
+        sanity_check_model(mock_io, model)
+
+        # Verify check_for_dependencies was called with the model name
+        mock_check_deps.assert_called_once_with(mock_io, "test-model")
+
     def test_get_model_info_nonexistent(self):
         manager = ModelInfoManager()
         info = manager.get_model_info("non-existent-model")
@@ -94,32 +109,15 @@ class TestModels(unittest.TestCase):
             result
         )  # Should return True because there's a problem with the editor model
         mock_io.tool_warning.assert_called_with(ANY)  # Ensure a warning was issued
-
         warning_messages = [
             warning_call.args[0] for warning_call in mock_io.tool_warning.call_args_list
         ]
         print("Warning messages:", warning_messages)  # Add this line
-
         self.assertGreaterEqual(mock_io.tool_warning.call_count, 1)  # Expect two warnings
         self.assertTrue(
             any("bogus-model" in msg for msg in warning_messages)
         )  # Check that one of the warnings mentions the bogus model
 
-    @patch("aider.models.check_for_dependencies")
-    def test_sanity_check_model_calls_check_dependencies(self, mock_check_deps):
-        """Test that sanity_check_model calls check_for_dependencies"""
-        mock_io = MagicMock()
-        model = MagicMock()
-        model.name = "test-model"
-        model.missing_keys = []
-        model.keys_in_environment = True
-        model.info = {"some": "info"}
-
-        sanity_check_model(mock_io, model)
-
-        # Verify check_for_dependencies was called with the model name
-        mock_check_deps.assert_called_once_with(mock_io, "test-model")
-
     def test_model_aliases(self):
         # Test common aliases
         model = Model("4")
@@ -160,50 +158,9 @@ class TestModels(unittest.TestCase):
         self.assertEqual(model.name, "github/aider_tests_basic_test_models.py_extracted.txt (actual):
-        # Create a model instance to test the parse_token_value method
-        model = Model("gpt-4")
-
-        # Test integer inputs
-        self.assertEqual(model.parse_token_value(8096), 8096)
-        self.assertEqual(model.parse_token_value(1000), 1000)
-
-        # Test string inputs
-        self.assertEqual(model.parse_token_value("8096"), 8096)
-
-        # Test k/K suffix (kilobytes)
-        self.assertEqual(model.parse_token_value("8k"), 8 * 1024)
-        self.assertEqual(model.parse_token_value("8K"), 8 * 1024)
-        self.assertEqual(model.parse_token_value("10.5k"), 10.5 * 1024)
-        self.assertEqual(model.parse_token_value("0.5K"), 0.5 * 1024)
-
-        # Test m/M suffix (megabytes)
-        self.assertEqual(model.parse_token_value("1m"), 1 * 1024 * 1024)
-        self.assertEqual(model.parse_token_value("1M"), 1 * 1024 * 1024)
-        self.assertEqual(model.parse_token_value("0.5M"), 0.5 * 1024 * 1024)
-
-        # Test with spaces
-        self.assertEqual(model.parse_token_value(" 8k "), 8 * 1024)
-
-        # Test conversion from other types
-        self.assertEqual(model.parse_token_value(8.0), 8)
-
-    def test_set_thinking_tokens(self):
-        # Test that set_thinking_tokens correctly sets the tokens with different formats
+        # Test non-alias passes through unchanged
         model = Model("gpt-4")
-
-        # Test with integer
-        model.set_thinking_tokens(8096)
-        self.assertEqual(model.extra_params["thinking"]["budget_tokens"], 8096)
-        self.assertFalse(model.use_temperature)
-
-        # Test with string
-        model.set_thinking_tokens("10k")
-        self.assertEqual(model.extra_params["thinking"]["budget_tokens"], 10 * 1024)
-
-        # Test with decimal value
-        model.set_thinking_tokens("0.5M")
-        self.assertEqual(model.extra_params["thinking"]["budget_tokens"], 0.5 * 1024 * 1024)
+        self.assertEqual(model.name, "gpt-4")
 
     @patch("aider.models.check_pip_install_extra")
     def test_check_for_dependencies_bedrock(self, mock_check_pip):
@@ -215,11 +172,16 @@ class TestModels(unittest.TestCase):
         # Test with a Bedrock model
         from aider.models import check_for_dependencies
 
-        check_for_dependencies(io, "bedrock/anthropic.claude-3-sonnet-20240229-v1:0")
+        check_for_dependencies(
+            io, "bedrock/anthropic.claude-3-sonnet-20240229-v1:0"
+        )
 
         # Verify check_pip_install_extra was called with correct arguments
         mock_check_pip.assert_called_once_with(
-            io, "boto3", "AWS Bedrock models require the boto3 package.", ["boto3"]
+            io,
+            "boto3",
+            "AWS Bedrock models require the boto3 package.",
+            ["boto3"],
         )
 
     @patch("aider.models.check_pip_install_extra")
@@ -257,126 +219,8 @@ class TestModels(unittest.TestCase):
         # Verify check_pip_install_extra was not called
         mock_check_pip.assert_not_called()
 
-    def test_get_repo_map_tokens(self):
-        # Test default case (no max_input_tokens in info)
-        model = Model("gpt-4")
-        model.info = {}
-        self.assertEqual(model.get_repo_map_tokens(), 1024)
-
-        # Test minimum boundary (max_input_tokens < 8192)
-        model.info = {"max_input_tokens": 4096}
-        self.assertEqual(model.get_repo_map_tokens(), 1024)
-
-        # Test middle range (max_input_tokens = 16384)
-        model.info = {"max_input_tokens": 16384}
-        self.assertEqual(model.get_repo_map_tokens(), 2048)
-
-        # Test maximum boundary (max_input_tokens > 32768)
-        model.info = {"max_input_tokens": 65536}
-        self.assertEqual(model.get_repo_map_tokens(), 4096)
-
-        # Test exact boundary values
-        model.info = {"max_input_tokens": 8192}
-        self.assertEqual(model.get_repo_map_tokens(), 1024)
-
-        model.info = {"max_input_tokens": 32768}
-        self.assertEqual(model.get_repo_map_tokens(), 4096)
-
-    def test_configure_model_settings(self):
-        # Test o3-mini case
-        model = Model("something/o3-mini")
-        self.assertEqual(model.edit_format, "diff")
-        self.assertTrue(model.use_repo_map)
-        self.assertFalse(model.use_temperature)
-
-        # Test o1-mini case
-        model = Model("something/o1-mini")
-        self.assertTrue(model.use_repo_map)
-        self.assertFalse(model.use_temperature)
-        self.assertFalse(model.use_system_prompt)
-
-        # Test o1-preview case
-        model = Model("something/o1-preview")
-        self.assertEqual(model.edit_format, "diff")
-        self.assertTrue(model.use_repo_map)
-        self.assertFalse(model.use_temperature)
-        self.assertFalse(model.use_system_prompt)
-
-        # Test o1 case
-        model = Model("something/o1")
-        self.assertEqual(model.edit_format, "diff")
-        self.assertTrue(model.use_repo_map)
-        self.assertFalse(model.use_temperature)
-        self.assertFalse(model.streaming)
-
-        # Test deepseek v3 case
-        model = Model("deepseek-v3")
-        self.assertEqual(model.edit_format, "diff")
-        self.assertTrue(model.use_repo_map)
-        self.assertEqual(model.reminder, "sys")
-        self.assertTrue(model.examples_as_sys_msg)
-
-        # Test deepseek reasoner case
-        model = Model("deepseek-r1")
-        self.assertEqual(model.edit_format, "diff")
-        self.assertTrue(model.use_repo_map)
-        self.assertTrue(model.examples_as_sys_msg)
-        self.assertFalse(model.use_temperature)
-        self.assertEqual(model.reasoning_tag, "think")
-
-        # Test provider/deepseek-r1 case
-        model = Model("someprovider/deepseek-r1")
-        self.assertEqual(model.edit_format, "diff")
-        self.assertTrue(model.use_repo_map)
-        self.assertTrue(model.examples_as_sys_msg)
-        self.assertFalse(model.use_temperature)
-        self.assertEqual(model.reasoning_tag, "think")
-
-        # Test provider/deepseek-v3 case
-        model = Model("anotherprovider/deepseek-v3")
-        self.assertEqual(model.edit_format, "diff")
-        self.assertTrue(model.use_repo_map)
-        self.assertEqual(model.reminder, "sys")
-        self.assertTrue(model.examples_as_sys_msg)
-
-        # Test llama3 70b case
-        model = Model("llama3-70b")
-        self.assertEqual(model.edit_format, "diff")
-        self.assertTrue(model.use_repo_map)
-        self.assertTrue(model.send_undo_reply)
-        self.assertTrue(model.examples_as_sys_msg)
-
-        # Test gpt-4 case
-        model = Model("gpt-4")
-        self.assertEqual(model.edit_format, "diff")
-        self.assertTrue(model.use_repo_map)
-        self.assertTrue(model.send_undo_reply)
-
-        # Test gpt-3.5 case
-        model = Model("gpt-3.5")
-        self.assertEqual(model.reminder, "sys")
-
-        # Test 3.5-sonnet case
-        model = Model("claude-3.5-sonnet")
-        self.assertEqual(model.edit_format, "diff")
-        self.assertTrue(model.use_repo_map)
-        self.assertTrue(model.examples_as_sys_msg)
-        self.assertEqual(model.reminder, "user")
-
-        # Test o1- prefix case
-        model = Model("o1-something")
-        self.assertFalse(model.use_system_prompt)
-        self.assertFalse(model.use_temperature)
-
-        # Test qwen case
-        model = Model("qwen-coder-2.5-32b")
-        self.assertEqual(model.edit_format, "diff")
-        self.assertEqual(model.editor_edit_format, "editor-diff")
-        self.assertTrue(model.use_repo_map)
-
     def test_aider_extra_model_settings(self):
         import tempfile
-
         import yaml
 
         # Create temporary YAML file with test settings
@@ -391,7 +235,6 @@ class TestModels(unittest.TestCase):
         ]
 
         # Write to a regular file instead of NamedTemporaryFile
-        # for better cross-platform compatibility
         tmp = tempfile.mktemp(suffix=".yml")
         try:
             with open(tmp, "w") as f:
@@ -400,8 +243,6 @@ class TestModels(unittest.TestCase):
             # Register the test settings
             register_models([tmp])
 
-            # Test that defaults are applied when no exact match
-            model = Model("claude-3-5-sonnet-20240620")
             # Test that both the override and existing headers are present
             model = Model("claude-3-5-sonnet-20240620")
             self.assertEqual(model.extra_params["extra_headers"]["Foo"], "bar")
@@ -425,11 +266,35 @@ class TestModels(unittest.TestCase):
             except OSError:
                 pass
 
+    def test_get_repo_map_tokens(self):
+        # Test default case (no max_input_tokens in info)
+        model = Model("gpt-4")
+        model.info = {}
+        self.assertEqual(model.get_repo_map_tokens(), 1024)
+
+        # Test minimum boundary (max_input_tokens < 8192)
+        model.info = {"max_input_tokens": 4096}
+        self.assertEqual(model.get_repo_map_tokens(), 1024)
+
+        # Test middle range (max_input_tokens = 16384)
+        model.info = {"max_input_tokens": 16384}
+        self.assertEqual(model.get_repo_map_tokens(), 2048)
+
+        # Test maximum boundary (max_input_tokens > 32768)
+        model.info = {"max_input_tokens": 65536}
+        self.assertEqual(model.get_repo_map_tokens(), 4096)
+
+        # Test exact boundary values
+        model.info = {"max_input_tokens": 8192}
+        self.assertEqual(model.get_repo_map_tokens(), 1024)
+
+        model.info = {"max_input_tokens": 32768}
+        self.assertEqual(model.get_repo_map_tokens(), 4096)
+
     @patch("aider.models.litellm.completion")
     @patch.object(Model, "token_count")
     def test_ollama_num_ctx_set_when_missing(self, mock_token_count, mock_completion):
         mock_token_count.return_value = 1000
-
         model = Model("ollama/aider_tests_basic_test_models.py_expectedoutput.txt (expected): "user", "content": "Hello"}]
 
@@ -497,8 +362,8 @@ class TestModels(unittest.TestCase):
         self.assertEqual(model.use_temperature, 0.7)
 
     @patch("aider.models.litellm.completion")
-    def test_request_timeout_default(self, mock_completion):
-        # Test default timeout is used when not specified in extra_params
+    def test_use_temperature_in_send_completion(self, mock_completion):
+        # Test use_temperature=True sends temperature=0
         model = Model("gpt-4")
         messages = [{"role": "user", "content": "Hello"}]
         model.send_completion(messages, functions=None, stream=False)
@@ -507,27 +372,31 @@ class TestModels(unittest.TestCase):
             messages=messages,
             stream=False,
             temperature=0,
-            timeout=600,  # Default timeout
+            timeout=600,
         )
 
-    @patch("aider.models.litellm.completion")
-    def test_request_timeout_from_extra_params(self, mock_completion):
-        # Test timeout from extra_params overrides default
+        # Test use_temperature=False doesn't send temperature
+        model = Model("github/aider_tests_basic_test_models.py_extracted.txt (actual): "user", "content": "Hello"}]
+        model.send_completion(messages, functions=None, stream=False)
+        self.assertNotIn("temperature", mock_completion.call_args.kwargs)
+
+        # Test use_temperature as float sends that value
         model = Model("gpt-4")
-        model.extra_params = {"timeout": 300}  # 5 minutes
+        model.use_temperature = 0.7
         messages = [{"role": "user", "content": "Hello"}]
         model.send_completion(messages, functions=None, stream=False)
         mock_completion.assert_called_with(
             model=model.name,
             messages=messages,
             stream=False,
-            temperature=0,
-            timeout=300,  # From extra_params
+            temperature=0.7,
+            timeout=600,
         )
 
     @patch("aider.models.litellm.completion")
-    def test_use_temperature_in_send_completion(self, mock_completion):
-        # Test use_temperature=True sends temperature=0
+    def test_request_timeout_default(self, mock_completion):
+        # Test default timeout is used when not specified in extra_params
         model = Model("gpt-4")
         messages = [{"role": "user", "content": "Hello"}]
         model.send_completion(messages, functions=None, stream=False)
@@ -539,25 +408,158 @@ class TestModels(unittest.TestCase):
             timeout=600,
         )
 
-        # Test use_temperature=False doesn't send temperature
-        model = Model("github/aider_tests_basic_test_models.py_extracted.txt (actual): "user", "content": "Hello"}]
-        model.send_completion(messages, functions=None, stream=False)
-        self.assertNotIn("temperature", mock_completion.call_args.kwargs)
-
-        # Test use_temperature as float sends that value
+    @patch("aider.models.litellm.completion")
+    def test_request_timeout_from_extra_params(self, mock_completion):
+        # Test timeout from extra_params overrides default
         model = Model("gpt-4")
-        model.use_temperature = 0.7
+        model.extra_params = {"timeout": 300}
         messages = [{"role": "user", "content": "Hello"}]
         model.send_completion(messages, functions=None, stream=False)
         mock_completion.assert_called_with(
             model=model.name,
             messages=messages,
             stream=False,
-            temperature=0.7,
-            timeout=600,
+            temperature=0,
+            timeout=300,
         )
 
+    def test_configure_model_settings(self):
+        # Test o3-mini case
+        model = Model("something/o3-mini")
+        self.assertEqual(model.edit_format, "diff")
+        self.assertTrue(model.use_repo_map)
+        self.assertFalse(model.use_temperature)
+
+        # Test o1-mini case
+        model = Model("something/o1-mini")
+        self.assertTrue(model.use_repo_map)
+        self.assertFalse(model.use_temperature)
+        self.assertFalse(model.use_system_prompt)
+
+        # Test o1-preview case
+        model = Model("something/o1-preview")
+        self.assertEqual(model.edit_format, "diff")
+        self.assertTrue(model.use_repo_map)
+        self.assertFalse(model.use_temperature)
+        self.assertFalse(model.use_system_prompt)
+
+        # Test o1 case
+        model = Model("something/o1")
+        self.assertEqual(model.edit_format, "diff")
+        self.assertTrue(model.use_repo_map)
+        self.assertFalse(model.use_temperature)
+        self.assertFalse(model.streaming)
+
+        # Test deepseek v3 case
+        model = Model("deepseek-v3")
+        self.assertEqual(model.edit_format, "diff")
+        self.assertTrue(model.use_repo_map)
+        self.assertEqual(model.reminder, "sys")
+        self.assertTrue(model.examples_as_sys_msg)
+
+        # Test deepseek reasoner case
+        model = Model("deepseek-r1")
+        self.assertEqual(model.edit_format, "diff")
+        self.assertTrue(model.use_repo_map)
+        self.assertTrue(model.examples_as_sys_msg)
+        self.assertFalse(model.use_temperature)
+        self.assertEqual(model.reasoning_tag, "think")
+
+        # Test provider/deepseek-r1 case
+        model = Model("someprovider/deepseek-r1")
+        self.assertEqual(model.edit_format, "diff")
+        self.assertTrue(model.use_repo_map)
+        self.assertTrue(model.examples_as_sys_msg)
+        self.assertFalse(model.use_temperature)
+        self.assertEqual(model.reasoning_tag, "think")
+
+        # Test provider/deepseek-v3 case
+        model = Model("anotherprovider/deepseek-v3")
+        self.assertEqual(model.edit_format, "diff")
+        self.assertTrue(model.use_repo_map)
+        self.assertEqual(model.reminder, "sys")
+        self.assertTrue(model.examples_as_sys_msg)
+
+        # Test llama3 70b case
+        model = Model("llama3-70b")
+        self.assertEqual(model.edit_format, "diff")
+        self.assertTrue(model.use_repo_map)
+        self.assertTrue(model.send_undo_reply)
+        self.assertTrue(model.examples_as_sys_msg)
+
+        # Test gpt-4 case
+        model = Model("gpt-4")
+        self.assertEqual(model.edit_format, "diff")
+        self.assertTrue(model.use_repo_map)
+        self.assertTrue(model.send_undo_reply)
+
+        # Test gpt-3.5 case
+        model = Model("gpt-3.5")
+        self.assertEqual(model.reminder, "sys")
+
+        # Test 3.5-sonnet case
+        model = Model("claude-3.5-sonnet")
+        self.assertEqual(model.edit_format, "diff")
+        self.assertTrue(model.use_repo_map)
+        self.assertTrue(model.examples_as_sys_msg)
+        self.assertEqual(model.reminder, "user")
+
+        # Test o1- prefix case
+        model = Model("o1-something")
+        self.assertFalse(model.use_system_prompt)
+        self.assertFalse(model.use_temperature)
+
+        # Test qwen case
+        model = Model("qwen-coder-2.5-32b")
+        self.assertEqual(model.edit_format, "diff")
+        self.assertEqual(model.editor_edit_format, "editor-diff")
+        self.assertTrue(model.use_repo_map)
+
+    def test_parse_token_value(self):
+        # Create a model instance to test the parse_token_value method
+        model = Model("gpt-4")
+
+        # Test integer inputs
+        self.assertEqual(model.parse_token_value(8096), 8096)
+        self.assertEqual(model.parse_token_value(1000), 1000)
+
+        # Test string inputs
+        self.assertEqual(model.parse_token_value("8096"), 8096)
+
+        # Test k/K suffix (kilobytes)
+        self.assertEqual(model.parse_token_value("8k"), 8 * 1024)
+        self.assertEqual(model.parse_token_value("8K"), 8 * 1024)
+        self.assertEqual(model.parse_token_value("10.5k"), 10.5 * 1024)
+        self.assertEqual(model.parse_token_value("0.5K"), 0.5 * 1024)
+
+        # Test m/M suffix (megabytes)
+        self.assertEqual(model.parse_token_value("1m"), 1 * 1024 * 1024)
+        self.assertEqual(model.parse_token_value("1M"), 1 * 1024 * 1024)
+        self.assertEqual(model.parse_token_value("0.5M"), 0.5 * 1024 * 1024)
+
+        # Test with spaces
+        self.assertEqual(model.parse_token_value(" 8k "), 8 * 1024)
+
+        # Test conversion from other types
+        self.assertEqual(model.parse_token_value(8.0), 8)
+
+    def test_set_thinking_tokens(self):
+        # Test that set_thinking_tokens correctly sets the tokens with different formats
+        model = Model("gpt-4")
+
+        # Test with integer
+        model.set_thinking_tokens(8096)
+        self.assertEqual(model.extra_params["thinking"]["budget_tokens"], 8096)
+        self.assertFalse(model.use_temperature)
+
+        # Test with string
+        model.set_thinking_tokens("10k")
+        self.assertEqual(model.extra_params["thinking"]["budget_tokens"], 10 * 1024)
+
+        # Test with decimal value
+        model.set_thinking_tokens("0.5M")
+        self.assertEqual(model.extra_params["thinking"]["budget_tokens"], 0.5 * 1024 * 1024)
+
 
 if __name__ == "__main__":
     unittest.main()
\ No newline at end of file
