
index dbe4ed68c..529083962 100644
--- a/aider_tests_basic_test_models.py_expectedoutput.txt (expected):tmp/tmp9r9dpxfj_expected.txt	
+++ b/aider_tests_basic_test_models.py_extracted.txt (actual):tmp/tmplmmlnhqd_actual.txt	
@@ -49,40 +49,6 @@ class TestModels(unittest.TestCase):
         model = Model("gpt-4-0613")
         self.assertEqual(model.info["max_input_tokens"], 8 * 1024)
 
-    @patch("os.environ")
-    def test_sanity_check_model_all_set(self, mock_environ):
-        mock_environ.get.return_value = "dummy_value"
-        mock_io = MagicMock()
-        model = MagicMock()
-        model.name = "test-model"
-        model.missing_keys = ["API_KEY1", "API_KEY2"]
-        model.keys_in_environment = True
-        model.info = {"some": "info"}
-
-        sanity_check_model(mock_io, model)
-
-        mock_io.tool_output.assert_called()
-        calls = mock_io.tool_output.call_args_list
-        self.assertIn("- API_KEY1: Set", str(calls))
-        self.assertIn("- API_KEY2: Set", str(calls))
-
-    @patch("os.environ")
-    def test_sanity_check_model_not_set(self, mock_environ):
-        mock_environ.get.return_value = ""
-        mock_io = MagicMock()
-        model = MagicMock()
-        model.name = "test-model"
-        model.missing_keys = ["API_KEY1", "API_KEY2"]
-        model.keys_in_environment = True
-        model.info = {"some": "info"}
-
-        sanity_check_model(mock_io, model)
-
-        mock_io.tool_output.assert_called()
-        calls = mock_io.tool_output.call_args_list
-        self.assertIn("- API_KEY1: Not set", str(calls))
-        self.assertIn("- API_KEY2: Not set", str(calls))
-
     def test_sanity_check_models_bogus_editor(self):
         mock_io = MagicMock()
         main_model = Model("gpt-4")
@@ -105,61 +71,6 @@ class TestModels(unittest.TestCase):
             any("bogus-model" in msg for msg in warning_messages)
         )  # Check that one of the warnings mentions the bogus model
 
-    @patch("aider.models.check_for_dependencies")
-    def test_sanity_check_model_calls_check_dependencies(self, mock_check_deps):
-        """Test that sanity_check_model calls check_for_dependencies"""
-        mock_io = MagicMock()
-        model = MagicMock()
-        model.name = "test-model"
-        model.missing_keys = []
-        model.keys_in_environment = True
-        model.info = {"some": "info"}
-
-        sanity_check_model(mock_io, model)
-
-        # Verify check_for_dependencies was called with the model name
-        mock_check_deps.assert_called_once_with(mock_io, "test-model")
-
-    def test_model_aliases(self):
-        # Test common aliases
-        model = Model("4")
-        self.assertEqual(model.name, "gpt-4-0613")
-
-        model = Model("4o")
-        self.assertEqual(model.name, "gpt-4o")
-
-        model = Model("35turbo")
-        self.assertEqual(model.name, "gpt-3.5-turbo")
-
-        model = Model("35-turbo")
-        self.assertEqual(model.name, "gpt-3.5-turbo")
-
-        model = Model("3")
-        self.assertEqual(model.name, "gpt-3.5-turbo")
-
-        model = Model("sonnet")
-        self.assertEqual(model.name, "anthropic/claude-3-7-sonnet-20250219")
-
-        model = Model("haiku")
-        self.assertEqual(model.name, "claude-3-5-haiku-20241022")
-
-        model = Model("opus")
-        self.assertEqual(model.name, "claude-3-opus-20240229")
-
-        # Test non-alias passes through unchanged
-        model = Model("gpt-4")
-        self.assertEqual(model.name, "gpt-4")
-
-    def test_o1_use_temp_false(self):
-        # Test GitHub Copilot models
-        model = Model("github/aider_tests_basic_test_models.py_extracted.txt (actual):
         # Create a model instance to test the parse_token_value method
         model = Model("gpt-4")
@@ -257,6 +168,57 @@ class TestModels(unittest.TestCase):
         # Verify check_pip_install_extra was not called
         mock_check_pip.assert_not_called()
 
+    @patch("aider.models.check_for_dependencies")
+    def test_sanity_check_model_calls_check_dependencies(self, mock_check_deps):
+        """Test that sanity_check_model calls check_for_dependencies"""
+        mock_io = MagicMock()
+        model = MagicMock()
+        model.name = "test-model"
+        model.missing_keys = []
+        model.keys_in_environment = True
+        model.info = {"some": "info"}
+
+        sanity_check_model(mock_io, model)
+
+        # Verify check_for_dependencies was called with the model name
+        mock_check_deps.assert_called_once_with(mock_io, "test-model")
+
+    def test_model_aliases(self):
+        # Test common aliases
+        model = Model("4")
+        self.assertEqual(model.name, "gpt-4-0613")
+
+        model = Model("4o")
+        self.assertEqual(model.name, "gpt-4o")
+
+        model = Model("35turbo")
+        self.assertEqual(model.name, "gpt-3.5-turbo")
+
+        model = Model("35-turbo")
+        self.assertEqual(model.name, "gpt-3.5-turbo")
+
+        model = Model("3")
+        self.assertEqual(model.name, "gpt-3.5-turbo")
+
+        model = Model("sonnet")
+        self.assertEqual(model.name, "anthropic/claude-3-7-sonnet-20250219")
+
+        model = Model("haiku")
+        self.assertEqual(model.name, "claude-3-5-haiku-20241022")
+
+        model = Model("opus")
+        self.assertEqual(model.name, "claude-3-opus-20240229")
+
+    def test_o1_use_temp_false(self):
+        # Test GitHub Copilot models
+        model = Model("github/aider_tests_basic_test_models.py_extracted.txt (actual):
         # Test default case (no max_input_tokens in info)
         model = Model("gpt-4")
@@ -374,57 +336,6 @@ class TestModels(unittest.TestCase):
         self.assertEqual(model.editor_edit_format, "editor-diff")
         self.assertTrue(model.use_repo_map)
 
-    def test_aider_extra_model_settings(self):
-        import tempfile
-
-        import yaml
-
-        # Create temporary YAML file with test settings
-        test_settings = [
-            {
-                "name": "aider/extra_params",
-                "extra_params": {
-                    "extra_headers": {"Foo": "bar"},
-                    "some_param": "some value",
-                },
-            },
-        ]
-
-        # Write to a regular file instead of NamedTemporaryFile
-        # for better cross-platform compatibility
-        tmp = tempfile.mktemp(suffix=".yml")
-        try:
-            with open(tmp, "w") as f:
-                yaml.dump(test_settings, f)
-
-            # Register the test settings
-            register_models([tmp])
-
-            # Test that defaults are applied when no exact match
-            model = Model("claude-3-5-sonnet-20240620")
-            # Test that both the override and existing headers are present
-            model = Model("claude-3-5-sonnet-20240620")
-            self.assertEqual(model.extra_params["extra_headers"]["Foo"], "bar")
-            self.assertEqual(
-                model.extra_params["extra_headers"]["anthropic-beta"],
-                ANTHROPIC_BETA_HEADER,
-            )
-            self.assertEqual(model.extra_params["some_param"], "some value")
-            self.assertEqual(model.extra_params["max_tokens"], 8192)
-
-            # Test that exact match overrides defaults but not overrides
-            model = Model("gpt-4")
-            self.assertEqual(model.extra_params["extra_headers"]["Foo"], "bar")
-            self.assertEqual(model.extra_params["some_param"], "some value")
-        finally:
-            # Clean up the temporary file
-            import os
-
-            try:
-                os.unlink(tmp)
-            except OSError:
-                pass
-
     @patch("aider.models.litellm.completion")
     @patch.object(Model, "token_count")
     def test_ollama_num_ctx_set_when_missing(self, mock_token_count, mock_completion):
@@ -481,6 +392,57 @@ class TestModels(unittest.TestCase):
         )
         self.assertNotIn("num_ctx", mock_completion.call_args.kwargs)
 
+    def test_aider_extra_model_settings(self):
+        import tempfile
+
+        import yaml
+
+        # Create temporary YAML file with test settings
+        test_settings = [
+            {
+                "name": "aider/extra_params",
+                "extra_params": {
+                    "extra_headers": {"Foo": "bar"},
+                    "some_param": "some value",
+                },
+            },
+        ]
+
+        # Write to a regular file instead of NamedTemporaryFile
+        # for better cross-platform compatibility
+        tmp = tempfile.mktemp(suffix=".yml")
+        try:
+            with open(tmp, "w") as f:
+                yaml.dump(test_settings, f)
+
+            # Register the test settings
+            register_models([tmp])
+
+            # Test that defaults are applied when no exact match
+            model = Model("claude-3-5-sonnet-20240620")
+            # Test that both the override and existing headers are present
+            model = Model("claude-3-5-sonnet-20240620")
+            self.assertEqual(model.extra_params["extra_headers"]["Foo"], "bar")
+            self.assertEqual(
+                model.extra_params["extra_headers"]["anthropic-beta"],
+                ANTHROPIC_BETA_HEADER,
+            )
+            self.assertEqual(model.extra_params["some_param"], "some value")
+            self.assertEqual(model.extra_params["max_tokens"], 8192)
+
+            # Test that exact match overrides defaults but not overrides
+            model = Model("gpt-4")
+            self.assertEqual(model.extra_params["extra_headers"]["Foo"], "bar")
+            self.assertEqual(model.extra_params["some_param"], "some value")
+        finally:
+            # Clean up the temporary file
+            import os
+
+            try:
+                os.unlink(tmp)
+            except OSError:
+                pass
+
     def test_use_temperature_settings(self):
         # Test use_temperature=True (default) uses temperature=0
         model = Model("gpt-4")
@@ -497,8 +459,8 @@ class TestModels(unittest.TestCase):
         self.assertEqual(model.use_temperature, 0.7)
 
     @patch("aider.models.litellm.completion")
-    def test_request_timeout_default(self, mock_completion):
-        # Test default timeout is used when not specified in extra_params
+    def test_use_temperature_in_send_completion(self, mock_completion):
+        # Test use_temperature=True sends temperature=0
         model = Model("gpt-4")
         messages = [{"role": "user", "content": "Hello"}]
         model.send_completion(messages, functions=None, stream=False)
@@ -507,27 +469,31 @@ class TestModels(unittest.TestCase):
             messages=messages,
             stream=False,
             temperature=0,
-            timeout=600,  # Default timeout
+            timeout=600,
         )
 
-    @patch("aider.models.litellm.completion")
-    def test_request_timeout_from_extra_params(self, mock_completion):
-        # Test timeout from extra_params overrides default
+        # Test use_temperature=False doesn't send temperature
+        model = Model("github/aider_tests_basic_test_models.py_extracted.txt (actual): "user", "content": "Hello"}]
+        model.send_completion(messages, functions=None, stream=False)
+        self.assertNotIn("temperature", mock_completion.call_args.kwargs)
+
+        # Test use_temperature as float sends that value
         model = Model("gpt-4")
-        model.extra_params = {"timeout": 300}  # 5 minutes
+        model.use_temperature = 0.7
         messages = [{"role": "user", "content": "Hello"}]
         model.send_completion(messages, functions=None, stream=False)
         mock_completion.assert_called_with(
             model=model.name,
             messages=messages,
             stream=False,
-            temperature=0,
-            timeout=300,  # From extra_params
+            temperature=0.7,
+            timeout=600,
         )
 
     @patch("aider.models.litellm.completion")
-    def test_use_temperature_in_send_completion(self, mock_completion):
-        # Test use_temperature=True sends temperature=0
+    def test_request_timeout_default(self, mock_completion):
+        # Test default timeout is used when not specified in extra_params
         model = Model("gpt-4")
         messages = [{"role": "user", "content": "Hello"}]
         model.send_completion(messages, functions=None, stream=False)
@@ -536,26 +502,22 @@ class TestModels(unittest.TestCase):
             messages=messages,
             stream=False,
             temperature=0,
-            timeout=600,
+            timeout=600,  # Default timeout
         )
 
-        # Test use_temperature=False doesn't send temperature
-        model = Model("github/aider_tests_basic_test_models.py_extracted.txt (actual): "user", "content": "Hello"}]
-        model.send_completion(messages, functions=None, stream=False)
-        self.assertNotIn("temperature", mock_completion.call_args.kwargs)
-
-        # Test use_temperature as float sends that value
+    @patch("aider.models.litellm.completion")
+    def test_request_timeout_from_extra_params(self, mock_completion):
+        # Test timeout from extra_params overrides default
         model = Model("gpt-4")
-        model.use_temperature = 0.7
+        model.extra_params = {"timeout": 300}  # 5 minutes
         messages = [{"role": "user", "content": "Hello"}]
         model.send_completion(messages, functions=None, stream=False)
         mock_completion.assert_called_with(
             model=model.name,
             messages=messages,
             stream=False,
-            temperature=0.7,
-            timeout=600,
+            temperature=0,
+            timeout=300,  # From extra_params
         )
 
 
