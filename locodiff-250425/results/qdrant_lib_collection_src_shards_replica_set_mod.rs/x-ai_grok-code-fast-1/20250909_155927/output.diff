
index 1a8af4043..3ae6a62fc 100644
--- a/qdrant_lib_collection_src_shards_replica_set_mod.rs_expectedoutput.txt (expected):tmp/tmpt38e3pp__expected.txt	
+++ b/qdrant_lib_collection_src_shards_replica_set_mod.rs_extracted.txt (actual):tmp/tmpljdvu_sz_actual.txt	
@@ -1,4 +1,4 @@
-pub mod clock_set;
+mod clock_set;
 mod execute_read_operation;
 mod locally_disabled_peers;
 mod read_ops;
@@ -208,8 +208,8 @@ impl ShardReplicaSet {
             replica_state: replica_state.into(),
             locally_disabled_peers: Default::default(),
             shard_path,
-            abort_shard_transfer_cb: abort_shard_transfer,
             notify_peer_failure_cb: on_peer_failure,
+            abort_shard_transfer_cb: abort_shard_transfer,
             channel_service,
             collection_id,
             collection_config,
@@ -406,12 +406,6 @@ impl ShardReplicaSet {
         self.replica_state.read().peers()
     }
 
-    pub fn is_last_active_replica(&self, peer_id: PeerId) -> bool {
-        // This includes `Active` and `ReshardingScaleDown` replicas!
-        let active_peers = self.replica_state.read().active_peers();
-        active_peers.len() == 1 && active_peers.contains(&peer_id)
-    }
-
     pub fn peer_state(&self, peer_id: PeerId) -> Option<ReplicaState> {
         self.replica_state.read().get_peer_state(peer_id)
     }
@@ -420,7 +414,6 @@ impl ShardReplicaSet {
     pub fn active_shards(&self) -> Vec<PeerId> {
         let replica_state = self.replica_state.read();
         replica_state
-            // This is a part of deprecated built-in resharding implementation, so we don't care
             .active_peers()
             .into_iter()
             .filter(|&peer_id| !self.is_locally_disabled(peer_id))
@@ -432,12 +425,18 @@ impl ShardReplicaSet {
         let replica_state = self.replica_state.read();
         let this_peer_id = replica_state.this_peer_id;
         replica_state
-            .active_peers() // This includes `Active` and `ReshardingScaleDown` replicas!
+            .active_peers()
             .into_iter()
             .filter(|&peer_id| !self.is_locally_disabled(peer_id) && peer_id != this_peer_id)
             .collect()
     }
 
+    pub fn is_last_active_replica(&self, peer_id: PeerId) -> bool {
+        // This includes `Active` and `ReshardingScaleDown` replicas!
+        let active_peers = self.replica_state.read().active_peers();
+        active_peers.len() == 1 && active_peers.contains(&peer_id)
+    }
+
     /// Wait for a local shard to be initialized.
     ///
     /// Uses a blocking thread internally.
@@ -446,31 +445,6 @@ impl ShardReplicaSet {
             .await
     }
 
-    pub fn wait_for_state_condition_sync<F>(&self, check: F, timeout: Duration) -> bool
-    where
-        F: Fn(&ReplicaSetState) -> bool,
-    {
-        let replica_state = self.replica_state.clone();
-        replica_state.wait_for(check, timeout)
-    }
-
-    /// Wait for a local shard to get into `state`
-    ///
-    /// Uses a blocking thread internally.
-    pub async fn wait_for_local_state(
-        &self,
-        state: ReplicaState,
-        timeout: Duration,
-    ) -> CollectionResult<()> {
-        self.wait_for(
-            move |replica_set_state| {
-                replica_set_state.get_peer_state(replica_set_state.this_peer_id) == Some(state)
-            },
-            timeout,
-        )
-        .await
-    }
-
     /// Wait for a peer shard to get into `state`
     ///
     /// Uses a blocking thread internally.
@@ -491,6 +465,14 @@ impl ShardReplicaSet {
         .await
     }
 
+    pub fn wait_for_state_condition_sync<F>(&self, check: F, timeout: Duration) -> bool
+    where
+        F: Fn(&ReplicaSetState) -> bool,
+    {
+        let replica_state = self.replica_state.clone();
+        replica_state.wait_for(check, timeout)
+    }
+
     /// Wait for a replica set state condition to be true.
     ///
     /// Uses a blocking thread internally.
@@ -575,6 +557,7 @@ impl ShardReplicaSet {
                 }
             })?;
         }
+
         self.update_locally_disabled(self.this_peer_id());
         Ok(old_shard)
     }
@@ -730,7 +713,6 @@ impl ShardReplicaSet {
                     self.optimizers_config.clone(),
                 )
                 .await?;
-
                 match state {
                     ReplicaState::Active
                     | ReplicaState::Listener
@@ -750,7 +732,6 @@ impl ShardReplicaSet {
                         self.set_local(local_shard, Some(state)).await?;
                     }
                 }
-
                 continue;
             }
 
@@ -833,49 +814,6 @@ impl ShardReplicaSet {
         Ok(())
     }
 
-    /// Check if there are any locally disabled peers
-    /// And if so, report them to the consensus
-    pub fn sync_local_state<F>(&self, get_shard_transfers: F) -> CollectionResult<()>
-    where
-        F: Fn(ShardId, PeerId) -> Vec<ShardTransfer>,
-    {
-        let peers_to_notify: Vec<_> = self
-            .locally_disabled_peers
-            .write()
-            .notify_elapsed()
-            .collect();
-
-        for (failed_peer_id, from_state) in peers_to_notify {
-            self.notify_peer_failure(failed_peer_id, from_state);
-
-            for transfer in get_shard_transfers(self.shard_id, failed_peer_id) {
-                self.abort_shard_transfer(
-                    transfer,
-                    &format!(
-                        "{failed_peer_id}/{}:{} replica failed",
-                        self.collection_id, self.shard_id,
-                    ),
-                );
-            }
-        }
-
-        Ok(())
-    }
-
-    pub(crate) async fn health_check(&self, peer_id: PeerId) -> CollectionResult<()> {
-        let remotes = self.remotes.read().await;
-
-        let Some(remote) = remotes.iter().find(|remote| remote.peer_id == peer_id) else {
-            return Err(CollectionError::NotFound {
-                what: format!("{}/{}:{} shard", peer_id, self.collection_id, self.shard_id),
-            });
-        };
-
-        remote.health_check().await?;
-
-        Ok(())
-    }
-
     pub async fn delete_local_points(
         &self,
         filter: Filter,
@@ -930,12 +868,11 @@ impl ShardReplicaSet {
 
         drop(local_shard_guard);
 
-        let op =
-            CollectionUpdateOperations::PointOperation(point_ops::PointOperations::DeletePoints {
-                ids,
-            });
+        let op = CollectionUpdateOperations::PointOperation(point_ops::PointOperations::DeletePoints {
+            ids,
+        });
 
-        // TODO(resharding): Assign clock tag to the operation!? ðŸ¤”
+        // TODO(resharding): Assign clock tag to the operation!?
         let result = self
             .update_local(op.into(), true, hw_measurement_acc, force)
             .await?
@@ -949,6 +886,20 @@ impl ShardReplicaSet {
         Ok(result)
     }
 
+    pub(crate) async fn health_check(&self, peer_id: PeerId) -> CollectionResult<()> {
+        let remotes = self.remotes.read().await;
+
+        let Some(remote) = remotes.iter().find(|remote| remote.peer_id == peer_id) else {
+            return Err(CollectionError::NotFound {
+                what: format!("{}/{}:{} shard", peer_id, self.collection_id, self.shard_id),
+            });
+        };
+
+        remote.health_check().await?;
+
+        Ok(())
+    }
+
     fn init_remote_shards(
         shard_id: ShardId,
         collection_id: CollectionId,
@@ -1005,9 +956,6 @@ impl ShardReplicaSet {
     /// Disables the peer and notifies consensus periodically.
     ///
     /// Prevents disabling the last peer (according to consensus).
-    ///
-    /// If `from_state` is given, the peer will only be disabled if the given state matches
-    /// consensus.
     fn add_locally_disabled(
         &self,
         state: &ReplicaSetState,
@@ -1215,7 +1163,6 @@ pub enum ReplicaState {
     // A shard which receives data, but is not used for search
     // Useful for backup shards
     Listener,
-    // Deprecated since Qdrant 1.9.0, used in Qdrant 1.7.0 and 1.8.0
     //
     // Snapshot shard transfer is in progress, updates aren't sent to the shard
     // Normally rejects updates. Since 1.8 it allows updates if force is true.
@@ -1265,8 +1212,6 @@ impl ReplicaState {
     /// Check whether the replica state is partial or partial-like.
     ///
     /// In other words: is the state related to shard transfers?
-    //
-    // TODO(resharding): What's the best way to handle `ReshardingScaleDown` properly!?
     pub fn is_partial_or_recovery(self) -> bool {
         match self {
             ReplicaState::Partial
