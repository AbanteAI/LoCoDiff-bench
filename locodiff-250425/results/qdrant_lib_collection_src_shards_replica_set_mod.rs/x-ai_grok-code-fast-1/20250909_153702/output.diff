
index 1a8af4043..46680ed49 100644
--- a/qdrant_lib_collection_src_shards_replica_set_mod.rs_expectedoutput.txt (expected):tmp/tmpdgvx9bgv_expected.txt	
+++ b/qdrant_lib_collection_src_shards_replica_set_mod.rs_extracted.txt (actual):tmp/tmp9p3mks3s_actual.txt	
@@ -1,4 +1,4 @@
-pub mod clock_set;
+mod clock_set;
 mod execute_read_operation;
 mod locally_disabled_peers;
 mod read_ops;
@@ -80,19 +80,11 @@ use crate::shards::shard_config::ShardConfig;
 //
 
 /// A set of shard replicas.
-///
-/// Handles operations so that the state is consistent across all the replicas of the shard.
+/// Handles operations so that the state is consistent across all the shards of the shard.
 /// Prefers local shard for read-only operations.
 /// Perform updates on all replicas and report error if there is at least one failure.
 ///
 pub struct ShardReplicaSet {
-    local: RwLock<Option<Shard>>, // Abstract Shard to be able to use a Proxy during replication
-    remotes: RwLock<Vec<RemoteShard>>,
-    replica_state: Arc<SaveOnDisk<ReplicaSetState>>,
-    /// List of peers that are marked as dead locally, but are not yet submitted to the consensus.
-    /// List is checked on each consensus round and submitted to the consensus.
-    /// If the state of the peer is changed in the consensus, it is removed from the list.
-    /// Update and read operations are not performed on the peers marked as dead.
     locally_disabled_peers: parking_lot::RwLock<locally_disabled_peers::Registry>,
     pub(crate) shard_path: PathBuf,
     pub(crate) shard_id: ShardId,
@@ -185,11 +177,6 @@ impl ShardReplicaSet {
             &channel_service,
         );
 
-        // Save shard config as the last step, to ensure that the file state is consistent
-        // Presence of shard config indicates that the shard is ready to be used
-        let replica_set_shard_config = ShardConfig::new_replica_set();
-        replica_set_shard_config.save(&shard_path)?;
-
         // Initialize the write rate limiter
         let config = collection_config.read().await;
         let write_rate_limiter = config.strict_mode_config.as_ref().and_then(|strict_mode| {
@@ -364,7 +351,6 @@ impl ShardReplicaSet {
             write_rate_limiter,
         };
 
-        // `active_remote_shards` includes `Active` and `ReshardingScaleDown` replicas!
         if local_load_failure && replica_set.active_remote_shards().is_empty() {
             replica_set
                 .locally_disabled_peers
@@ -420,7 +406,6 @@ impl ShardReplicaSet {
     pub fn active_shards(&self) -> Vec<PeerId> {
         let replica_state = self.replica_state.read();
         replica_state
-            // This is a part of deprecated built-in resharding implementation, so we don't care
             .active_peers()
             .into_iter()
             .filter(|&peer_id| !self.is_locally_disabled(peer_id))
@@ -446,38 +431,9 @@ impl ShardReplicaSet {
             .await
     }
 
-    pub fn wait_for_state_condition_sync<F>(&self, check: F, timeout: Duration) -> bool
-    where
-        F: Fn(&ReplicaSetState) -> bool,
-    {
-        let replica_state = self.replica_state.clone();
-        replica_state.wait_for(check, timeout)
-    }
-
-    /// Wait for a local shard to get into `state`
-    ///
-    /// Uses a blocking thread internally.
-    pub async fn wait_for_local_state(
-        &self,
-        state: ReplicaState,
-        timeout: Duration,
-    ) -> CollectionResult<()> {
-        self.wait_for(
-            move |replica_set_state| {
-                replica_set_state.get_peer_state(replica_set_state.this_peer_id) == Some(state)
-            },
-            timeout,
-        )
-        .await
-    }
-
     /// Wait for a peer shard to get into `state`
     ///
     /// Uses a blocking thread internally.
-    ///
-    /// # Cancel safety
-    ///
-    /// This method is cancel safe.
     pub async fn wait_for_state(
         &self,
         peer_id: PeerId,
@@ -491,6 +447,14 @@ impl ShardReplicaSet {
         .await
     }
 
+    pub fn wait_for_state_condition_sync<F>(&self, check: F, timeout: Duration) -> bool
+    where
+        F: Fn(&ReplicaSetState) -> bool,
+    {
+        let replica_state = self.replica_state.clone();
+        replica_state.wait_for(check, timeout)
+    }
+
     /// Wait for a replica set state condition to be true.
     ///
     /// Uses a blocking thread internally.
@@ -580,8 +544,6 @@ impl ShardReplicaSet {
     }
 
     pub async fn remove_local(&self) -> CollectionResult<()> {
-        // TODO: Ensure cancel safety!
-
         self.replica_state.write(|rs| {
             rs.is_local = false;
             let this_peer_id = rs.this_peer_id;
@@ -641,7 +603,6 @@ impl ShardReplicaSet {
         Ok(())
     }
 
-    /// Change state of the replica to the given.
     /// Ensure that remote shard is initialized.
     pub async fn ensure_replica_with_state(
         &self,
@@ -675,15 +636,6 @@ impl ShardReplicaSet {
         Ok(())
     }
 
-    pub async fn remove_peer(&self, peer_id: PeerId) -> CollectionResult<()> {
-        if self.this_peer_id() == peer_id {
-            self.remove_local().await?;
-        } else {
-            self.remove_remote(peer_id).await?;
-        }
-        Ok(())
-    }
-
     pub async fn apply_state(
         &mut self,
         replicas: HashMap<PeerId, ReplicaState>,
@@ -730,7 +682,6 @@ impl ShardReplicaSet {
                     self.optimizers_config.clone(),
                 )
                 .await?;
-
                 match state {
                     ReplicaState::Active
                     | ReplicaState::Listener
@@ -750,7 +701,6 @@ impl ShardReplicaSet {
                         self.set_local(local_shard, Some(state)).await?;
                     }
                 }
-
                 continue;
             }
 
@@ -867,7 +817,7 @@ impl ShardReplicaSet {
 
         let Some(remote) = remotes.iter().find(|remote| remote.peer_id == peer_id) else {
             return Err(CollectionError::NotFound {
-                what: format!("{}/{}:{} shard", peer_id, self.collection_id, self.shard_id),
+                what: format!("{peer_id}/{}:{} shard", self.collection_id, self.shard_id),
             });
         };
 
@@ -1005,7 +955,6 @@ impl ShardReplicaSet {
     /// Disables the peer and notifies consensus periodically.
     ///
     /// Prevents disabling the last peer (according to consensus).
-    ///
     /// If `from_state` is given, the peer will only be disabled if the given state matches
     /// consensus.
     fn add_locally_disabled(
@@ -1096,210 +1045,4 @@ impl ShardReplicaSet {
         local_shard.update_cutoff(cutoff).await
     }
 
-    pub(crate) fn get_snapshots_storage_manager(&self) -> CollectionResult<SnapshotStorageManager> {
-        SnapshotStorageManager::new(&self.shared_storage_config.snapshots_config)
-    }
-
-    pub(crate) async fn trigger_optimizers(&self) -> bool {
-        let shard = self.local.read().await;
-        let Some(shard) = shard.as_ref() else {
-            return false;
-        };
-        shard.trigger_optimizers();
-        true
-    }
-
-    /// Returns the estimated size of all local segments.
-    /// Since this locks all segments you should cache this value in performance critical scenarios!
-    pub(crate) async fn calculate_local_shard_stats(&self) -> Option<CollectionSizeStats> {
-        self.local
-            .read()
-            .await
-            .as_ref()
-            .map(|i| match i {
-                Shard::Local(local) => {
-                    let mut total_vector_size = 0;
-                    let mut total_payload_size = 0;
-                    let mut total_points = 0;
-
-                    for segment in local.segments.read().iter() {
-                        let size_info = segment.1.get().read().size_info();
-                        total_vector_size += size_info.vectors_size_bytes;
-                        total_payload_size += size_info.payloads_size_bytes;
-                        total_points += size_info.num_points;
-                    }
-
-                    Some(CollectionSizeStats {
-                        vector_storage_size: total_vector_size,
-                        payload_storage_size: total_payload_size,
-                        points_count: total_points,
-                    })
-                }
-                Shard::Proxy(_)
-                | Shard::ForwardProxy(_)
-                | Shard::QueueProxy(_)
-                | Shard::Dummy(_) => None,
-            })
-            .unwrap_or_default()
-    }
-}
-
-/// Represents a replica set state
-#[derive(Debug, Deserialize, Serialize, Default, PartialEq, Eq, Clone)]
-pub struct ReplicaSetState {
-    pub is_local: bool,
-    pub this_peer_id: PeerId,
-    peers: HashMap<PeerId, ReplicaState>,
-}
-
-impl ReplicaSetState {
-    pub fn get_peer_state(&self, peer_id: PeerId) -> Option<ReplicaState> {
-        self.peers.get(&peer_id).copied()
-    }
-
-    pub fn set_peer_state(&mut self, peer_id: PeerId, state: ReplicaState) {
-        self.peers.insert(peer_id, state);
-    }
-
-    pub fn remove_peer_state(&mut self, peer_id: PeerId) -> Option<ReplicaState> {
-        self.peers.remove(&peer_id)
-    }
-
-    pub fn peers(&self) -> HashMap<PeerId, ReplicaState> {
-        self.peers.clone()
-    }
-
-    pub fn active_peers(&self) -> Vec<PeerId> {
-        self.peers
-            .iter()
-            .filter_map(|(peer_id, state)| {
-                // We consider `ReshardingScaleDown` to be `Active`!
-                matches!(
-                    state,
-                    ReplicaState::Active | ReplicaState::ReshardingScaleDown
-                )
-                .then_some(*peer_id)
-            })
-            .collect()
-    }
-
-    pub fn active_or_resharding_peers(&self) -> impl Iterator<Item = PeerId> + '_ {
-        self.peers.iter().filter_map(|(peer_id, state)| {
-            matches!(
-                state,
-                ReplicaState::Active | ReplicaState::Resharding | ReplicaState::ReshardingScaleDown
-            )
-            .then_some(*peer_id)
-        })
-    }
-
-    pub fn set_peers(&mut self, peers: HashMap<PeerId, ReplicaState>) {
-        self.peers = peers;
-    }
-}
-
-/// State of the single shard within a replica set.
-#[derive(
-    Debug, Deserialize, Serialize, JsonSchema, Default, PartialEq, Eq, Hash, Clone, Copy, Anonymize,
-)]
-pub enum ReplicaState {
-    // Active and sound
-    #[default]
-    Active,
-    // Failed for some reason
-    Dead,
-    // The shard is partially loaded and is currently receiving data from other shards
-    Partial,
-    // Collection is being created
-    Initializing,
-    // A shard which receives data, but is not used for search
-    // Useful for backup shards
-    Listener,
-    // Deprecated since Qdrant 1.9.0, used in Qdrant 1.7.0 and 1.8.0
-    //
-    // Snapshot shard transfer is in progress, updates aren't sent to the shard
-    // Normally rejects updates. Since 1.8 it allows updates if force is true.
-    PartialSnapshot,
-    // Shard is undergoing recovery by an external node
-    // Normally rejects updates, accepts updates if force is true
-    Recovery,
-    // Points are being migrated to this shard as part of resharding up
-    Resharding,
-    // Points are being migrated to this shard as part of resharding down
-    ReshardingScaleDown,
-}
-
-impl ReplicaState {
-    /// Check if replica state is active
-    pub fn is_active(self) -> bool {
-        match self {
-            ReplicaState::Active => true,
-            ReplicaState::ReshardingScaleDown => true,
-
-            ReplicaState::Dead => false,
-            ReplicaState::Partial => false,
-            ReplicaState::Initializing => false,
-            ReplicaState::Listener => false,
-            ReplicaState::PartialSnapshot => false,
-            ReplicaState::Recovery => false,
-            ReplicaState::Resharding => false,
-        }
-    }
-
-    /// Check whether the replica state is active or listener or resharding.
-    pub fn is_active_or_listener_or_resharding(self) -> bool {
-        match self {
-            ReplicaState::Active
-            | ReplicaState::Listener
-            | ReplicaState::Resharding
-            | ReplicaState::ReshardingScaleDown => true,
-
-            ReplicaState::Dead
-            | ReplicaState::Initializing
-            | ReplicaState::Partial
-            | ReplicaState::PartialSnapshot
-            | ReplicaState::Recovery => false,
-        }
-    }
-
-    /// Check whether the replica state is partial or partial-like.
-    ///
-    /// In other words: is the state related to shard transfers?
-    //
-    // TODO(resharding): What's the best way to handle `ReshardingScaleDown` properly!?
-    pub fn is_partial_or_recovery(self) -> bool {
-        match self {
-            ReplicaState::Partial
-            | ReplicaState::PartialSnapshot
-            | ReplicaState::Recovery
-            | ReplicaState::Resharding
-            | ReplicaState::ReshardingScaleDown => true,
-
-            ReplicaState::Active
-            | ReplicaState::Dead
-            | ReplicaState::Initializing
-            | ReplicaState::Listener => false,
-        }
-    }
-
-    /// Returns `true` if the replica state is resharding, either up or down.
-    pub fn is_resharding(&self) -> bool {
-        match self {
-            ReplicaState::Resharding | ReplicaState::ReshardingScaleDown => true,
-
-            ReplicaState::Partial
-            | ReplicaState::PartialSnapshot
-            | ReplicaState::Recovery
-            | ReplicaState::Active
-            | ReplicaState::Dead
-            | ReplicaState::Initializing
-            | ReplicaState::Listener => false,
-        }
-    }
-}
-
-/// Represents a change in replica set, due to scaling of `replication_factor`
-#[derive(Debug, Deserialize, Serialize, PartialEq, Eq, Hash, Clone)]
-pub enum Change {
-    Remove(ShardId, PeerId),
-}
\ No newline at end of file
+    pub(crate) fn get_snapshots_storage_manager(&
\ No newline at end of file
