
index 1a8af404..1cedf4da 100644
--- a/qdrant_lib_collection_src_shards_replica_set_mod.rs_expectedoutput.txt (expected):tmp/tmpapvrn7mr_expected.txt	
+++ b/qdrant_lib_collection_src_shards_replica_set_mod.rs_extracted.txt (actual):tmp/tmpzafvymtf_actual.txt	
@@ -1,4 +1,4 @@
-pub mod clock_set;
+mod clock_set;
 mod execute_read_operation;
 mod locally_disabled_peers;
 mod read_ops;
@@ -84,7 +84,6 @@ use crate::shards::shard_config::ShardConfig;
 /// Handles operations so that the state is consistent across all the replicas of the shard.
 /// Prefers local shard for read-only operations.
 /// Perform updates on all replicas and report error if there is at least one failure.
-///
 pub struct ShardReplicaSet {
     local: RwLock<Option<Shard>>, // Abstract Shard to be able to use a Proxy during replication
     remotes: RwLock<Vec<RemoteShard>>,
@@ -116,7 +115,6 @@ pub struct ShardReplicaSet {
 }
 
 pub type AbortShardTransfer = Arc<dyn Fn(ShardTransfer, &str) + Send + Sync>;
-pub type ChangePeerState = Arc<dyn Fn(PeerId, ShardId) + Send + Sync>;
 pub type ChangePeerFromState = Arc<dyn Fn(PeerId, ShardId, Option<ReplicaState>) + Send + Sync>;
 
 const REPLICA_STATE_FILE: &str = "replica_state.json";
@@ -256,10 +254,7 @@ impl ShardReplicaSet {
             replica_state
                 .write(|rs| {
                     let this_peer_id = rs.this_peer_id;
-                    let local_state = rs.remove_peer_state(this_peer_id);
-                    if let Some(state) = local_state {
-                        rs.set_peer_state(this_peer_id, state);
-                    }
+                    rs.remove_peer_state(this_peer_id);
                     rs.this_peer_id = this_peer_id;
                 })
                 .map_err(|e| {
@@ -348,8 +343,8 @@ impl ShardReplicaSet {
             // TODO: move to collection config
             locally_disabled_peers: Default::default(),
             shard_path: shard_path.to_path_buf(),
-            notify_peer_failure_cb: on_peer_failure,
             abort_shard_transfer_cb: abort_shard_transfer,
+            notify_peer_failure_cb: on_peer_failure,
             channel_service,
             collection_id,
             collection_config,
@@ -387,25 +382,6 @@ impl ShardReplicaSet {
         self.local.read().await.is_some()
     }
 
-    pub async fn is_local(&self) -> bool {
-        let local_read = self.local.read().await;
-        matches!(*local_read, Some(Shard::Local(_) | Shard::Dummy(_)))
-    }
-
-    pub async fn is_queue_proxy(&self) -> bool {
-        let local_read = self.local.read().await;
-        matches!(*local_read, Some(Shard::QueueProxy(_)))
-    }
-
-    pub async fn is_dummy(&self) -> bool {
-        let local_read = self.local.read().await;
-        matches!(*local_read, Some(Shard::Dummy(_)))
-    }
-
-    pub fn peers(&self) -> HashMap<PeerId, ReplicaState> {
-        self.replica_state.read().peers()
-    }
-
     pub fn is_last_active_replica(&self, peer_id: PeerId) -> bool {
         // This includes `Active` and `ReshardingScaleDown` replicas!
         let active_peers = self.replica_state.read().active_peers();
@@ -446,14 +422,6 @@ impl ShardReplicaSet {
             .await
     }
 
-    pub fn wait_for_state_condition_sync<F>(&self, check: F, timeout: Duration) -> bool
-    where
-        F: Fn(&ReplicaSetState) -> bool,
-    {
-        let replica_state = self.replica_state.clone();
-        replica_state.wait_for(check, timeout)
-    }
-
     /// Wait for a local shard to get into `state`
     ///
     /// Uses a blocking thread internally.
@@ -491,38 +459,6 @@ impl ShardReplicaSet {
         .await
     }
 
-    /// Wait for a replica set state condition to be true.
-    ///
-    /// Uses a blocking thread internally.
-    ///
-    /// # Cancel safety
-    ///
-    /// This method is cancel safe.
-    async fn wait_for<F>(&self, check: F, timeout: Duration) -> CollectionResult<()>
-    where
-        F: Fn(&ReplicaSetState) -> bool + Send + 'static,
-    {
-        // TODO: Propagate cancellation into `spawn_blocking` task!?
-
-        let replica_state = self.replica_state.clone();
-        let timed_out =
-            !tokio::task::spawn_blocking(move || replica_state.wait_for(check, timeout))
-                .await
-                .map_err(|err| {
-                    CollectionError::service_error(format!(
-                        "Failed to wait for replica set state: {err}"
-                    ))
-                })?;
-
-        if timed_out {
-            return Err(CollectionError::service_error(
-                "Failed to wait for replica set state, timed out",
-            ));
-        }
-
-        Ok(())
-    }
-
     /// Clears the local shard data and loads an empty local shard
     pub async fn init_empty_local_shard(&self) -> CollectionResult<()> {
         let mut local = self.local.write().await;
@@ -730,11 +666,8 @@ impl ShardReplicaSet {
                     self.optimizers_config.clone(),
                 )
                 .await?;
-
                 match state {
-                    ReplicaState::Active
-                    | ReplicaState::Listener
-                    | ReplicaState::ReshardingScaleDown => {
+                    ReplicaState::Active | ReplicaState::Listener | ReplicaState::ReshardingScaleDown => {
                         // No way we can provide up-to-date replica right away at this point,
                         // so we report a failure to consensus
                         self.set_local(local_shard, Some(state)).await?;
@@ -750,7 +683,6 @@ impl ShardReplicaSet {
                         self.set_local(local_shard, Some(state)).await?;
                     }
                 }
-
                 continue;
             }
 
@@ -806,33 +738,6 @@ impl ShardReplicaSet {
         Ok(())
     }
 
-    /// Check if the write rate limiter allows the operation to proceed
-    /// - hw_measurement_acc: the current hardware measurement accumulator
-    /// - cost_fn: the cost of the operation called lazily
-    ///
-    /// Returns an error if the rate limit is exceeded.
-    fn check_write_rate_limiter<F>(
-        &self,
-        hw_measurement_acc: &HwMeasurementAcc,
-        cost_fn: F,
-    ) -> CollectionResult<()>
-    where
-        F: FnOnce() -> usize,
-    {
-        // Do not rate limit internal operation tagged with disposable measurement
-        if hw_measurement_acc.is_disposable() {
-            return Ok(());
-        }
-        if let Some(rate_limiter) = &self.write_rate_limiter {
-            let cost = cost_fn();
-            rate_limiter
-                .lock()
-                .try_consume(cost as f64)
-                .map_err(|err| CollectionError::rate_limit_error(err, cost, true))?;
-        }
-        Ok(())
-    }
-
     /// Check if there are any locally disabled peers
     /// And if so, report them to the consensus
     pub fn sync_local_state<F>(&self, get_shard_transfers: F) -> CollectionResult<()>
@@ -845,15 +750,16 @@ impl ShardReplicaSet {
             .notify_elapsed()
             .collect();
 
-        for (failed_peer_id, from_state) in peers_to_notify {
-            self.notify_peer_failure(failed_peer_id, from_state);
+        for (failed_peer, from_state) in peers_to_notify {
+            // TODO: Only `notify_peer_failure` if `failed_peer` is *not* the last `Active` peer? ðŸ¤”
+            self.notify_peer_failure(failed_peer, from_state);
 
-            for transfer in get_shard_transfers(self.shard_id, failed_peer_id) {
+            for transfer in get_shard_transfers(self.shard_id, failed_peer) {
                 self.abort_shard_transfer(
                     transfer,
                     &format!(
-                        "{failed_peer_id}/{}:{} replica failed",
-                        self.collection_id, self.shard_id,
+                        "{failed_peer}/{}:{} replica failed",
+                        self.collection_id, self.shard_id
                     ),
                 );
             }
@@ -862,21 +768,7 @@ impl ShardReplicaSet {
         Ok(())
     }
 
-    pub(crate) async fn health_check(&self, peer_id: PeerId) -> CollectionResult<()> {
-        let remotes = self.remotes.read().await;
-
-        let Some(remote) = remotes.iter().find(|remote| remote.peer_id == peer_id) else {
-            return Err(CollectionError::NotFound {
-                what: format!("{}/{}:{} shard", peer_id, self.collection_id, self.shard_id),
-            });
-        };
-
-        remote.health_check().await?;
-
-        Ok(())
-    }
-
-    pub async fn delete_local_points(
+    pub(crate) async fn delete_local_points(
         &self,
         filter: Filter,
         hw_measurement_acc: HwMeasurementAcc,
@@ -930,10 +822,9 @@ impl ShardReplicaSet {
 
         drop(local_shard_guard);
 
-        let op =
-            CollectionUpdateOperations::PointOperation(point_ops::PointOperations::DeletePoints {
-                ids,
-            });
+        let op = CollectionUpdateOperations::PointOperation(point_ops::PointOperations::DeletePoints {
+            ids,
+        });
 
         // TODO(resharding): Assign clock tag to the operation!? ðŸ¤”
         let result = self
@@ -959,7 +850,7 @@ impl ShardReplicaSet {
             .peers()
             .iter()
             .filter(|(peer, _)| **peer != state.this_peer_id)
-            .map(|(peer_id, _is_active)| {
+            .map(|(peer_id, _)| {
                 RemoteShard::new(
                     shard_id,
                     collection_id.clone(),
@@ -973,8 +864,6 @@ impl ShardReplicaSet {
     /// Check whether a peer is registered as `active`.
     /// Unknown peers are not active.
     fn peer_is_active(&self, peer_id: PeerId) -> bool {
-        // This is used *exclusively* during `execute_*_read_operation`, and so it *should* consider
-        // `ReshardingScaleDown` replicas
         let is_active = matches!(
             self.peer_state(peer_id),
             Some(ReplicaState::Active | ReplicaState::ReshardingScaleDown)
@@ -1096,6 +985,20 @@ impl ShardReplicaSet {
         local_shard.update_cutoff(cutoff).await
     }
 
+    pub(crate) async fn health_check(&self, peer_id: PeerId) -> CollectionResult<()> {
+        let remotes = self.remotes.read().await;
+
+        let Some(remote) = remotes.iter().find(|remote| remote.peer_id == peer_id) else {
+            return Err(CollectionError::NotFound {
+                what: format!("{}/{}:{} shard", peer_id, self.collection_id, self.shard_id),
+            });
+        };
+
+        remote.health_check().await?;
+
+        Ok(())
+    }
+
     pub(crate) fn get_snapshots_storage_manager(&self) -> CollectionResult<SnapshotStorageManager> {
         SnapshotStorageManager::new(&self.shared_storage_config.snapshots_config)
     }
@@ -1105,7 +1008,7 @@ impl ShardReplicaSet {
         let Some(shard) = shard.as_ref() else {
             return false;
         };
-        shard.trigger_optimizers();
+        shard.trigger_optimizers().await;
         true
     }
 
@@ -1173,7 +1076,6 @@ impl ReplicaSetState {
         self.peers
             .iter()
             .filter_map(|(peer_id, state)| {
-                // We consider `ReshardingScaleDown` to be `Active`!
                 matches!(
                     state,
                     ReplicaState::Active | ReplicaState::ReshardingScaleDown
@@ -1215,8 +1117,6 @@ pub enum ReplicaState {
     // A shard which receives data, but is not used for search
     // Useful for backup shards
     Listener,
-    // Deprecated since Qdrant 1.9.0, used in Qdrant 1.7.0 and 1.8.0
-    //
     // Snapshot shard transfer is in progress, updates aren't sent to the shard
     // Normally rejects updates. Since 1.8 it allows updates if force is true.
     PartialSnapshot,
@@ -1247,7 +1147,8 @@ impl ReplicaState {
     }
 
     /// Check whether the replica state is active or listener or resharding.
-    pub fn is_active_or_listener_or_resharding(self) -> bool {
+    #[inline]
+    pub const fn is_active_or_listener_or_resharding(self) -> bool {
         match self {
             ReplicaState::Active
             | ReplicaState::Listener
@@ -1267,7 +1168,8 @@ impl ReplicaState {
     /// In other words: is the state related to shard transfers?
     //
     // TODO(resharding): What's the best way to handle `ReshardingScaleDown` properly!?
-    pub fn is_partial_or_recovery(self) -> bool {
+    #[inline]
+    pub const fn is_partial_or_recovery(self) -> bool {
         match self {
             ReplicaState::Partial
             | ReplicaState::PartialSnapshot
