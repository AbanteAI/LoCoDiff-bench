--- qdrant_lib_collection_src_shards_replica_set_mod.rs_expectedoutput.txt (expected)+++ qdrant_lib_collection_src_shards_replica_set_mod.rs_extracted.txt (actual)@@ -1,4 +1,4 @@-pub mod clock_set;
+mod clock_set;
 mod execute_read_operation;
 mod locally_disabled_peers;
 mod read_ops;
@@ -16,6 +16,7 @@ use common::budget::ResourceBudget;
 use common::counter::hardware_accumulator::HwMeasurementAcc;
 use common::rate_limiting::RateLimiter;
+use common::types::TelemetryDetail;
 use schemars::JsonSchema;
 use segment::common::anonymize::Anonymize;
 use segment::types::{ExtendedPointId, Filter, ShardKey};
@@ -42,6 +43,7 @@ use crate::shards::replica_set::clock_set::ClockSet;
 use crate::shards::shard::{PeerId, Shard, ShardId};
 use crate::shards::shard_config::ShardConfig;
+use crate::shards::telemetry::ReplicaSetTelemetry;
 
 //    â”‚    Collection Created
 //    â”‚
@@ -78,7 +80,7 @@ //    â”‚                                         â”‚
 //    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 //
-
+//
 /// A set of shard replicas.
 ///
 /// Handles operations so that the state is consistent across all the replicas of the shard.
@@ -254,7 +256,7 @@ 
         if replica_state.read().this_peer_id != this_peer_id {
             replica_state
-                .write(|rs| {
+                .write(|mut rs| {
                     let this_peer_id = rs.this_peer_id;
                     let local_state = rs.remove_peer_state(this_peer_id);
                     if let Some(state) = local_state {
@@ -441,22 +443,22 @@     /// Wait for a local shard to be initialized.
     ///
     /// Uses a blocking thread internally.
+    ///
+    /// # Cancel safety
+    ///
+    /// This method is cancel safe.
     pub async fn wait_for_local(&self, timeout: Duration) -> CollectionResult<()> {
         self.wait_for(|replica_set_state| replica_set_state.is_local, timeout)
             .await
     }
 
-    pub fn wait_for_state_condition_sync<F>(&self, check: F, timeout: Duration) -> bool
-    where
-        F: Fn(&ReplicaSetState) -> bool,
-    {
-        let replica_state = self.replica_state.clone();
-        replica_state.wait_for(check, timeout)
-    }
-
     /// Wait for a local shard to get into `state`
     ///
     /// Uses a blocking thread internally.
+    ///
+    /// # Cancel safety
+    ///
+    /// This method is cancel safe.
     pub async fn wait_for_local_state(
         &self,
         state: ReplicaState,
@@ -489,6 +491,15 @@             timeout,
         )
         .await
+    }
+
+    #[cfg(test)]
+    pub fn wait_for_state_condition_sync<F>(&self, check: F, timeout: Duration) -> bool
+    where
+        F: Fn(&ReplicaSetState) -> bool,
+    {
+        let replica_state = self.replica_state.clone();
+        replica_state.wait_for(check, timeout)
     }
 
     /// Wait for a replica set state condition to be true.
@@ -582,7 +593,7 @@     pub async fn remove_local(&self) -> CollectionResult<()> {
         // TODO: Ensure cancel safety!
 
-        self.replica_state.write(|rs| {
+        self.replica_state.write(|mut rs| {
             rs.is_local = false;
             let this_peer_id = rs.this_peer_id;
             rs.remove_peer_state(this_peer_id);
@@ -606,7 +617,7 @@     pub async fn add_remote(&self, peer_id: PeerId, state: ReplicaState) -> CollectionResult<()> {
         debug_assert!(peer_id != self.this_peer_id());
 
-        self.replica_state.write(|rs| {
+        self.replica_state.write(|mut rs| {
             rs.set_peer_state(peer_id, state);
         })?;
 
@@ -630,7 +641,7 @@     }
 
     pub async fn remove_remote(&self, peer_id: PeerId) -> CollectionResult<()> {
-        self.replica_state.write(|rs| {
+        self.replica_state.write(|mut rs| {
             rs.remove_peer_state(peer_id);
         })?;
 
@@ -665,7 +676,7 @@             self.replica_state.read().get_peer_state(peer_id),
         );
 
-        self.replica_state.write(|rs| {
+        self.replica_state.write(|mut rs| {
             if rs.this_peer_id == peer_id {
                 rs.is_local = true;
             }
@@ -785,6 +796,7 @@     pub(crate) async fn on_strict_mode_config_update(&mut self) -> CollectionResult<()> {
         let mut read_local = self.local.write().await;
         if let Some(shard) = read_local.as_mut() {
+            // TODO(ratelimiting) take &mut self and use Option<Mutex> for read_rate_limiter
             shard.on_strict_mode_config_update().await
         }
         drop(read_local);
@@ -862,6 +874,33 @@         Ok(())
     }
 
+    pub(crate) async fn get_telemetry_data(
+        &self,
+        detail: TelemetryDetail,
+    ) -> ReplicaSetTelemetry {
+        let local_shard = self.local.read().await;
+        let local = local_shard.as_ref();
+
+        let local_telemetry = match local {
+            Some(local_shard) => Some(local_shard.get_telemetry_data(detail).await),
+            None => None,
+        };
+
+        ReplicaSetTelemetry {
+            id: self.shard_id,
+            key: self.shard_key.clone(),
+            local: local_telemetry,
+            remote: self
+                .remotes
+                .read()
+                .await
+                .iter()
+                .map(|remote| remote.get_telemetry_data(detail))
+                .collect(),
+            replicate_states: self.replica_state.read().peers().anonymize(detail),
+        }
+    }
+
     pub(crate) async fn health_check(&self, peer_id: PeerId) -> CollectionResult<()> {
         let remotes = self.remotes.read().await;
 
@@ -874,79 +913,6 @@         remote.health_check().await?;
 
         Ok(())
-    }
-
-    pub async fn delete_local_points(
-        &self,
-        filter: Filter,
-        hw_measurement_acc: HwMeasurementAcc,
-        force: bool,
-    ) -> CollectionResult<UpdateResult> {
-        let local_shard_guard = self.local.read().await;
-
-        let Some(local_shard) = local_shard_guard.deref() else {
-            return Err(CollectionError::NotFound {
-                what: format!("local shard {}:{}", self.collection_id, self.shard_id),
-            });
-        };
-
-        let mut next_offset = Some(ExtendedPointId::NumId(0));
-        let mut ids = Vec::new();
-
-        while let Some(current_offset) = next_offset {
-            const BATCH_SIZE: usize = 1000;
-
-            let mut points = local_shard
-                .get()
-                .scroll_by(
-                    Some(current_offset),
-                    BATCH_SIZE + 1,
-                    &false.into(),
-                    &false.into(),
-                    Some(&filter),
-                    &self.search_runtime,
-                    None,
-                    None,
-                    hw_measurement_acc.clone(),
-                )
-                .await?;
-
-            if points.len() > BATCH_SIZE {
-                next_offset = points.pop().map(|points| points.id);
-            } else {
-                next_offset = None;
-            }
-
-            ids.extend(points.into_iter().map(|points| points.id));
-        }
-
-        if ids.is_empty() {
-            return Ok(UpdateResult {
-                operation_id: None,
-                status: UpdateStatus::Completed,
-                clock_tag: None,
-            });
-        }
-
-        drop(local_shard_guard);
-
-        let op =
-            CollectionUpdateOperations::PointOperation(point_ops::PointOperations::DeletePoints {
-                ids,
-            });
-
-        // TODO(resharding): Assign clock tag to the operation!? ðŸ¤”
-        let result = self
-            .update_local(op.into(), true, hw_measurement_acc, force)
-            .await?
-            .ok_or_else(|| {
-                CollectionError::bad_request(format!(
-                    "local shard {}:{} does not exist or is unavailable",
-                    self.collection_id, self.shard_id,
-                ))
-            })?;
-
-        Ok(result)
     }
 
     fn init_remote_shards(
@@ -1030,7 +996,7 @@             }
         }
 
-        locally_disabled_peers_guard.with_upgraded(|locally_disabled_peers| {
+        locally_disabled_peers_guard.with_upgraded(|mut locally_disabled_peers| {
             if locally_disabled_peers.disable_peer_and_notify_if_elapsed(peer_id, from_state) {
                 self.notify_peer_failure(peer_id, from_state);
             }
@@ -1142,13 +1108,87 @@             })
             .unwrap_or_default()
     }
+
+    pub async fn delete_local_points(
+        &self,
+        filter: Filter,
+        hw_measurement_acc: HwMeasurementAcc,
+        force: bool,
+    ) -> CollectionResult<UpdateResult> {
+        let local_shard_guard = self.local.read().await;
+
+        let Some(local_shard) = local_shard_guard.deref() else {
+            return Err(CollectionError::NotFound {
+                what: format!("local shard {}:{}", self.collection_id, self.shard_id),
+            });
+        };
+
+        let mut next_offset = Some(ExtendedPointId::NumId(0));
+        let mut ids = Vec::new();
+
+        while let Some(current_offset) = next_offset {
+            const BATCH_SIZE: usize = 1000;
+
+            let mut points = local_shard
+                .get()
+                .scroll_by(
+                    Some(current_offset),
+                    BATCH_SIZE + 1,
+                    &false.into(),
+                    &false.into(),
+                    Some(&filter),
+                    &self.search_runtime,
+                    None,
+                    None,
+                    hw_measurement_acc.clone(),
+                )
+                .await?;
+
+            if points.len() > BATCH_SIZE {
+                next_offset = points.pop().map(|points| points.id);
+            } else {
+                next_offset = None;
+            }
+
+            ids.extend(points.into_iter().map(|points| points.id));
+        }
+
+        if ids.is_empty() {
+            return Ok(UpdateResult {
+                operation_id: None,
+                status: UpdateStatus::Completed,
+                clock_tag: None,
+            });
+        }
+
+        drop(local_shard_guard);
+
+        let op =
+            CollectionUpdateOperations::PointOperation(point_ops::PointOperations::DeletePoints {
+                ids,
+            });
+
+        // TODO(resharding): Assign clock tag to the operation!? ðŸ¤”
+        let result = self
+            .update_local(op.into(), true, hw_measurement_acc, force)
+            .await?
+            .ok_or_else(|| {
+                CollectionError::bad_request(format!(
+                    "local shard {}:{} does not exist or is unavailable",
+                    self.collection_id, self.shard_id,
+                ))
+            })?;
+
+        Ok(result)
+    }
 }
 
 /// Represents a replica set state
-#[derive(Debug, Deserialize, Serialize, Default, PartialEq, Eq, Clone)]
+#[derive(Debug, Deserialize, Serialize, Default, PartialEq, Eq, Clone, Anonymize)]
 pub struct ReplicaSetState {
     pub is_local: bool,
-    pub this_peer_id: PeerId,
+    #[serde(skip)]
+    pub this_peer_id: PeerId, // Not skipped in anonymize
     peers: HashMap<PeerId, ReplicaState>,
 }
 
@@ -1296,6 +1336,24 @@             | ReplicaState::Listener => false,
         }
     }
+
+    /// Check whether this is a state in which we ignore local clocks.
+    ///
+    /// During some replica states, using clocks may create gaps. That'll be problematic if WAL
+    /// delta recovery is used later, resulting in missing operations. In these states we ignore
+    /// clocks all together to prevent this problem.
+    pub const fn is_ignore_local_clocks(self) -> bool {
+        match self {
+            ReplicaState::Initializing | ReplicaState::Partial => true,
+            ReplicaState::Active
+            | ReplicaState::Listener
+            | ReplicaState::Resharding
+            | ReplicaState::ReshardingScaleDown
+            | ReplicaState::Dead
+            | ReplicaState::PartialSnapshot
+            | ReplicaState::Recovery => false,
+        }
+    }
 }
 
 /// Represents a change in replica set, due to scaling of `replication_factor`
