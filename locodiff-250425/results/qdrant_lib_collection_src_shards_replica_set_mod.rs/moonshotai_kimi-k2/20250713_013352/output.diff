
index 1a8af4043..65d4bfa3a 100644
--- a/qdrant_lib_collection_src_shards_replica_set_mod.rs_expectedoutput.txt (expected):tmp/tmpuym8v7cv_expected.txt	
+++ b/qdrant_lib_collection_src_shards_replica_set_mod.rs_extracted.txt (actual):tmp/tmpjriiff0e_actual.txt	
@@ -1,4 +1,4 @@
-pub mod clock_set;
+mod clock_set;
 mod execute_read_operation;
 mod locally_disabled_peers;
 mod read_ops;
@@ -32,9 +32,9 @@ use crate::collection::payload_index_schema::PayloadIndexSchema;
 use crate::common::collection_size_stats::CollectionSizeStats;
 use crate::common::snapshots_manager::SnapshotStorageManager;
 use crate::config::CollectionConfigInternal;
+use crate::operations::{CollectionUpdateOperations, point_ops};
 use crate::operations::shared_storage_config::SharedStorageConfig;
 use crate::operations::types::{CollectionError, CollectionResult, UpdateResult, UpdateStatus};
-use crate::operations::{CollectionUpdateOperations, point_ops};
 use crate::optimizers_builder::OptimizersConfig;
 use crate::save_on_disk::SaveOnDisk;
 use crate::shards::channel_service::ChannelService;
@@ -208,8 +208,8 @@ impl ShardReplicaSet {
             replica_state: replica_state.into(),
             locally_disabled_peers: Default::default(),
             shard_path,
-            abort_shard_transfer_cb: abort_shard_transfer,
             notify_peer_failure_cb: on_peer_failure,
+            abort_shard_transfer_cb: abort_shard_transfer,
             channel_service,
             collection_id,
             collection_config,
@@ -293,7 +293,6 @@ impl ShardReplicaSet {
                     collection_id.clone(),
                     shard_path,
                     collection_config.clone(),
-                    effective_optimizers_config.clone(),
                     shared_storage_config.clone(),
                     payload_index_schema.clone(),
                     update_runtime.clone(),
@@ -345,7 +344,6 @@ impl ShardReplicaSet {
             local: RwLock::new(local),
             remotes: RwLock::new(remote_shards),
             replica_state: replica_state.into(),
-            // TODO: move to collection config
             locally_disabled_peers: Default::default(),
             shard_path: shard_path.to_path_buf(),
             notify_peer_failure_cb: on_peer_failure,
@@ -420,7 +418,6 @@ impl ShardReplicaSet {
     pub fn active_shards(&self) -> Vec<PeerId> {
         let replica_state = self.replica_state.read();
         replica_state
-            // This is a part of deprecated built-in resharding implementation, so we don't care
             .active_peers()
             .into_iter()
             .filter(|&peer_id| !self.is_locally_disabled(peer_id))
@@ -432,7 +429,7 @@ impl ShardReplicaSet {
         let replica_state = self.replica_state.read();
         let this_peer_id = replica_state.this_peer_id;
         replica_state
-            .active_peers() // This includes `Active` and `ReshardingScaleDown` replicas!
+            .active_peers()
             .into_iter()
             .filter(|&peer_id| !self.is_locally_disabled(peer_id) && peer_id != this_peer_id)
             .collect()
@@ -740,7 +737,6 @@ impl ShardReplicaSet {
                         self.set_local(local_shard, Some(state)).await?;
                         self.notify_peer_failure(peer_id, Some(state));
                     }
-
                     ReplicaState::Dead
                     | ReplicaState::Partial
                     | ReplicaState::Initializing
@@ -764,45 +760,6 @@ impl ShardReplicaSet {
             );
             self.remotes.write().await.push(new_remote);
         }
-
-        // Apply shard key
-        self.shard_key = shard_key;
-
-        Ok(())
-    }
-
-    pub(crate) async fn on_optimizer_config_update(&self) -> CollectionResult<()> {
-        let read_local = self.local.read().await;
-        if let Some(shard) = &*read_local {
-            shard.on_optimizer_config_update().await
-        } else {
-            Ok(())
-        }
-    }
-
-    /// Apply shard's strict mode configuration update
-    /// - Update read and write rate limiters
-    pub(crate) async fn on_strict_mode_config_update(&mut self) -> CollectionResult<()> {
-        let mut read_local = self.local.write().await;
-        if let Some(shard) = read_local.as_mut() {
-            shard.on_strict_mode_config_update().await
-        }
-        drop(read_local);
-        let config = self.collection_config.read().await;
-        if let Some(strict_mode_config) = &config.strict_mode_config {
-            if strict_mode_config.enabled == Some(true) {
-                // update write rate limiter
-                if let Some(write_rate_limit_per_min) = strict_mode_config.write_rate_limit {
-                    let new_write_rate_limiter =
-                        RateLimiter::new_per_minute(write_rate_limit_per_min);
-                    self.write_rate_limiter
-                        .replace(parking_lot::Mutex::new(new_write_rate_limiter));
-                    return Ok(());
-                }
-            }
-        }
-        // remove write rate limiter for all other situations
-        self.write_rate_limiter.take();
         Ok(())
     }
 
@@ -862,117 +819,8 @@ impl ShardReplicaSet {
         Ok(())
     }
 
-    pub(crate) async fn health_check(&self, peer_id: PeerId) -> CollectionResult<()> {
-        let remotes = self.remotes.read().await;
-
-        let Some(remote) = remotes.iter().find(|remote| remote.peer_id == peer_id) else {
-            return Err(CollectionError::NotFound {
-                what: format!("{}/{}:{} shard", peer_id, self.collection_id, self.shard_id),
-            });
-        };
-
-        remote.health_check().await?;
-
-        Ok(())
-    }
-
-    pub async fn delete_local_points(
-        &self,
-        filter: Filter,
-        hw_measurement_acc: HwMeasurementAcc,
-        force: bool,
-    ) -> CollectionResult<UpdateResult> {
-        let local_shard_guard = self.local.read().await;
-
-        let Some(local_shard) = local_shard_guard.deref() else {
-            return Err(CollectionError::NotFound {
-                what: format!("local shard {}:{}", self.collection_id, self.shard_id),
-            });
-        };
-
-        let mut next_offset = Some(ExtendedPointId::NumId(0));
-        let mut ids = Vec::new();
-
-        while let Some(current_offset) = next_offset {
-            const BATCH_SIZE: usize = 1000;
-
-            let mut points = local_shard
-                .get()
-                .scroll_by(
-                    Some(current_offset),
-                    BATCH_SIZE + 1,
-                    &false.into(),
-                    &false.into(),
-                    Some(&filter),
-                    &self.search_runtime,
-                    None,
-                    None,
-                    hw_measurement_acc.clone(),
-                )
-                .await?;
-
-            if points.len() > BATCH_SIZE {
-                next_offset = points.pop().map(|points| points.id);
-            } else {
-                next_offset = None;
-            }
-
-            ids.extend(points.into_iter().map(|points| points.id));
-        }
-
-        if ids.is_empty() {
-            return Ok(UpdateResult {
-                operation_id: None,
-                status: UpdateStatus::Completed,
-                clock_tag: None,
-            });
-        }
-
-        drop(local_shard_guard);
-
-        let op =
-            CollectionUpdateOperations::PointOperation(point_ops::PointOperations::DeletePoints {
-                ids,
-            });
-
-        // TODO(resharding): Assign clock tag to the operation!? ðŸ¤”
-        let result = self
-            .update_local(op.into(), true, hw_measurement_acc, force)
-            .await?
-            .ok_or_else(|| {
-                CollectionError::bad_request(format!(
-                    "local shard {}:{} does not exist or is unavailable",
-                    self.collection_id, self.shard_id,
-                ))
-            })?;
-
-        Ok(result)
-    }
-
-    fn init_remote_shards(
-        shard_id: ShardId,
-        collection_id: CollectionId,
-        state: &ReplicaSetState,
-        channel_service: &ChannelService,
-    ) -> Vec<RemoteShard> {
-        state
-            .peers()
-            .iter()
-            .filter(|(peer, _)| **peer != state.this_peer_id)
-            .map(|(peer_id, _is_active)| {
-                RemoteShard::new(
-                    shard_id,
-                    collection_id.clone(),
-                    *peer_id,
-                    channel_service.clone(),
-                )
-            })
-            .collect()
-    }
-
-    /// Check whether a peer is registered as `active`.
-    /// Unknown peers are not active.
-    fn peer_is_active(&self, peer_id: PeerId) -> bool {
+    /// Check if the requested peer state is active
+    pub fn peer_is_active(&self, peer_id: PeerId) -> bool {
         // This is used *exclusively* during `execute_*_read_operation`, and so it *should* consider
         // `ReshardingScaleDown` replicas
         let is_active = matches!(
@@ -1016,7 +864,7 @@ impl ShardReplicaSet {
     ) {
         let other_peers = state
             .active_or_resharding_peers()
-            .filter(|id| id != &peer_id);
+            .filter(|&id| id != peer_id);
 
         let mut locally_disabled_peers_guard = self.locally_disabled_peers.upgradable_read();
 
@@ -1052,6 +900,7 @@ impl ShardReplicaSet {
         }
     }
 
+    /// Report peer failure to consensus and abort any active shard transfers
     fn notify_peer_failure(&self, peer_id: PeerId, from_state: Option<ReplicaState>) {
         log::debug!("Notify peer failure: {peer_id}");
         self.notify_peer_failure_cb.deref()(peer_id, self.shard_id, from_state)
@@ -1069,6 +918,65 @@ impl ShardReplicaSet {
         self.abort_shard_transfer_cb.deref()(transfer, reason)
     }
 
+    fn init_remote_shards(
+        shard_id: ShardId,
+        collection_id: CollectionId,
+        state: &ReplicaSetState,
+        channel_service: &ChannelService,
+    ) -> Vec<RemoteShard> {
+        state
+            .peers()
+            .iter()
+            .filter(|(&peer, _)| peer != state.this_peer_id)
+            .map(|(&peer_id, _is_active)| {
+                RemoteShard::new(
+                    shard_id,
+                    collection_id.clone(),
+                    peer_id,
+                    channel_service.clone(),
+                )
+            })
+            .collect()
+    }
+
+    pub(crate) async fn on_optimizer_config_update(&mut self) -> CollectionResult<()> {
+        let mut read_local = self.local.write().await;
+        if let Some(shard) = read_local.as_mut() {
+            shard.on_optimizer_config_update().await
+        }
+        drop(read_local);
+        let config = self.collection_config.read().await;
+        if let Some(strict_mode_config) = &config.strict_mode_config {
+            if strict_mode_config.enabled == Some(true) {
+                // update write rate limiter
+                if let Some(write_rate_limit_per_min) = strict_mode_config.write_rate_limit {
+                    let new_write_rate_limiter =
+                        RateLimiter::new_per_minute(write_rate_limit_per_min);
+                    self.write_rate_limiter
+                        .replace(parking_lot::Mutex::new(new_write_rate_limiter));
+                    return Ok(());
+                }
+            }
+        }
+        // remove write rate limiter for all other situations
+        self.write_rate_limiter.take();
+        Ok(())
+    }
+
+    pub(crate) async fn health_check(&self, peer_id: PeerId) -> CollectionResult<()> {
+        let remotes = self.remotes.read().await;
+
+        let Some(remote) = remotes.iter().find(|remote| remote.peer_id == peer_id) else {
+            return Err(CollectionError::NotFound {
+                what: format!("{}/{}:{} shard", peer_id, self.collection_id, self.shard_id),
+            });
+        };
+
+        remote.health_check().await?;
+
+        Ok(())
+    }
+
     /// Get shard recovery point for WAL.
     pub(crate) async fn shard_recovery_point(&self) -> CollectionResult<RecoveryPoint> {
         let local_shard = self.local.read().await;
@@ -1100,7 +1008,7 @@ impl ShardReplicaSet {
         SnapshotStorageManager::new(&self.shared_storage_config.snapshots_config)
     }
 
-    pub(crate) async fn trigger_optimizers(&self) -> bool {
+    pub(crate) fn trigger_optimizers(&self) -> bool {
         let shard = self.local.read().await;
         let Some(shard) = shard.as_ref() else {
             return false;
@@ -1142,6 +1050,79 @@ impl ShardReplicaSet {
             })
             .unwrap_or_default()
     }
+
+    pub async fn delete_local_points(
+        &self,
+        filter: Filter,
+        hw_measurement_acc: HwMeasurementAcc,
+        force: bool,
+    ) -> CollectionResult<UpdateResult> {
+        let local_shard_guard = self.local.read().await;
+
+        let Some(local_shard) = local_shard_guard.deref() else {
+            return Err(CollectionError::NotFound {
+                what: format!("local shard {}:{}", self.collection_id, self.shard_id),
+            });
+        };
+
+        let mut next_offset = Some(ExtendedPointId::NumId(0));
+        let mut ids = Vec::new();
+
+        while let Some(current_offset) = next_offset {
+            const BATCH_SIZE: usize = 1000;
+
+            let mut points = local_shard
+                .get()
+                .scroll_by(
+                    Some(current_offset),
+                    BATCH_SIZE + 1,
+                    &false.into(),
+                    &false.into(),
+                    Some(&filter),
+                    &self.search_runtime,
+                    None,
+                    None,
+                    hw_measurement_acc.clone(),
+                )
+                .await?;
+
+            if points.len() > BATCH_SIZE {
+                next_offset = points.pop().map(|points| points.id);
+            } else {
+                next_offset = None;
+            }
+
+            ids.extend(points.into_iter().map(|points| points.id));
+        }
+
+        if ids.is_empty() {
+            return Ok(UpdateResult {
+                operation_id: None,
+                status: UpdateStatus::Completed,
+                clock_tag: None,
+            });
+        }
+
+        drop(local_shard_guard);
+
+        let op =
+            CollectionUpdateOperations::PointOperation(point_ops::PointOperations::DeletePoints {
+                ids,
+            });
+
+        // TODO(resharding): Assign clock tag to the operation!? ðŸ¤”
+        let result = self
+            .update_local(op.into(), true, hw_measurement_acc, force)
+            .await?
+            .ok_or_else(|| {
+                CollectionError::bad_request(format!(
+                    "local shard {}:{} does not exist or is unavailable",
+                    self.collection_id, self.shard_id,
+                ))
+            })?;
+
+        Ok(result)
+    }
 }
 
 /// Represents a replica set state
@@ -1172,24 +1153,24 @@ impl ReplicaSetState {
     pub fn active_peers(&self) -> Vec<PeerId> {
         self.peers
             .iter()
-            .filter_map(|(peer_id, state)| {
+            .filter_map(|(&peer_id, &state)| {
                 // We consider `ReshardingScaleDown` to be `Active`!
                 matches!(
                     state,
                     ReplicaState::Active | ReplicaState::ReshardingScaleDown
                 )
-                .then_some(*peer_id)
+                .then_some(peer_id)
             })
             .collect()
     }
 
     pub fn active_or_resharding_peers(&self) -> impl Iterator<Item = PeerId> + '_ {
-        self.peers.iter().filter_map(|(peer_id, state)| {
+        self.peers.iter().filter_map(|(&peer_id, &state)| {
             matches!(
                 state,
                 ReplicaState::Active | ReplicaState::Resharding | ReplicaState::ReshardingScaleDown
             )
-            .then_some(*peer_id)
+            .then_some(peer_id)
         })
     }
 
@@ -1215,7 +1196,6 @@ pub enum ReplicaState {
     // A shard which receives data, but is not used for search
     // Useful for backup shards
     Listener,
-    // Deprecated since Qdrant 1.9.0, used in Qdrant 1.7.0 and 1.8.0
     //
     // Snapshot shard transfer is in progress, updates aren't sent to the shard
     // Normally rejects updates. Since 1.8 it allows updates if force is true.
@@ -1231,8 +1211,8 @@ pub enum ReplicaState {
 
 impl ReplicaState {
     /// Check if replica state is active
-    pub fn is_active(self) -> bool {
-        match self {
+    pub fn is_active(&self) -> bool {
+        match *self {
             ReplicaState::Active => true,
             ReplicaState::ReshardingScaleDown => true,
 
@@ -1284,7 +1264,7 @@ impl ReplicaState {
 
     /// Returns `true` if the replica state is resharding, either up or down.
     pub fn is_resharding(&self) -> bool {
-        match self {
+        match *self {
             ReplicaState::Resharding | ReplicaState::ReshardingScaleDown => true,
 
             ReplicaState::Partial
