
index 1a8af404..1962bb53 100644
--- a/qdrant_lib_collection_src_shards_replica_set_mod.rs_expectedoutput.txt (expected):tmp/tmpcbztf68t_expected.txt	
+++ b/qdrant_lib_collection_src_shards_replica_set_mod.rs_extracted.txt (actual):tmp/tmpwpgsujea_actual.txt	
@@ -1,4 +1,3 @@
-pub mod clock_set;
 mod execute_read_operation;
 mod locally_disabled_peers;
 mod read_ops;
@@ -81,9 +80,9 @@ use crate::shards::shard_config::ShardConfig;
 
 /// A set of shard replicas.
 ///
-/// Handles operations so that the state is consistent across all the replicas of the shard.
-/// Prefers local shard for read-only operations.
-/// Perform updates on all replicas and report error if there is at least one failure.
+ /// Handles operations so that the state is consistent across all the replicas of the shard.
+ /// Prefers local shard for read-only operations.
+ /// Perform updates on all replicas and report error if there is at least one failure.
 ///
 pub struct ShardReplicaSet {
     local: RwLock<Option<Shard>>, // Abstract Shard to be able to use a Proxy during replication
@@ -208,8 +207,8 @@ impl ShardReplicaSet {
             replica_state: replica_state.into(),
             locally_disabled_peers: Default::default(),
             shard_path,
-            abort_shard_transfer_cb: abort_shard_transfer,
             notify_peer_failure_cb: on_peer_failure,
+            abort_shard_transfer_cb: abort_shard_transfer,
             channel_service,
             collection_id,
             collection_config,
@@ -364,7 +363,6 @@ impl ShardReplicaSet {
             write_rate_limiter,
         };
 
-        // `active_remote_shards` includes `Active` and `ReshardingScaleDown` replicas!
         if local_load_failure && replica_set.active_remote_shards().is_empty() {
             replica_set
                 .locally_disabled_peers
@@ -406,12 +404,6 @@ impl ShardReplicaSet {
         self.replica_state.read().peers()
     }
 
-    pub fn is_last_active_replica(&self, peer_id: PeerId) -> bool {
-        // This includes `Active` and `ReshardingScaleDown` replicas!
-        let active_peers = self.replica_state.read().active_peers();
-        active_peers.len() == 1 && active_peers.contains(&peer_id)
-    }
-
     pub fn peer_state(&self, peer_id: PeerId) -> Option<ReplicaState> {
         self.replica_state.read().get_peer_state(peer_id)
     }
@@ -438,30 +430,35 @@ impl ShardReplicaSet {
             .collect()
     }
 
-    /// Wait for a local shard to be initialized.
-    ///
-    /// Uses a blocking thread internally.
     pub async fn wait_for_local(&self, timeout: Duration) -> CollectionResult<()> {
         self.wait_for(|replica_set_state| replica_set_state.is_local, timeout)
             .await
     }
 
-    pub fn wait_for_state_condition_sync<F>(&self, check: F, timeout: Duration) -> bool
-    where
-        F: Fn(&ReplicaSetState) -> bool,
-    {
-        let replica_state = self.replica_state.clone();
-        replica_state.wait_for(check, timeout)
-    }
-
-    /// Wait for a local shard to get into `state`
+    /// Wait for the replica set to reach the `Partial` state for a peer
     ///
     /// Uses a blocking thread internally.
-    pub async fn wait_for_local_state(
+    pub async fn wait_for_partial(
         &self,
-        state: ReplicaState,
+        peer_id: PeerId,
         timeout: Duration,
     ) -> CollectionResult<()> {
+        self.wait_for(
+            move |replica_set_state| {
+                matches!(
+                    replica_set_state.get_peer_state(peer_id),
+                    Some(ReplicaState::Partial)
+                )
+            },
+            timeout,
+        )
+        .await
+    }
+
+    /// Wait for a local shard to get into `state`
+    ///
+    /// Uses a blocking thread internally.
+    pub async fn wait_for_local_state(&self, state: ReplicaState, timeout: Duration) -> CollectionResult<()> {
         self.wait_for(
             move |replica_set_state| {
                 replica_set_state.get_peer_state(replica_set_state.this_peer_id) == Some(state)
@@ -523,6 +520,14 @@ impl ShardReplicaSet {
         Ok(())
     }
 
+    pub fn wait_for_state_condition_sync<F>(&self, check: F, timeout: Duration) -> bool
+    where
+        F: Fn(&ReplicaSetState) -> bool,
+    {
+        let replica_state = self.replica_state.clone();
+        replica_state.wait_for(check, timeout)
+    }
+
     /// Clears the local shard data and loads an empty local shard
     pub async fn init_empty_local_shard(&self) -> CollectionResult<()> {
         let mut local = self.local.write().await;
@@ -730,7 +735,6 @@ impl ShardReplicaSet {
                     self.optimizers_config.clone(),
                 )
                 .await?;
-
                 match state {
                     ReplicaState::Active
                     | ReplicaState::Listener
@@ -740,7 +744,6 @@ impl ShardReplicaSet {
                         self.set_local(local_shard, Some(state)).await?;
                         self.notify_peer_failure(peer_id, Some(state));
                     }
-
                     ReplicaState::Dead
                     | ReplicaState::Partial
                     | ReplicaState::Initializing
@@ -750,7 +753,6 @@ impl ShardReplicaSet {
                         self.set_local(local_shard, Some(state)).await?;
                     }
                 }
-
                 continue;
             }
 
@@ -862,6 +864,26 @@ impl ShardReplicaSet {
         Ok(())
     }
 
+    pub async fn sync_local_state_async<F>(&self, get_shard_transfers: F) -> CollectionResult<()>
+    where
+        F: Fn(ShardId, PeerId) -> Vec<ShardTransfer>,
+    {
+        for &failed_peer in self.locally_disabled_peers.read().iter() {
+            self.notify_peer_failure(failed_peer, None);
+
+            for transfer in get_shard_transfers(self.shard_id, failed_peer) {
+                self.abort_shard_transfer(
+                    transfer,
+                    &format!(
+                        "{failed_peer}/{}:{} replica failed",
+                        self.collection_id, self.shard_id
+                    ),
+                );
+            }
+        }
+        Ok(())
+    }
+
     pub(crate) async fn health_check(&self, peer_id: PeerId) -> CollectionResult<()> {
         let remotes = self.remotes.read().await;
 
@@ -876,79 +898,6 @@ impl ShardReplicaSet {
         Ok(())
     }
 
-    pub async fn delete_local_points(
-        &self,
-        filter: Filter,
-        hw_measurement_acc: HwMeasurementAcc,
-        force: bool,
-    ) -> CollectionResult<UpdateResult> {
-        let local_shard_guard = self.local.read().await;
-
-        let Some(local_shard) = local_shard_guard.deref() else {
-            return Err(CollectionError::NotFound {
-                what: format!("local shard {}:{}", self.collection_id, self.shard_id),
-            });
-        };
-
-        let mut next_offset = Some(ExtendedPointId::NumId(0));
-        let mut ids = Vec::new();
-
-        while let Some(current_offset) = next_offset {
-            const BATCH_SIZE: usize = 1000;
-
-            let mut points = local_shard
-                .get()
-                .scroll_by(
-                    Some(current_offset),
-                    BATCH_SIZE + 1,
-                    &false.into(),
-                    &false.into(),
-                    Some(&filter),
-                    &self.search_runtime,
-                    None,
-                    None,
-                    hw_measurement_acc.clone(),
-                )
-                .await?;
-
-            if points.len() > BATCH_SIZE {
-                next_offset = points.pop().map(|points| points.id);
-            } else {
-                next_offset = None;
-            }
-
-            ids.extend(points.into_iter().map(|points| points.id));
-        }
-
-        if ids.is_empty() {
-            return Ok(UpdateResult {
-                operation_id: None,
-                status: UpdateStatus::Completed,
-                clock_tag: None,
-            });
-        }
-
-        drop(local_shard_guard);
-
-        let op =
-            CollectionUpdateOperations::PointOperation(point_ops::PointOperations::DeletePoints {
-                ids,
-            });
-
-        // TODO(resharding): Assign clock tag to the operation!? ðŸ¤”
-        let result = self
-            .update_local(op.into(), true, hw_measurement_acc, force)
-            .await?
-            .ok_or_else(|| {
-                CollectionError::bad_request(format!(
-                    "local shard {}:{} does not exist or is unavailable",
-                    self.collection_id, self.shard_id,
-                ))
-            })?;
-
-        Ok(result)
-    }
-
     fn init_remote_shards(
         shard_id: ShardId,
         collection_id: CollectionId,
@@ -1069,6 +1018,79 @@ impl ShardReplicaSet {
         self.abort_shard_transfer_cb.deref()(transfer, reason)
     }
 
+    pub async fn delete_local_points(
+        &self,
+        filter: Filter,
+        hw_measurement_acc: HwMeasurementAcc,
+        force: bool,
+    ) -> CollectionResult<UpdateResult> {
+        let local_shard_guard = self.local.read().await;
+
+        let Some(local_shard) = local_shard_guard.deref() else {
+            return Err(CollectionError::NotFound {
+                what: format!("local shard {}:{}", self.collection_id, self.shard_id),
+            });
+        };
+
+        let mut next_offset = Some(ExtendedPointId::NumId(0));
+        let mut ids = Vec::new();
+
+        while let Some(current_offset) = next_offset {
+            const BATCH_SIZE: usize = 1000;
+
+            let mut points = local_shard
+                .get()
+                .scroll_by(
+                    Some(current_offset),
+                    BATCH_SIZE + 1,
+                    &false.into(),
+                    &false.into(),
+                    Some(&filter),
+                    &self.search_runtime,
+                    None,
+                    None,
+                    hw_measurement_acc.clone(),
+                )
+                .await?;
+
+            if points.len() > BATCH_SIZE {
+                next_offset = points.pop().map(|points| points.id);
+            } else {
+                next_offset = None;
+            }
+
+            ids.extend(points.into_iter().map(|points| points.id));
+        }
+
+        if ids.is_empty() {
+            return Ok(UpdateResult {
+                operation_id: None,
+                status: UpdateStatus::Completed,
+                clock_tag: None,
+            });
+        }
+
+        drop(local_shard_guard);
+
+        let op =
+            CollectionUpdateOperations::PointOperation(point_ops::PointOperations::DeletePoints {
+                ids,
+            });
+
+        // TODO(resharding): Assign clock tag to the operation!? ðŸ¤”
+        let result = self
+            .update_local(op.into(), true, hw_measurement_acc, force)
+            .await?
+            .ok_or_else(|| {
+                CollectionError::bad_request(format!(
+                    "local shard {}:{} does not exist or is unavailable",
+                    self.collection_id, self.shard_id,
+                ))
+            })?;
+
+        Ok(result)
+    }
+
     /// Get shard recovery point for WAL.
     pub(crate) async fn shard_recovery_point(&self) -> CollectionResult<RecoveryPoint> {
         let local_shard = self.local.read().await;
@@ -1215,8 +1237,6 @@ pub enum ReplicaState {
     // A shard which receives data, but is not used for search
     // Useful for backup shards
     Listener,
-    // Deprecated since Qdrant 1.9.0, used in Qdrant 1.7.0 and 1.8.0
-    //
     // Snapshot shard transfer is in progress, updates aren't sent to the shard
     // Normally rejects updates. Since 1.8 it allows updates if force is true.
     PartialSnapshot,
