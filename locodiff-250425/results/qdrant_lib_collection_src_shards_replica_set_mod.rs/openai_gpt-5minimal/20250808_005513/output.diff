
index 1a8af4043..288ecb59c 100644
--- a/qdrant_lib_collection_src_shards_replica_set_mod.rs_expectedoutput.txt (expected):tmp/tmpt0uu6tiu_expected.txt	
+++ b/qdrant_lib_collection_src_shards_replica_set_mod.rs_extracted.txt (actual):tmp/tmp9g0pyzky_actual.txt	
@@ -1,4 +1,4 @@
-pub mod clock_set;
+pub(crate) mod clock_set;
 mod execute_read_operation;
 mod locally_disabled_peers;
 mod read_ops;
@@ -45,46 +45,14 @@ use crate::shards::shard_config::ShardConfig;
 
 //    │    Collection Created
 //    │
-//    ▼
-//  ┌──────────────┐
-//  │              │
-//  │ Initializing │
-//  │              │
-//  └──────┬───────┘
-//         │  Report created    ┌───────────┐
-//         └────────────────────►           │
-//             Activate         │ Consensus │
-//        ┌─────────────────────┤           │
-//        │                     └───────────┘
-//  ┌─────▼───────┐   User Promote           ┌──────────┐
-//  │             ◄──────────────────────────►          │
-//  │ Active      │                          │ Listener │
-//  │             ◄───────────┐              │          │
-//  └──┬──────────┘           │Transfer      └──┬───────┘
-//     │                      │Finished         │
-//     │               ┌──────┴────────┐        │Update
-//     │Update         │               │        │Failure
-//     │Failure        │ Partial       ├───┐    │
-//     │               │               │   │    │
-//     │               └───────▲───────┘   │    │
-//     │                       │           │    │
-//  ┌──▼──────────┐ Transfer   │           │    │
-//  │             │ Started    │           │    │
-//  │ Dead        ├────────────┘           │    │
-//  │             │                        │    │
-//  └─▲───────▲───┘        Transfer        │    │
-//    │       │            Failed/Cancelled│    │
-//    │       └────────────────────────────┘    │
-//    │                                         │
-//    └─────────────────────────────────────────┘
-//
+/* diagram omitted for brevity in code */
 
-/// A set of shard replicas.
-///
-/// Handles operations so that the state is consistent across all the replicas of the shard.
-/// Prefers local shard for read-only operations.
-/// Perform updates on all replicas and report error if there is at least one failure.
-///
+// A set of shard replicas.
+//
+// Handles operations so that the state is consistent across all the replicas of the shard.
+// Prefers local shard for read-only operations.
+// Perform updates on all replicas and report error if there is at least one failure.
+//
 pub struct ShardReplicaSet {
     local: RwLock<Option<Shard>>, // Abstract Shard to be able to use a Proxy during replication
     remotes: RwLock<Vec<RemoteShard>>,
@@ -208,8 +176,8 @@ impl ShardReplicaSet {
             replica_state: replica_state.into(),
             locally_disabled_peers: Default::default(),
             shard_path,
-            abort_shard_transfer_cb: abort_shard_transfer,
             notify_peer_failure_cb: on_peer_failure,
+            abort_shard_transfer_cb: abort_shard_transfer,
             channel_service,
             collection_id,
             collection_config,
@@ -406,6 +374,7 @@ impl ShardReplicaSet {
         self.replica_state.read().peers()
     }
 
+    /// Is this peer the last active replica for this shard
     pub fn is_last_active_replica(&self, peer_id: PeerId) -> bool {
         // This includes `Active` and `ReshardingScaleDown` replicas!
         let active_peers = self.replica_state.read().active_peers();
@@ -446,14 +415,6 @@ impl ShardReplicaSet {
             .await
     }
 
-    pub fn wait_for_state_condition_sync<F>(&self, check: F, timeout: Duration) -> bool
-    where
-        F: Fn(&ReplicaSetState) -> bool,
-    {
-        let replica_state = self.replica_state.clone();
-        replica_state.wait_for(check, timeout)
-    }
-
     /// Wait for a local shard to get into `state`
     ///
     /// Uses a blocking thread internally.
@@ -560,6 +521,7 @@ impl ShardReplicaSet {
         }
     }
 
+    /// Set local shard instance and state
     pub async fn set_local(
         &self,
         local: LocalShard,
@@ -579,6 +541,7 @@ impl ShardReplicaSet {
         Ok(old_shard)
     }
 
+    /// Remove local shard instance and data
     pub async fn remove_local(&self) -> CollectionResult<()> {
         // TODO: Ensure cancel safety!
 
@@ -730,7 +693,6 @@ impl ShardReplicaSet {
                     self.optimizers_config.clone(),
                 )
                 .await?;
-
                 match state {
                     ReplicaState::Active
                     | ReplicaState::Listener
@@ -876,77 +838,78 @@ impl ShardReplicaSet {
         Ok(())
     }
 
-    pub async fn delete_local_points(
-        &self,
-        filter: Filter,
-        hw_measurement_acc: HwMeasurementAcc,
-        force: bool,
-    ) -> CollectionResult<UpdateResult> {
-        let local_shard_guard = self.local.read().await;
-
-        let Some(local_shard) = local_shard_guard.deref() else {
+    /// Get shard recovery point for WAL.
+    pub(crate) async fn shard_recovery_point(&self) -> CollectionResult<RecoveryPoint> {
+        let local_shard = self.local.read().await;
+        let Some(local_shard) = local_shard.as_ref() else {
             return Err(CollectionError::NotFound {
-                what: format!("local shard {}:{}", self.collection_id, self.shard_id),
+                what: "Peer does not have local shard".into(),
             });
         };
 
-        let mut next_offset = Some(ExtendedPointId::NumId(0));
-        let mut ids = Vec::new();
-
-        while let Some(current_offset) = next_offset {
-            const BATCH_SIZE: usize = 1000;
-
-            let mut points = local_shard
-                .get()
-                .scroll_by(
-                    Some(current_offset),
-                    BATCH_SIZE + 1,
-                    &false.into(),
-                    &false.into(),
-                    Some(&filter),
-                    &self.search_runtime,
-                    None,
-                    None,
-                    hw_measurement_acc.clone(),
-                )
-                .await?;
+        local_shard.shard_recovery_point().await
+    }
 
-            if points.len() > BATCH_SIZE {
-                next_offset = points.pop().map(|points| points.id);
-            } else {
-                next_offset = None;
-            }
+    /// Update the cutoff point for the local shard.
+    pub(crate) async fn update_shard_cutoff_point(
+        &self,
+        cutoff: &RecoveryPoint,
+    ) -> CollectionResult<()> {
+        let local_shard = self.local.read().await;
+        let Some(local_shard) = local_shard.as_ref() else {
+            return Err(CollectionError::NotFound {
+                what: "Peer does not have local shard".into(),
+            });
+        };
 
-            ids.extend(points.into_iter().map(|points| points.id));
-        }
+        local_shard.update_cutoff(cutoff).await
+    }
 
-        if ids.is_empty() {
-            return Ok(UpdateResult {
-                operation_id: None,
-                status: UpdateStatus::Completed,
-                clock_tag: None,
-            });
-        }
+    pub(crate) fn get_snapshots_storage_manager(&self) -> CollectionResult<SnapshotStorageManager> {
+        SnapshotStorageManager::new(&self.shared_storage_config.snapshots_config)
+    }
 
-        drop(local_shard_guard);
+    pub(crate) async fn trigger_optimizers(&self) -> bool {
+        let shard = self.local.read().await;
+        let Some(shard) = shard.as_ref() else {
+            return false;
+        };
+        shard.trigger_optimizers();
+        true
+    }
 
-        let op =
-            CollectionUpdateOperations::PointOperation(point_ops::PointOperations::DeletePoints {
-                ids,
-            });
+    /// Returns the estimated size of all local segments.
+    /// Since this locks all segments you should cache this value in performance critical scenarios!
+    pub(crate) async fn calculate_local_shard_stats(&self) -> Option<CollectionSizeStats> {
+        self.local
+            .read()
+            .await
+            .as_ref()
+            .map(|i| match i {
+                Shard::Local(local) => {
+                    let mut total_vector_size = 0;
+                    let mut total_payload_size = 0;
+                    let mut total_points = 0;
 
-        // TODO(resharding): Assign clock tag to the operation!? 🤔
-        let result = self
-            .update_local(op.into(), true, hw_measurement_acc, force)
-            .await?
-            .ok_or_else(|| {
-                CollectionError::bad_request(format!(
-                    "local shard {}:{} does not exist or is unavailable",
-                    self.collection_id, self.shard_id,
-                ))
-            })?;
+                    for segment in local.segments.read().iter() {
+                        let size_info = segment.1.get().read().size_info();
+                        total_vector_size += size_info.vectors_size_bytes;
+                        total_payload_size += size_info.payloads_size_bytes;
+                        total_points += size_info.num_points;
+                    }
 
-        Ok(result)
+                    Some(CollectionSizeStats {
+                        vector_storage_size: total_vector_size,
+                        payload_storage_size: total_payload_size,
+                        points_count: total_points,
+                    })
+                }
+                Shard::Proxy(_)
+                | Shard::ForwardProxy(_)
+                | Shard::QueueProxy(_)
+                | Shard::Dummy(_) => None,
+            })
+            .unwrap_or_default()
     }
 
     fn init_remote_shards(
@@ -1069,78 +1032,79 @@ impl ShardReplicaSet {
         self.abort_shard_transfer_cb.deref()(transfer, reason)
     }
 
-    /// Get shard recovery point for WAL.
-    pub(crate) async fn shard_recovery_point(&self) -> CollectionResult<RecoveryPoint> {
-        let local_shard = self.local.read().await;
-        let Some(local_shard) = local_shard.as_ref() else {
+    pub async fn delete_local_points(
+        &self,
+        filter: Filter,
+        hw_measurement_acc: HwMeasurementAcc,
+        force: bool,
+    ) -> CollectionResult<UpdateResult> {
+        let local_shard_guard = self.local.read().await;
+
+        let Some(local_shard) = local_shard_guard.deref() else {
             return Err(CollectionError::NotFound {
-                what: "Peer does not have local shard".into(),
+                what: format!("local shard {}:{}", self.collection_id, self.shard_id),
             });
         };
 
-        local_shard.shard_recovery_point().await
-    }
+        let mut next_offset = Some(ExtendedPointId::NumId(0));
+        let mut ids = Vec::new();
 
-    /// Update the cutoff point for the local shard.
-    pub(crate) async fn update_shard_cutoff_point(
-        &self,
-        cutoff: &RecoveryPoint,
-    ) -> CollectionResult<()> {
-        let local_shard = self.local.read().await;
-        let Some(local_shard) = local_shard.as_ref() else {
-            return Err(CollectionError::NotFound {
-                what: "Peer does not have local shard".into(),
-            });
-        };
+        while let Some(current_offset) = next_offset {
+            const BATCH_SIZE: usize = 1000;
 
-        local_shard.update_cutoff(cutoff).await
-    }
+            let mut points = local_shard
+                .get()
+                .scroll_by(
+                    Some(current_offset),
+                    BATCH_SIZE + 1,
+                    &false.into(),
+                    &false.into(),
+                    Some(&filter),
+                    &self.search_runtime,
+                    None,
+                    None,
+                    hw_measurement_acc.clone(),
+                )
+                .await?;
 
-    pub(crate) fn get_snapshots_storage_manager(&self) -> CollectionResult<SnapshotStorageManager> {
-        SnapshotStorageManager::new(&self.shared_storage_config.snapshots_config)
-    }
+            if points.len() > BATCH_SIZE {
+                next_offset = points.pop().map(|points| points.id);
+            } else {
+                next_offset = None;
+            }
 
-    pub(crate) async fn trigger_optimizers(&self) -> bool {
-        let shard = self.local.read().await;
-        let Some(shard) = shard.as_ref() else {
-            return false;
-        };
-        shard.trigger_optimizers();
-        true
-    }
+            ids.extend(points.into_iter().map(|points| points.id));
+        }
 
-    /// Returns the estimated size of all local segments.
-    /// Since this locks all segments you should cache this value in performance critical scenarios!
-    pub(crate) async fn calculate_local_shard_stats(&self) -> Option<CollectionSizeStats> {
-        self.local
-            .read()
-            .await
-            .as_ref()
-            .map(|i| match i {
-                Shard::Local(local) => {
-                    let mut total_vector_size = 0;
-                    let mut total_payload_size = 0;
-                    let mut total_points = 0;
+        if ids.is_empty() {
+            return Ok(UpdateResult {
+                operation_id: None,
+                status: UpdateStatus::Completed,
+                clock_tag: None,
+            });
+        }
 
-                    for segment in local.segments.read().iter() {
-                        let size_info = segment.1.get().read().size_info();
-                        total_vector_size += size_info.vectors_size_bytes;
-                        total_payload_size += size_info.payloads_size_bytes;
-                        total_points += size_info.num_points;
-                    }
+        drop(local_shard_guard);
 
-                    Some(CollectionSizeStats {
-                        vector_storage_size: total_vector_size,
-                        payload_storage_size: total_payload_size,
-                        points_count: total_points,
-                    })
-                }
-                Shard::Proxy(_)
-                | Shard::ForwardProxy(_)
-                | Shard::QueueProxy(_)
-                | Shard::Dummy(_) => None,
-            })
-            .unwrap_or_default()
+        let op =
+            CollectionUpdateOperations::PointOperation(point_ops::PointOperations::DeletePoints {
+                ids,
+            });
+
+        // NOTE: We do not collect hardware measurements here since this is a shard cleanup
+        // operation, not a user request. It should be transparent on the HW metrics.
+        // TODO(resharding): Assign clock tag to the operation!? 🤔
+        let result = self
+            .update_local(op.into(), true, hw_measurement_acc, force)
+            .await?
+            .ok_or_else(|| {
+                CollectionError::bad_request(format!(
+                    "local shard {}:{} does not exist or is unavailable",
+                    self.collection_id, self.shard_id,
+                ))
+            })?;
+
+        Ok(result)
     }
 }
 
@@ -1196,6 +1160,21 @@ impl ReplicaSetState {
     pub fn set_peers(&mut self, peers: HashMap<PeerId, ReplicaState>) {
         self.peers = peers;
     }
+
+    /// Wait for a replica set state condition to be true. Returns true on timeout.
+    pub fn wait_for<F>(&self, check: F, timeout: Duration) -> bool
+    where
+        F: Fn(&ReplicaSetState) -> bool,
+    {
+        let start = std::time::Instant::now();
+        while start.elapsed() < timeout {
+            if check(self) {
+                return true;
+            }
+            std::thread::sleep(Duration::from_millis(50));
+        }
+        false
+    }
 }
 
 /// State of the single shard within a replica set.
