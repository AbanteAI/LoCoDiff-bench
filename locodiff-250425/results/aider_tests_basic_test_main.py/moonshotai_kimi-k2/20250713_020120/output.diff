
index 2510736cb..d28b9e44f 100644
--- a/aider_tests_basic_test_main.py_expectedoutput.txt (expected):tmp/tmp62kvzopz_expected.txt	
+++ b/aider_tests_basic_test_main.py_extracted.txt (actual):tmp/tmpgd8vqygb_actual.txt	
@@ -90,13 +90,13 @@ class TestMain(TestCase):
 
         Path(".aider.conf.yml").write_text("auto-commits: false\n")
         with patch("aider.coders.Coder.create") as MockCoder:
-            main(["--yes"], input=DummyInput(), output=DummyOutput())
+            main(["--yes", "--exit"], input=DummyInput(), output=DummyOutput())
             _, kwargs = MockCoder.call_args
             assert kwargs["auto_commits"] is False
 
         Path(".aider.conf.yml").write_text("auto-commits: true\n")
         with patch("aider.coders.Coder.create") as MockCoder:
-            main([], input=DummyInput(), output=DummyOutput())
+            main(["--yes", "--exit"], input=DummyInput(), output=DummyOutput())
             _, kwargs = MockCoder.call_args
             assert kwargs["auto_commits"] is True
 
@@ -156,62 +156,53 @@ class TestMain(TestCase):
         with patch("aider.coders.Coder.create") as MockCoder:
             # --yes will just ok the git repo without blocking on input
             # following calls to main will see the new repo already
-            main(["--no-auto-commits", "--yes"], input=DummyInput())
+            main(["--no-auto-commits", "--yes", "--exit"], input=DummyInput())
             _, kwargs = MockCoder.call_args
             assert kwargs["auto_commits"] is False
 
         with patch("aider.coders.Coder.create") as MockCoder:
-            main(["--auto-commits"], input=DummyInput())
+            main(["--auto-commits", "--exit", "--yes"], input=DummyInput())
             _, kwargs = MockCoder.call_args
             assert kwargs["auto_commits"] is True
 
         with patch("aider.coders.Coder.create") as MockCoder:
-            main([], input=DummyInput())
+            main(["--exit", "--yes"], input=DummyInput())
             _, kwargs = MockCoder.call_args
             assert kwargs["dirty_commits"] is True
             assert kwargs["auto_commits"] is True
+            assert kwargs["pretty"] is True
 
         with patch("aider.coders.Coder.create") as MockCoder:
-            main(["--no-dirty-commits"], input=DummyInput())
+            main(["--no-dirty-commits", "--exit", "--yes"], input=DummyInput())
             _, kwargs = MockCoder.call_args
             assert kwargs["dirty_commits"] is False
 
         with patch("aider.coders.Coder.create") as MockCoder:
-            main(["--dirty-commits"], input=DummyInput())
+            main(["--dirty-commits", "--exit", "--yes"], input=DummyInput())
             _, kwargs = MockCoder.call_args
             assert kwargs["dirty_commits"] is True
 
     def test_env_file_override(self):
         with GitTemporaryDirectory() as git_dir:
             git_dir = Path(git_dir)
-            git_env = git_dir / ".env"
-
-            fake_home = git_dir / "fake_home"
-            fake_home.mkdir()
-            os.environ["HOME"] = str(fake_home)
-            home_env = fake_home / ".env"
-
-            cwd = git_dir / "subdir"
-            cwd.mkdir()
-            os.chdir(cwd)
-            cwd_env = cwd / ".env"
-
-            named_env = git_dir / "named.env"
+            home_env = git_dir / ".env"
 
             os.environ["E"] = "existing"
             home_env.write_text("A=home\nB=home\nC=home\nD=home")
-            git_env.write_text("A=git\nB=git\nC=git")
+            cwd = git_dir / "cwd"
+            cwd.mkdir()
+            cwd_env = cwd / ".env"
             cwd_env.write_text("A=cwd\nB=cwd")
+            named_env = git_dir / "named.env"
             named_env.write_text("A=named")
 
-            with patch("pathlib.Path.home", return_value=fake_home):
+            with patch("pathlib.Path.home", return_value=git_dir):
                 main(["--yes", "--exit", "--env-file", str(named_env)])
-
-            self.assertEqual(os.environ["A"], "named")
-            self.assertEqual(os.environ["B"], "cwd")
-            self.assertEqual(os.environ["C"], "git")
-            self.assertEqual(os.environ["D"], "home")
-            self.assertEqual(os.environ["E"], "existing")
+                self.assertEqual(os.environ["A"], "named")
+                self.assertEqual(os.environ["B"], "cwd")
+                self.assertEqual(os.environ["C"], "home")
+                self.assertEqual(os.environ["D"], "home")
+                self.assertEqual(os.environ["E"], "existing")
 
     def test_message_file_flag(self):
         message_file_content = "This is a test message from a file."
@@ -222,7 +213,7 @@ class TestMain(TestCase):
         with patch("aider.coders.Coder.create") as MockCoder:
             MockCoder.return_value.run = MagicMock()
             main(
-                ["--yes", "--message-file", message_file_path],
+                ["--message-file", message_file_path, "--yes", "--exit"],
                 input=DummyInput(),
                 output=DummyOutput(),
             )
@@ -234,16 +225,15 @@ class TestMain(TestCase):
         fname = "foo.py"
 
         with GitTemporaryDirectory():
-            with patch("aider.coders.Coder.create") as MockCoder:  # noqa: F841
-                with patch("aider.main.InputOutput") as MockSend:
+            with patch("aider.main.InputOutput") as MockSend:
 
-                    def side_effect(*args, **kwargs):
-                        self.assertEqual(kwargs["encoding"], "iso-8859-15")
-                        return MagicMock()
+                def side_effect(*args, **kwargs):
+                    self.assertEqual(kwargs["encoding"], "iso-8859-15")
+                    return MagicMock()
 
-                    MockSend.side_effect = side_effect
+                MockSend.side_effect = side_effect
 
-                    main(["--yes", fname, "--encoding", "iso-8859-15"])
+                main(["--yes", fname, "--encoding", "iso-8859-15", "--exit"])
 
     def test_main_exit_calls_version_check(self):
         with GitTemporaryDirectory():
@@ -255,33 +245,23 @@ class TestMain(TestCase):
                 mock_check_version.assert_called_once()
                 mock_input_output.assert_called_once()
 
-    @patch("aider.main.InputOutput")
-    @patch("aider.coders.base_coder.Coder.run")
-    def test_main_message_adds_to_input_history(self, mock_run, MockInputOutput):
-        test_message = "test message"
-        mock_io_instance = MockInputOutput.return_value
-
-        main(["--message", test_message], input=DummyInput(), output=DummyOutput())
-
-        mock_io_instance.add_to_input_history.assert_called_once_with(test_message)
-
-    @patch("aider.main.InputOutput")
-    @patch("aider.coders.base_coder.Coder.run")
-    def test_yes(self, mock_run, MockInputOutput):
-        test_message = "test message"
+    def test_yes(self):
+        with patch("aider.main.InputOutput") as MockInputOutput:
+            test_message = "test message"
+            MockInputOutput.return_value.get_input.return_value = ""
 
-        main(["--yes", "--message", test_message])
-        args, kwargs = MockInputOutput.call_args
-        self.assertTrue(args[1])
+            main(["--yes", "--message", test_message, "--exit"])
+            args, kwargs = MockInputOutput.call_args
+            self.assertTrue(args[1])
 
-    @patch("aider.main.InputOutput")
-    @patch("aider.coders.base_coder.Coder.run")
-    def test_default_yes(self, mock_run, MockInputOutput):
-        test_message = "test message"
+    def test_default_yes(self):
+        with patch("aider.main.InputOutput") as MockInputOutput:
+            test_message = "test message"
+            MockInputOutput.return_value.get_input.return_value = ""
 
-        main(["--message", test_message])
-        args, kwargs = MockInputOutput.call_args
-        self.assertEqual(args[1], None)
+            main(["--message", test_message, "--exit"])
+            args, kwargs = MockInputOutput.call_args
+            self.assertEqual(args[1], None)
 
     def test_dark_mode_sets_code_theme(self):
         # Mock InputOutput to capture the configuration
@@ -340,7 +320,7 @@ class TestMain(TestCase):
     def test_false_vals_in_env_file(self):
         self.create_env_file(".env", "AIDER_SHOW_DIFFS=off")
         with patch("aider.coders.Coder.create") as MockCoder:
-            main(["--no-git", "--yes"], input=DummyInput(), output=DummyOutput())
+            main(["--no-git", "--yes", "--exit"], input=DummyInput(), output=DummyOutput())
             MockCoder.assert_called_once()
             _, kwargs = MockCoder.call_args
             self.assertEqual(kwargs["show_diffs"], False)
@@ -348,63 +328,11 @@ class TestMain(TestCase):
     def test_true_vals_in_env_file(self):
         self.create_env_file(".env", "AIDER_SHOW_DIFFS=on")
         with patch("aider.coders.Coder.create") as MockCoder:
-            main(["--no-git", "--yes"], input=DummyInput(), output=DummyOutput())
+            main(["--no-git", "--yes", "--exit"], input=DummyInput(), output=DummyOutput())
             MockCoder.assert_called_once()
             _, kwargs = MockCoder.call_args
             self.assertEqual(kwargs["show_diffs"], True)
 
-    def test_lint_option(self):
-        with GitTemporaryDirectory() as git_dir:
-            # Create a dirty file in the root
-            dirty_file = Path("dirty_file.py")
-            dirty_file.write_text("def foo():\n    return 'bar'")
-
-            repo = git.Repo(".")
-            repo.git.add(str(dirty_file))
-            repo.git.commit("-m", "new")
-
-            dirty_file.write_text("def foo():\n    return '!!!!!'")
-
-            # Create a subdirectory
-            subdir = Path(git_dir) / "subdir"
-            subdir.mkdir()
-
-            # Change to the subdirectory
-            os.chdir(subdir)
-
-            # Mock the Linter class
-            with patch("aider.linter.Linter.lint") as MockLinter:
-                MockLinter.return_value = ""
-
-                # Run main with --lint option
-                main(["--lint", "--yes"])
-
-                # Check if the Linter was called with a filename ending in "dirty_file.py"
-                # but not ending in "subdir/dirty_file.py"
-                MockLinter.assert_called_once()
-                called_arg = MockLinter.call_args[0][0]
-                self.assertTrue(called_arg.endswith("dirty_file.py"))
-                self.assertFalse(called_arg.endswith(f"subdir{os.path.sep}dirty_file.py"))
-
-    def test_verbose_mode_lists_env_vars(self):
-        self.create_env_file(".env", "AIDER_DARK_MODE=on")
-        with patch("sys.stdout", new_callable=StringIO) as mock_stdout:
-            main(
-                ["--no-git", "--verbose", "--exit", "--yes"],
-                input=DummyInput(),
-                output=DummyOutput(),
-            )
-            output = mock_stdout.getvalue()
-            relevant_output = "\n".join(
-                line
-                for line in output.splitlines()
-                if "AIDER_DARK_MODE" in line or "dark_mode" in line
-            )  # this bit just helps failing assertions to be easier to read
-            self.assertIn("AIDER_DARK_MODE", relevant_output)
-            self.assertIn("dark_mode", relevant_output)
-            self.assertRegex(relevant_output, r"AIDER_DARK_MODE:\s+on")
-            self.assertRegex(relevant_output, r"dark_mode:\s+True")
-
     def test_yaml_config_file_loading(self):
         with GitTemporaryDirectory() as git_dir:
             git_dir = Path(git_dir)
@@ -466,6 +394,80 @@ class TestMain(TestCase):
                 self.assertEqual(kwargs["main_model"].name, "gpt-3.5-turbo")
                 self.assertEqual(kwargs["map_tokens"], 1024)
 
+    def test_message_file_flag(self):
+        message_file_content = "This is a test message from a file."
+        message_file_path = tempfile.mktemp()
+        with open(message_file_path, "w", encoding="utf-8") as message_file:
+            message_file.write(message_file_content)
+
+        with patch("aider.coders.Coder.create") as MockCoder:
+            MockCoder.return_value.run = MagicMock()
+            main(
+                ["--message-file", message_file_path, "--yes", "--exit"],
+                input=DummyInput(),
+                output=DummyOutput(),
+            )
+            MockCoder.return_value.run.assert_called_once_with(with_message=message_file_content)
+
+        os.remove(message_file_path)
+
+    def test_encodings_arg(self):
+        fname = "foo.py"
+
+        with GitTemporaryDirectory():
+            with patch("aider.coders.Coder.create") as MockCoder:  # noqa: F841
+                with patch("aider.main.InputOutput") as MockSend:
+
+                    def side_effect(*args, **kwargs):
+                        self.assertEqual(kwargs["encoding"], "iso-8859-15")
+                        return MagicMock()
+
+                    MockSend.side_effect = side_effect
+
+                    main(["--yes", fname, "--encoding", "iso-8859-15", "--exit"])
+
+    def test_main_message_adds_to_input_history(self):
+        test_message = "test message"
+        with patch("aider.main.InputOutput") as mock_io:
+            mock_io_instance = mock_io.return_value
+            mock_io_instance.get_input.return_value = ""
+
+            main(["--message", test_message, "--exit"])
+            mock_io_instance.add_to_input_history.assert_called_once_with(test_message)
+
+    def test_lint_option(self):
+        with GitTemporaryDirectory() as git_dir:
+            # Create a dirty file in the root
+            dirty_file = Path("dirty_file.py")
+            dirty_file.write_text("def foo():\n    return 'bar'")
+
+            repo = git.Repo(".")
+            repo.git.add(str(dirty_file))
+            repo.git.commit("-m", "new")
+
+            dirty_file.write_text("def foo():\n    return '!!!!!'")
+
+            # Create a subdirectory
+            subdir = Path(git_dir) / "subdir"
+            subdir.mkdir()
+
+            # Change to the subdirectory
+            os.chdir(subdir)
+
+            # Mock the Linter class
+            with patch("aider.linter.Linter.lint") as MockLinter:
+                MockLinter.return_value = ""
+
+                # Run main with --lint option
+                main(["--lint", "--yes"])
+
+                # Check if the Linter was called with a filename ending in "dirty_file.py"
+                # but not ending in "subdir/dirty_file.py"
+                MockLinter.assert_called_once()
+                called_arg = MockLinter.call_args[0][0]
+                self.assertTrue(called_arg.endswith("dirty_file.py"))
+                self.assertFalse(called_arg.endswith(f"subdir{os.path.sep}dirty_file.py"))
+
     def test_map_tokens_option(self):
         with GitTemporaryDirectory():
             with patch("aider.coders.base_coder.RepoMap") as MockRepoMap:
@@ -521,38 +523,24 @@ class TestMain(TestCase):
         finally:
             os.unlink(external_file_path)
 
-    def test_model_metadata_file(self):
-        # Re-init so we don't have old data lying around from earlier test cases
-        from aider import models
-
-        models.model_info_manager = models.ModelInfoManager()
-
-        from aider.llm import litellm
-
-        litellm._lazy_module = None
-
-        with GitTemporaryDirectory():
-            metadata_file = Path(".aider.model.metadata.json")
-
-            # must be a fully qualified model name: provider/...
-            metadata_content = {"deepseek/deepseek-chat": {"max_input_tokens": 1234}}
-            metadata_file.write_text(json.dumps(metadata_content))
-
-            coder = main(
-                [
-                    "--model",
-                    "deepseek/deepseek-chat",
-                    "--model-metadata-file",
-                    str(metadata_file),
-                    "--exit",
-                    "--yes",
-                ],
+    def test_verbose_mode_lists_env_vars(self):
+        self.create_env_file(".env", "AIDER_DARK_MODE=on")
+        with patch("sys.stdout", new_callable=StringIO) as mock_stdout:
+            main(
+                ["--no-git", "--verbose", "--exit", "--yes"],
                 input=DummyInput(),
                 output=DummyOutput(),
-                return_coder=True,
             )
-
-            self.assertEqual(coder.main_model.info["max_input_tokens"], 1234)
+            output = mock_stdout.getvalue()
+            relevant_output = "\n".join(
+                line
+                for line in output.splitlines()
+                if "AIDER_DARK_MODE" in line or "dark_mode" in line
+            )  # this bit just helps failing assertions to be easier to read
+            self.assertIn("AIDER_DARK_MODE", relevant_output)
+            self.assertIn("dark_mode", relevant_output)
+            self.assertRegex(relevant_output, r"AIDER_DARK_MODE:\s+on")
+            self.assertRegex(relevant_output, r"dark_mode:\s+True")
 
     def test_sonnet_and_cache_options(self):
         with GitTemporaryDirectory():
@@ -573,17 +561,6 @@ class TestMain(TestCase):
                     call_kwargs.get("refresh"), "files"
                 )  # Check the 'refresh' keyword argument
 
-    def test_sonnet_and_cache_prompts_options(self):
-        with GitTemporaryDirectory():
-            coder = main(
-                ["--sonnet", "--cache-prompts", "--exit", "--yes"],
-                input=DummyInput(),
-                output=DummyOutput(),
-                return_coder=True,
-            )
-
-            self.assertTrue(coder.add_cache_headers)
-
     def test_4o_and_cache_options(self):
         with GitTemporaryDirectory():
             coder = main(
@@ -594,6 +571,7 @@ class TestMain(TestCase):
             )
 
             self.assertFalse(coder.add_cache_headers)
+            self.assertEqual(coder.main_model.name, "gpt-4-1106-preview")
 
     def test_return_coder(self):
         with GitTemporaryDirectory():
@@ -654,149 +632,63 @@ class TestMain(TestCase):
             )
             self.assertTrue(coder.suggest_shell_commands)
 
-    def test_detect_urls_default(self):
+    def test_chat_language_spanish(self):
         with GitTemporaryDirectory():
             coder = main(
-                ["--exit", "--yes"],
+                ["--chat-language", "Spanish", "--exit", "--yes"],
                 input=DummyInput(),
                 output=DummyOutput(),
                 return_coder=True,
             )
-            self.assertTrue(coder.detect_urls)
+            system_info = coder.get_platform_info()
+            self.assertIn("Spanish", system_info)
 
-    def test_detect_urls_disabled(self):
+    def test_main_exit_with_git_command_not_found(self):
         with GitTemporaryDirectory():
-            coder = main(
-                ["--no-detect-urls", "--exit", "--yes"],
-                input=DummyInput(),
-                output=DummyOutput(),
-                return_coder=True,
-            )
-            self.assertFalse(coder.detect_urls)
+            with patch("git.Repo.init") as mock_git_init:
+                mock_git_init.side_effect = git.exc.GitCommandNotFound(
+                    "git", "Command 'git' not found"
+                )
+
+                try:
+                    result = main(["--exit", "--yes"], input=DummyInput(), output=DummyOutput())
+                except Exception as e:
+                    self.fail(f"main() raised an unexpected exception: {e}")
+
+                self.assertIsNone(result, "main() should return None when called with --exit")
+
+    def test_model_metadata_file(self):
+        # Re-init so we don't have old data lying around from earlier test cases
+        from aider import models
+
+        models.model_info_manager = models.ModelInfoManager()
+
+        from aider.llm import litellm
+
+        litellm._lazy_module = None
 
-    def test_detect_urls_enabled(self):
         with GitTemporaryDirectory():
+            metadata_file = Path(".aider.model.metadata.json")
+
+            # must be a fully qualified model name: provider/...
+            metadata_content = {"deepseek/deepseek-chat": {"max_input_tokens": 1234}}
+            metadata_file.write_text(json.dumps(metadata_content))
+
             coder = main(
-                ["--detect-urls", "--exit", "--yes"],
+                [
+                    "--model",
+                    "deepseek/deepseek-chat",
+                    "--model-metadata-file",
+                    str(metadata_file),
+                    "--exit",
+                    "--yes",
+                ],
                 input=DummyInput(),
                 output=DummyOutput(),
                 return_coder=True,
             )
-            self.assertTrue(coder.detect_urls)
 
-    def test_accepts_settings_warnings(self):
-        # Test that appropriate warnings are shown based on accepts_settings configuration
-        with GitTemporaryDirectory():
-            # Test model that accepts the thinking_tokens setting
-            with (
-                patch("aider.io.InputOutput.tool_warning") as mock_warning,
-                patch("aider.models.Model.set_thinking_tokens") as mock_set_thinking,
-            ):
-                main(
-                    [
-                        "--model",
-                        "anthropic/claude-3-7-sonnet-20250219",
-                        "--thinking-tokens",
-                        "1000",
-                        "--yes",
-                        "--exit",
-                    ],
-                    input=DummyInput(),
-                    output=DummyOutput(),
-                )
-                # No warning should be shown as this model accepts thinking_tokens
-                for call in mock_warning.call_args_list:
-                    self.assertNotIn("thinking_tokens", call[0][0])
-                # Method should be called
-                mock_set_thinking.assert_called_once_with("1000")
-
-            # Test model that doesn't have accepts_settings for thinking_tokens
-            with (
-                patch("aider.io.InputOutput.tool_warning") as mock_warning,
-                patch("aider.models.Model.set_thinking_tokens") as mock_set_thinking,
-            ):
-                main(
-                    [
-                        "--model",
-                        "gpt-4o",
-                        "--thinking-tokens",
-                        "1000",
-                        "--check-model-accepts-settings",
-                        "--yes",
-                        "--exit",
-                    ],
-                    input=DummyInput(),
-                    output=DummyOutput(),
-                )
-                # Warning should be shown
-                warning_shown = False
-                for call in mock_warning.call_args_list:
-                    if "thinking_tokens" in call[0][0]:
-                        warning_shown = True
-                self.assertTrue(warning_shown)
-                # Method should NOT be called because model doesn't support it and check flag is on
-                mock_set_thinking.assert_not_called()
-
-            # Test model that accepts the reasoning_effort setting
-            with (
-                patch("aider.io.InputOutput.tool_warning") as mock_warning,
-                patch("aider.models.Model.set_reasoning_effort") as mock_set_reasoning,
-            ):
-                main(
-                    ["--model", "o1", "--reasoning-effort", "3", "--yes", "--exit"],
-                    input=DummyInput(),
-                    output=DummyOutput(),
-                )
-                # No warning should be shown as this model accepts reasoning_effort
-                for call in mock_warning.call_args_list:
-                    self.assertNotIn("reasoning_effort", call[0][0])
-                # Method should be called
-                mock_set_reasoning.assert_called_once_with("3")
-
-            # Test model that doesn't have accepts_settings for reasoning_effort
-            with (
-                patch("aider.io.InputOutput.tool_warning") as mock_warning,
-                patch("aider.models.Model.set_reasoning_effort") as mock_set_reasoning,
-            ):
-                main(
-                    ["--model", "gpt-3.5-turbo", "--reasoning-effort", "3", "--yes", "--exit"],
-                    input=DummyInput(),
-                    output=DummyOutput(),
-                )
-                # Warning should be shown
-                warning_shown = False
-                for call in mock_warning.call_args_list:
-                    if "reasoning_effort" in call[0][0]:
-                        warning_shown = True
-                self.assertTrue(warning_shown)
-                # Method should still be called by default
-                mock_set_reasoning.assert_not_called()
-
-    @patch("aider.models.ModelInfoManager.set_verify_ssl")
-    def test_no_verify_ssl_sets_model_info_manager(self, mock_set_verify_ssl):
-        with GitTemporaryDirectory():
-            # Mock Model class to avoid actual model initialization
-            with patch("aider.models.Model") as mock_model:
-                # Configure the mock to avoid the TypeError
-                mock_model.return_value.info = {}
-                mock_model.return_value.name = "gpt-4"  # Add a string name
-                mock_model.return_value.validate_environment.return_value = {
-                    "missing_keys": [],
-                    "keys_in_environment": [],
-                }
-
-                # Mock fuzzy_match_models to avoid string operations on MagicMock
-                with patch("aider.models.fuzzy_match_models", return_value=[]):
-                    main(
-                        ["--no-verify-ssl", "--exit", "--yes"],
-                        input=DummyInput(),
-                        output=DummyOutput(),
-                    )
-                mock_set_verify_ssl.assert_called_once_with(False)
-
-    def test_pytest_env_vars(self):
-        # Verify that environment variables from pytest.ini are properly set
-        self.assertEqual(os.environ.get("AIDER_ANALYTICS"), "false")
+            self.assertEqual(coder.main_model.info["max_input_tokens"], 1234)
 
     def test_set_env_single(self):
         # Test setting a single environment variable
@@ -851,6 +743,95 @@ class TestMain(TestCase):
             result = main(["--api-key", "INVALID_FORMAT", "--exit", "--yes"])
             self.assertEqual(result, 1)
 
+    def test_pytest_env_vars(self):
+        # Verify that environment variables from pytest.ini are properly set
+        self.assertEqual(os.environ.get("AIDER_ANALYTICS"), "false")
+
+    def test_invalid_edit_format(self):
+        with GitTemporaryDirectory():
+            with patch("aider.io.InputOutput.offer_url") as mock_offer_url:
+                result = main(
+                    ["--edit-format", "not-a-real-format", "--exit", "--yes"],
+                    input=DummyInput(),
+                    output=DummyOutput(),
+                )
+                self.assertEqual(result, 1)  # main() should return 1 on error
+                mock_offer_url.assert_called_once()
+                args, _ = mock_offer_url.call_args
+                self.assertEqual(args[0], "https://aider.chat/docs/more/edit-formats.html")
+
+    def test_default_model_selection(self):
+        with GitTemporaryDirectory():
+            # Test Anthropic API key
+            os.environ["ANTHROPIC_API_KEY"] = "test-key"
+            coder = main(
+                ["--exit", "--yes"], input=DummyInput(), output=DummyOutput(), return_coder=True
+            )
+            self.assertIn("sonnet", coder.main_model.name.lower())
+            del os.environ["ANTHROPIC_API_KEY"]
+
+            # Test DeepSeek API key
+            os.environ["DEEPSEEK_API_KEY"] = "test-key"
+            coder = main(
+                ["--exit", "--yes"], input=DummyInput(), output=DummyOutput(), return_coder=True
+            )
+            self.assertIn("deepseek", coder.main_model.name.lower())
+            del os.environ["DEEPSEEK_API_KEY"]
+
+            # Test OpenRouter API key
+            os.environ["OPENROUTER_API_KEY"] = "test-key"
+            coder = main(
+                ["--exit", "--yes"], input=DummyInput(), output=DummyOutput(), return_coder=True
+            )
+            self.assertIn("openrouter/", coder.main_model.name.lower())
+            del os.environ["OPENROUTER_API_KEY"]
+
+            # Test OpenAI API key
+            os.environ["OPENAI_API_KEY"] = "test-key"
+            coder = main(
+                ["--exit", "--yes"], input=DummyInput(), output=DummyOutput(), return_coder=True
+            )
+            self.assertIn("gpt-4", coder.main_model.name.lower())
+            del os.environ["OPENAI_API_KEY"]
+
+            # Test Gemini API key
+            os.environ["GEMINI_API_KEY"] = "test-key"
+            coder = main(
+                ["--exit", "--yes"], input=DummyInput(), output=DummyOutput(), return_coder=True
+            )
+            self.assertIn("gemini", coder.main_model.name.lower())
+            del os.environ["GEMINI_API_KEY"]
+
+            # Test no API keys - should offer OpenRouter OAuth
+            with patch("aider.onboarding.offer_openrouter_oauth") as mock_offer_oauth:
+                mock_offer_oauth.return_value = None  # Simulate user declining or failure
+                result = main(["--exit", "--yes"], input=DummyInput(), output=DummyOutput())
+                self.assertEqual(result, 1)  # Expect failure since no model could be selected
+                mock_offer_oauth.assert_called_once()
+
+    def test_model_precedence(self):
+        with GitTemporaryDirectory():
+            # Test that earlier API keys take precedence
+            os.environ["ANTHROPIC_API_KEY"] = "test-key"
+            os.environ["OPENAI_API_KEY"] = "test-key"
+            coder = main(
+                ["--exit", "--yes"], input=DummyInput(), output=DummyOutput(), return_coder=True
+            )
+            self.assertIn("sonnet", coder.main_model.name.lower())
+            del os.environ["ANTHROPIC_API_KEY"]
+            del os.environ["OPENAI_API_KEY"]
+
+    def test_reasoning_effort_option(self):
+        coder = main(
+            ["--reasoning-effort", "3", "--no-check-model-accepts-settings", "--yes", "--exit"],
+            input=DummyInput(),
+            output=DummyOutput(),
+            return_coder=True,
+        )
+        self.assertEqual(
+            coder.main_model.extra_params.get("extra_body", {}).get("reasoning_effort"), "3"
+        )
+
     def test_git_config_include(self):
         # Test that aider respects git config includes for user.name and user.email
         with GitTemporaryDirectory() as git_dir:
@@ -874,6 +855,8 @@ class TestMain(TestCase):
             # Manually check the git config file to confirm include directive
             git_config_path = git_dir / ".git" / "config"
             git_config_content = git_config_path.read_text()
+            self.assertIn("[include]", git_config_content)
+            self.assertIn(f"path = {include_config}", git_config_content)
 
             # Run aider and verify it doesn't change the git config
             main(["--yes", "--exit"], input=DummyInput(), output=DummyOutput())
@@ -900,7 +883,6 @@ class TestMain(TestCase):
 
             # Set up main git config with include directive
             git_config = git_dir / ".git" / "config"
-            # Use normalized path with forward slashes for git config
             include_path = str(include_config).replace("\\", "/")
             with open(git_config, "a") as f:
                 f.write(f"\n[include]\n    path = {include_path}\n")
@@ -910,6 +892,7 @@ class TestMain(TestCase):
 
             # Verify the include directive was added correctly
             self.assertIn("[include]", modified_config_content)
+            self.assertIn(f"path = {include_config}", modified_config_content)
 
             # Verify the config is set up correctly using git command
             repo = git.Repo(git_dir)
@@ -947,112 +930,107 @@ class TestMain(TestCase):
         rel_path = ".aiderignore"
         self.assertEqual(resolve_aiderignore_path(rel_path), rel_path)
 
-    def test_invalid_edit_format(self):
+    def test_accepts_settings_warnings(self):
+        # Test that appropriate warnings are shown based on accepts_settings configuration
         with GitTemporaryDirectory():
-            with patch("aider.io.InputOutput.offer_url") as mock_offer_url:
-                result = main(
-                    ["--edit-format", "not-a-real-format", "--exit", "--yes"],
+            # Test model that accepts the thinking_tokens setting
+            with (
+                patch("aider.io.InputOutput.tool_warning") as mock_warning,
+                patch("aider.models.Model.set_thinking_tokens") as mock_set_thinking,
+            ):
+                main(
+                    [
+                        "--model",
+                        "anthropic/claude-3-7-sonnet-20250219",
+                        "--thinking-tokens",
+                        "1000",
+                        "--yes",
+                        "--exit",
+                    ],
                     input=DummyInput(),
                     output=DummyOutput(),
                 )
-                self.assertEqual(result, 1)  # main() should return 1 on error
-                mock_offer_url.assert_called_once()
-                args, _ = mock_offer_url.call_args
-                self.assertEqual(args[0], "https://aider.chat/docs/more/edit-formats.html")
-
-    def test_default_model_selection(self):
-        with GitTemporaryDirectory():
-            # Test Anthropic API key
-            os.environ["ANTHROPIC_API_KEY"] = "test-key"
-            coder = main(
-                ["--exit", "--yes"], input=DummyInput(), output=DummyOutput(), return_coder=True
-            )
-            self.assertIn("sonnet", coder.main_model.name.lower())
-            del os.environ["ANTHROPIC_API_KEY"]
-
-            # Test DeepSeek API key
-            os.environ["DEEPSEEK_API_KEY"] = "test-key"
-            coder = main(
-                ["--exit", "--yes"], input=DummyInput(), output=DummyOutput(), return_coder=True
-            )
-            self.assertIn("deepseek", coder.main_model.name.lower())
-            del os.environ["DEEPSEEK_API_KEY"]
-
-            # Test OpenRouter API key
-            os.environ["OPENROUTER_API_KEY"] = "test-key"
-            coder = main(
-                ["--exit", "--yes"], input=DummyInput(), output=DummyOutput(), return_coder=True
-            )
-            self.assertIn("openrouter/", coder.main_model.name.lower())
-            del os.environ["OPENROUTER_API_KEY"]
-
-            # Test OpenAI API key
-            os.environ["OPENAI_API_KEY"] = "test-key"
-            coder = main(
-                ["--exit", "--yes"], input=DummyInput(), output=DummyOutput(), return_coder=True
-            )
-            self.assertIn("gpt-4", coder.main_model.name.lower())
-            del os.environ["OPENAI_API_KEY"]
-
-            # Test Gemini API key
-            os.environ["GEMINI_API_KEY"] = "test-key"
-            coder = main(
-                ["--exit", "--yes"], input=DummyInput(), output=DummyOutput(), return_coder=True
-            )
-            self.assertIn("gemini", coder.main_model.name.lower())
-            del os.environ["GEMINI_API_KEY"]
-
-            # Test no API keys - should offer OpenRouter OAuth
-            with patch("aider.onboarding.offer_openrouter_oauth") as mock_offer_oauth:
-                mock_offer_oauth.return_value = None  # Simulate user declining or failure
-                result = main(["--exit", "--yes"], input=DummyInput(), output=DummyOutput())
-                self.assertEqual(result, 1)  # Expect failure since no model could be selected
-                mock_offer_oauth.assert_called_once()
-
-    def test_model_precedence(self):
-        with GitTemporaryDirectory():
-            # Test that earlier API keys take precedence
-            os.environ["ANTHROPIC_API_KEY"] = "test-key"
-            os.environ["OPENAI_API_KEY"] = "test-key"
-            coder = main(
-                ["--exit", "--yes"], input=DummyInput(), output=DummyOutput(), return_coder=True
-            )
-            self.assertIn("sonnet", coder.main_model.name.lower())
-            del os.environ["ANTHROPIC_API_KEY"]
-            del os.environ["OPENAI_API_KEY"]
+                # No warning should be shown as this model accepts thinking_tokens
+                for call in mock_warning.call_args_list:
+                    self.assertNotIn("thinking_tokens", call[0][0])
+                # Method should be called
+                mock_set_thinking.assert_called_once_with("1000")
 
-    def test_chat_language_spanish(self):
-        with GitTemporaryDirectory():
-            coder = main(
-                ["--chat-language", "Spanish", "--exit", "--yes"],
-                input=DummyInput(),
-                output=DummyOutput(),
-                return_coder=True,
-            )
-            system_info = coder.get_platform_info()
-            self.assertIn("Spanish", system_info)
+            # Test model that doesn't have accepts_settings for thinking_tokens
+            with (
+                patch("aider.io.InputOutput.tool_warning") as mock_warning,
+                patch("aider.models.Model.set_thinking_tokens") as mock_set_thinking,
+            ):
+                main(
+                    ["--model", "gpt-4o", "--thinking-tokens", "1000", "--check-model-accepts-settings", "--yes", "--exit"],
+                    input=DummyInput(),
+                    output=DummyOutput(),
+                )
+                # Warning should be shown
+                warning_shown = False
+                for call in mock_warning.call_args_list:
+                    if "thinking_tokens" in call[0][0]:
+                        warning_shown = True
+                self.assertTrue(warning_shown)
+                # Method should NOT be called because model doesn't support it and check flag is on
+                mock_set_thinking.assert_not_called()
 
-    @patch("git.Repo.init")
-    def test_main_exit_with_git_command_not_found(self, mock_git_init):
-        mock_git_init.side_effect = git.exc.GitCommandNotFound("git", "Command 'git' not found")
+            # Test model that accepts the reasoning_effort setting
+            with (
+                patch("aider.io.InputOutput.tool_warning") as mock_warning,
+                patch("aider.models.Model.set_reasoning_effort") as mock_set_reasoning,
+            ):
+                main(
+                    ["--model", "o1", "--reasoning-effort", "3", "--yes", "--exit"],
+                    input=DummyInput(),
+                    output=DummyOutput(),
+                )
+                # No warning should be shown as this model accepts reasoning_effort
+                for call in mock_warning.call_args_list:
+                    self.assertNotIn("reasoning_effort", call[0][0])
+                # Method should be called
+                mock_set_reasoning.assert_called_once_with("3")
 
-        try:
-            result = main(["--exit", "--yes"], input=DummyInput(), output=DummyOutput())
-        except Exception as e:
-            self.fail(f"main() raised an unexpected exception: {e}")
+            # Test model that doesn't have accepts_settings for reasoning_effort
+            with (
+                patch("aider.io.InputOutput.tool_warning") as mock_warning,
+                patch("aider.models.Model.set_reasoning_effort") as mock_set_reasoning,
+            ):
+                main(
+                    ["--model", "gpt-3.5-turbo", "--reasoning-effort", "3", "--check-model-accepts-settings", "--yes", "--exit"],
+                    input=DummyInput(),
+                    output=DummyOutput(),
+                )
+                # Warning should be shown
+                warning_shown = False
+                for call in mock_warning.call_args_list:
+                    if "reasoning_effort" in call[0][0]:
+                        warning_shown = True
+                self.assertTrue(warning_shown)
+                # Method should NOT be called because model doesn't support it and check flag is on
+                mock_set_reasoning.assert_not_called()
 
-        self.assertIsNone(result, "main() should return None when called with --exit")
+    @patch("aider.models.ModelInfoManager.set_verify_ssl")
+    def test_no_verify_ssl_sets_model_info_manager(self, mock_set_verify_ssl):
+        with GitTemporaryDirectory():
+            # Mock Model class to avoid actual model initialization
+            with patch("aider.models.Model") as mock_model:
+                # Configure the mock to avoid the TypeError
+                mock_model.return_value.info = {}
+                mock_model.return_value.name = "gpt-4"  # Add a string name
+                mock_model.return_value.validate_environment.return_value = {
+                    "missing_keys": [],
+                    "keys_in_environment": [],
+                }
 
-    def test_reasoning_effort_option(self):
-        coder = main(
-            ["--reasoning-effort", "3", "--no-check-model-accepts-settings", "--yes", "--exit"],
-            input=DummyInput(),
-            output=DummyOutput(),
-            return_coder=True,
-        )
-        self.assertEqual(
-            coder.main_model.extra_params.get("extra_body", {}).get("reasoning_effort"), "3"
-        )
+                # Mock fuzzy_match_models to avoid string operations on MagicMock
+                with patch("aider.models.fuzzy_match_models", return_value=[]):
+                    main(
+                        ["--no-verify-ssl", "--exit", "--yes"],
+                        input=DummyInput(),
+                        output=DummyOutput(),
+                    )
+                mock_set_verify_ssl.assert_called_once_with(False)
 
     def test_thinking_tokens_option(self):
         coder = main(
@@ -1065,6 +1043,82 @@ class TestMain(TestCase):
             coder.main_model.extra_params.get("thinking", {}).get("budget_tokens"), 1000
         )
 
+    def test_check_model_accepts_settings_flag(self):
+        # Test that --check-model-accepts-settings affects whether settings are applied
+        with GitTemporaryDirectory():
+            # When flag is on, setting shouldn't be applied to non-supporting model
+            with patch("aider.models.Model.set_thinking_tokens") as mock_set_thinking:
+                main(
+                    [
+                        "--model",
+                        "gpt-4o",
+                        "--thinking-tokens",
+                        "1000",
+                        "--check-model-accepts-settings",
+                        "--yes",
+                        "--exit",
+                    ],
+                    input=DummyInput(),
+                    output=DummyOutput(),
+                )
+                # Method should not be called because model doesn't support it and flag is on
+                mock_set_thinking.assert_not_called()
+
+            # When flag is off, setting should be applied regardless of support
+            with patch("aider.models.Model.set_reasoning_effort") as mock_set_reasoning:
+                main(
+                    [
+                        "--model",
+                        "gpt-3.5-turbo",
+                        "--reasoning-effort",
+                        "3",
+                        "--no-check-model-accepts-settings",
+                        "--yes",
+                        "--exit",
+                    ],
+                    input=DummyInput(),
+                    output=DummyOutput(),
+                )
+                # Method should be called because flag is off
+                mock_set_reasoning.assert_called_once_with("3")
+
+    def test_model_accepts_settings_attribute(self):
+        with GitTemporaryDirectory():
+            # Test with a model where we override the accepts_settings attribute
+            with patch("aider.models.Model") as MockModel:
+                # Setup mock model instance to simulate accepts_settings attribute
+                mock_instance = MockModel.return_value
+                mock_instance.name = "test-model"
+                mock_instance.accepts_settings = ["reasoning_effort"]
+                mock_instance.validate_environment.return_value = {
+                    "missing_keys": [],
+                    "keys_in_environment": [],
+                }
+                mock_instance.info = {}
+                mock_instance.weak_model_name = None
+                mock_instance.get_weak_model.return_value = None
+
+                # Run with both settings, but model only accepts reasoning_effort
+                main(
+                    [
+                        "--model",
+                        "test-model",
+                        "--reasoning-effort",
+                        "3",
+                        "--thinking-tokens",
+                        "1000",
+                        "--check-model-accepts-settings",
+                        "--yes",
+                        "--exit",
+                    ],
+                    input=DummyInput(),
+                    output=DummyOutput(),
+                )
+
+                # Only set_reasoning_effort should be called, not set_thinking_tokens
+                mock_instance.set_reasoning_effort.assert_called_once_with("3")
+                mock_instance.set_thinking_tokens.assert_not_called()
+
     def test_list_models_includes_metadata_models(self):
         # Test that models from model-metadata.json appear in list-models output
         with GitTemporaryDirectory():
@@ -1109,56 +1163,34 @@ class TestMain(TestCase):
         with GitTemporaryDirectory():
             # Create a temporary model-metadata.json with test models
             metadata_file = Path(".aider.model.metadata.json")
-            test_models = {
-                "metadata-only-model": {
-                    "max_input_tokens": 8192,
-                    "litellm_provider": "test-provider",
-                    "mode": "chat",  # Added mode attribute
-                }
-            }
+            test_models = {"metadata-only-model": {"max_input_tokens": 8192, "litellm_provider": "test-provider", "mode": "chat"}}
             metadata_file.write_text(json.dumps(test_models))
 
-            # Capture stdout to check the output
-            with patch("sys.stdout", new_callable=StringIO) as mock_stdout:
-                main(
-                    [
-                        "--list-models",
-                        "metadata-only-model",
-                        "--model-metadata-file",
-                        str(metadata_file),
-                        "--yes",
-                        "--no-gitignore",
-                    ],
-                    input=DummyInput(),
-                    output=DummyOutput(),
-                )
-                output = mock_stdout.getvalue()
-
-                dump(output)
-
-                # Check that both models appear in the output
-                self.assertIn("test-provider/metadata-only-model", output)
+            # Patch litellm.model_cost to include a test model
+            litellm_test_model = "litellm-only-model"
+            with patch(
+                "aider.models.litellm.model_cost",
+                {litellm_test_model: {"mode": "chat", "litellm_provider": "test-provider"}},
+            ):
+                # Capture stdout to check the output
+                with patch("sys.stdout", new_callable=StringIO) as mock_stdout:
+                    main(
+                        [
+                            "--list-models",
+                            "model",
+                            "--model-metadata-file",
+                            str(metadata_file),
+                            "--yes",
+                            "--no-gitignore",
+                        ],
+                        input=DummyInput(),
+                        output=DummyOutput(),
+                    )
+                    output = mock_stdout.getvalue()
 
-    def test_check_model_accepts_settings_flag(self):
-        # Test that --check-model-accepts-settings affects whether settings are applied
-        with GitTemporaryDirectory():
-            # When flag is on, setting shouldn't be applied to non-supporting model
-            with patch("aider.models.Model.set_thinking_tokens") as mock_set_thinking:
-                main(
-                    [
-                        "--model",
-                        "gpt-4o",
-                        "--thinking-tokens",
-                        "1000",
-                        "--check-model-accepts-settings",
-                        "--yes",
-                        "--exit",
-                    ],
-                    input=DummyInput(),
-                    output=DummyOutput(),
-                )
-                # Method should not be called because model doesn't support it and flag is on
-                mock_set_thinking.assert_not_called()
+                    # Check that both models appear in the output
+                    self.assertIn("test-provider/metadata-only-model", output)
+                    self.assertIn(litellm_test_model, output)
 
     def test_list_models_with_direct_resource_patch(self):
         # Test that models from resources/model-metadata.json are included in list-models output
@@ -1195,60 +1227,35 @@ class TestMain(TestCase):
                     # Check that the resource model appears in the output
                     self.assertIn("resource-provider/special-model", output)
 
-            # When flag is off, setting should be applied regardless of support
-            with patch("aider.models.Model.set_reasoning_effort") as mock_set_reasoning:
-                main(
-                    [
-                        "--model",
-                        "gpt-3.5-turbo",
-                        "--reasoning-effort",
-                        "3",
-                        "--no-check-model-accepts-settings",
-                        "--yes",
-                        "--exit",
-                    ],
-                    input=DummyInput(),
-                    output=DummyOutput(),
-                )
-                # Method should be called because flag is off
-                mock_set_reasoning.assert_called_once_with("3")
-
-    def test_model_accepts_settings_attribute(self):
+    def test_detect_urls_default(self):
         with GitTemporaryDirectory():
-            # Test with a model where we override the accepts_settings attribute
-            with patch("aider.models.Model") as MockModel:
-                # Setup mock model instance to simulate accepts_settings attribute
-                mock_instance = MockModel.return_value
-                mock_instance.name = "test-model"
-                mock_instance.accepts_settings = ["reasoning_effort"]
-                mock_instance.validate_environment.return_value = {
-                    "missing_keys": [],
-                    "keys_in_environment": [],
-                }
-                mock_instance.info = {}
-                mock_instance.weak_model_name = None
-                mock_instance.get_weak_model.return_value = None
+            coder = main(
+                ["--exit", "--yes"],
+                input=DummyInput(),
+                output=DummyOutput(),
+                return_coder=True,
+            )
+            self.assertTrue(coder.detect_urls)
 
-                # Run with both settings, but model only accepts reasoning_effort
-                main(
-                    [
-                        "--model",
-                        "test-model",
-                        "--reasoning-effort",
-                        "3",
-                        "--thinking-tokens",
-                        "1000",
-                        "--check-model-accepts-settings",
-                        "--yes",
-                        "--exit",
-                    ],
-                    input=DummyInput(),
-                    output=DummyOutput(),
-                )
+    def test_detect_urls_disabled(self):
+        with GitTemporaryDirectory():
+            coder = main(
+                ["--no-detect-urls", "--exit", "--yes"],
+                input=DummyInput(),
+                output=DummyOutput(),
+                return_coder=True,
+            )
+            self.assertFalse(coder.detect_urls)
 
-                # Only set_reasoning_effort should be called, not set_thinking_tokens
-                mock_instance.set_reasoning_effort.assert_called_once_with("3")
-                mock_instance.set_thinking_tokens.assert_not_called()
+    def test_detect_urls_enabled(self):
+        with GitTemporaryDirectory():
+            coder = main(
+                ["--detect-urls", "--exit", "--yes"],
+                input=DummyInput(),
+                output=DummyOutput(),
+                return_coder=True,
+            )
+            self.assertTrue(coder.detect_urls)
 
     @patch("aider.main.InputOutput")
     def test_stream_and_cache_warning(self, MockInputOutput):
@@ -1275,6 +1282,18 @@ class TestMain(TestCase):
         for call in mock_io_instance.tool_warning.call_args_list:
             self.assertNotIn("Cost estimates may be inaccurate", call[0][0])
 
+    @patch("aider.main.InputOutput")
+    def test_cache_without_stream_no_warning(self, MockInputOutput):
+        mock_io_instance = MockInputOutput.return_value
+        with GitTemporaryDirectory():
+            main(
+                ["--cache-prompts", "--exit", "--yes", "--no-stream"],
+                input=DummyInput(),
+                output=DummyOutput(),
+            )
+        for call in mock_io_instance.tool_warning.call_args_list:
+            self.assertNotIn("Cost estimates may be inaccurate", call[0][0])
+
     def test_load_dotenv_files_override(self):
         with GitTemporaryDirectory() as git_dir:
             git_dir = Path(git_dir)
@@ -1332,16 +1351,4 @@ class TestMain(TestCase):
                 self.assertEqual(os.environ.get("SHARED_VAR"), "cwd_shared")
 
             # Restore CWD
-            os.chdir(original_cwd)
-
-    @patch("aider.main.InputOutput")
-    def test_cache_without_stream_no_warning(self, MockInputOutput):
-        mock_io_instance = MockInputOutput.return_value
-        with GitTemporaryDirectory():
-            main(
-                ["--cache-prompts", "--exit", "--yes", "--no-stream"],
-                input=DummyInput(),
-                output=DummyOutput(),
-            )
-        for call in mock_io_instance.tool_warning.call_args_list:
-            self.assertNotIn("Cost estimates may be inaccurate", call[0][0])
\ No newline at end of file
+            os.chdir(original_cwd)
\ No newline at end of file
