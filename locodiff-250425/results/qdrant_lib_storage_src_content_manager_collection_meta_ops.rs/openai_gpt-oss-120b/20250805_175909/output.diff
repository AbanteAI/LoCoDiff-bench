
index 7008d3e69..77e44e7c4 100644
--- a/qdrant_lib_storage_src_content_manager_collection_meta_ops.rs_expectedoutput.txt (expected):tmp/tmpduq9v1wi_expected.txt	
+++ b/qdrant_lib_storage_src_content_manager_collection_meta_ops.rs_extracted.txt (actual):tmp/tmp51erfyam_actual.txt	
@@ -22,13 +22,12 @@ use serde::{Deserialize, Serialize};
 use uuid::Uuid;
 use validator::Validate;
 
-use crate::content_manager::errors::{StorageError, StorageResult};
 use crate::content_manager::shard_distribution::ShardDistributionProposal;
+use crate::content_manager::errors::{StorageError, StorageResult};
 
-// *Operation wrapper structure is only required for better OpenAPI generation
+/// Operation wrapper structure is only required for better OpenAPI generation
 
 /// Create alternative name for a collection.
-/// Collection will be available under both names for search, retrieve,
 #[derive(Debug, Deserialize, Serialize, JsonSchema, PartialEq, Eq, Hash, Clone)]
 #[serde(rename_all = "snake_case")]
 pub struct CreateAlias {
@@ -42,21 +41,20 @@ pub struct CreateAliasOperation {
     pub create_alias: CreateAlias,
 }
 
-/// Delete alias if exists
+/// Delete alias if exists.
 #[derive(Debug, Deserialize, Serialize, JsonSchema, PartialEq, Eq, Hash, Clone)]
 #[serde(rename_all = "snake_case")]
 pub struct DeleteAlias {
     pub alias_name: String,
 }
 
-/// Delete alias if exists
 #[derive(Debug, Deserialize, Serialize, JsonSchema, PartialEq, Eq, Hash, Clone)]
 #[serde(rename_all = "snake_case")]
 pub struct DeleteAliasOperation {
     pub delete_alias: DeleteAlias,
 }
 
-/// Change alias to a new one
+/// Change alias to a new one.
 #[derive(Debug, Deserialize, Serialize, JsonSchema, PartialEq, Eq, Hash, Clone)]
 #[serde(rename_all = "snake_case")]
 pub struct RenameAlias {
@@ -64,14 +62,13 @@ pub struct RenameAlias {
     pub new_alias_name: String,
 }
 
-/// Change alias to a new one
 #[derive(Debug, Deserialize, Serialize, JsonSchema, PartialEq, Eq, Hash, Clone)]
 #[serde(rename_all = "snake_case")]
 pub struct RenameAliasOperation {
     pub rename_alias: RenameAlias,
 }
 
-/// Group of all the possible operations related to collection aliases
+/// Group of all possible operations related to collection aliases.
 #[derive(Debug, Deserialize, Serialize, JsonSchema, PartialEq, Eq, Hash, Clone)]
 #[serde(rename_all = "snake_case")]
 #[serde(untagged)]
@@ -86,103 +83,92 @@ impl From<CreateAlias> for AliasOperations {
         AliasOperations::CreateAlias(CreateAliasOperation { create_alias })
     }
 }
-
 impl From<DeleteAlias> for AliasOperations {
     fn from(delete_alias: DeleteAlias) -> Self {
-        AliasOperations::DeleteAlias(DeleteAliasOperation { delete_alias })
+        AliasOperations::Delete(DeleteAliasOperation { delete_alias })
     }
 }
-
 impl From<RenameAlias> for AliasOperations {
     fn from(rename_alias: RenameAlias) -> Self {
-        AliasOperations::RenameAlias(RenameAliasOperation { rename_alias })
+        AliasOperations::Rename(RenameAliasOperation { rename_alias })
     }
 }
 
-/// Operation for creating new collection and (optionally) specify index params
+/// Used for creating a collection from an existing collection.
 #[derive(Debug, Deserialize, Serialize, JsonSchema, PartialEq, Eq, Hash, Clone)]
-#[serde(rename_all = "snake_case")]
 pub struct InitFrom {
     pub collection: CollectionId,
 }
 
-/// Operation for creating new collection and (optionally) specify index params
+/// Create a new collection.
 #[derive(Debug, Deserialize, Serialize, JsonSchema, Validate, PartialEq, Eq, Hash, Clone)]
 #[serde(rename_all = "snake_case")]
 pub struct CreateCollection {
-    /// Vector data config.
-    /// It is possible to provide one config for single vector mode and list of configs for multiple vectors mode.
+    /// Vector data configuration.
     #[serde(default)]
     #[validate(nested)]
     pub vectors: VectorsConfig,
-    /// For auto sharding:
-    /// Number of shards in collection.
-    ///  - Default is 1 for standalone, otherwise equal to the number of nodes
-    ///  - Minimum is 1
+    /// For auto-sharding:
+    ///
+    /// Number of shards in the collection.
+    ///   - Default is 1 for standalone, otherwise equal to the number of nodes.
+    ///   - Minimum is 1
     ///
     /// For custom sharding:
-    /// Number of shards in collection per shard group.
-    ///  - Default is 1, meaning that each shard key will be mapped to a single shard
-    ///  - Minimum is 1
+    ///
+    /// Number of shards in the collection per shard group.
+    ///   - Default is 1, meaning that each shard key will be mapped to a single shard.
+    ///   - Minimum is 1
     #[serde(default)]
     #[validate(range(min = 1))]
     pub shard_number: Option<u32>,
-    /// Sharding method
-    /// Default is Auto - points are distributed across all available shards
-    /// Custom - points are distributed across shards according to shard key
+    /// Sharding method.
     #[serde(default)]
     pub sharding_method: Option<ShardingMethod>,
     /// Number of shards replicas.
-    /// Default is 1
-    /// Minimum is 1
+    /// Default is 1.
+    /// Minimum is 1.
     #[serde(default)]
     #[validate(range(min = 1))]
     pub replication_factor: Option<u32>,
-    /// Defines how many replicas should apply the operation for us to consider it successful.
-    /// Increasing this number will make the collection more resilient to inconsistencies, but will
-    /// also make it fail if not enough replicas are available.
-    /// Does not have any performance impact.
+    /// Number of replicas that must apply the operation for it to be considered successful.
     #[serde(default)]
-    #[validate(range(min = 1))]
+    #[range(min = 1)]
     pub write_consistency_factor: Option<u32>,
-    /// If true - point's payload will not be stored in memory.
-    /// It will be read from the disk every time it is requested.
-    /// This setting saves RAM by (slightly) increasing the response time.
-    /// Note: those payload values that are involved in filtering and are indexed - remain in RAM.
-    ///
-    /// Default: true
+    /// If `true` the point's payload is stored on disk.
+    /// Default: true.
     #[serde(default)]
     pub on_disk_payload: Option<bool>,
-    /// Custom params for HNSW index. If none - values from service configuration file are used.
+    /// Custom HNSW configuration.
     #[validate(nested)]
     pub hnsw_config: Option<HnswConfigDiff>,
-    /// Custom params for WAL. If none - values from service configuration file are used.
+    /// Custom WAL configuration.
     #[validate(nested)]
     pub wal_config: Option<WalConfigDiff>,
-    /// Custom params for Optimizers.  If none - values from service configuration file are used.
+    /// Optimizer configuration.
     #[serde(alias = "optimizer_config")]
     #[validate(nested)]
     pub optimizers_config: Option<OptimizersConfigDiff>,
     /// Specify other collection to copy data from.
     #[serde(default)]
     pub init_from: Option<InitFrom>,
-    /// Quantization parameters. If none - quantization is disabled.
+    /// Quantisation configuration.
     #[serde(default, alias = "quantization")]
     #[validate(nested)]
     pub quantization_config: Option<QuantizationConfig>,
-    /// Sparse vector data config.
+    /// Sparse vector data configuration.
     #[validate(nested)]
     pub sparse_vectors: Option<BTreeMap<VectorNameBuf, SparseVectorParams>>,
-    /// Strict-mode config.
+    /// Strict-mode configuration.
     #[validate(nested)]
+    #[schemars(skip)]
     pub strict_mode_config: Option<StrictModeConfig>,
     #[serde(default)]
     #[schemars(skip)]
     pub uuid: Option<Uuid>,
 }
 
-/// Operation for creating new collection and (optionally) specify index params
-#[derive(Debug, Deserialize, Serialize, PartialEq, Eq, Hash, Clone)]
+#[derive(Debug, Deserialize, Serialize, JsonSchema, PartialEq, Eq, Hash, Clone)]
 #[serde(rename_all = "snake_case")]
 pub struct CreateCollectionOperation {
     pub collection_name: String,
@@ -191,21 +177,24 @@ pub struct CreateCollectionOperation {
 }
 
 impl CreateCollectionOperation {
+    /// Creates a new `CreateCollectionOperation`.
+    ///
+    /// # Errors
+    ///
+    /// Returns an error if dense and sparse vector names conflict.
     pub fn new(
         collection_name: String,
         create_collection: CreateCollection,
     ) -> StorageResult<Self> {
-        // validate vector names are unique between dense and sparse vectors
-        if let Some(sparse_config) = &create_collection.sparse_vectors {
-            let mut dense_names = create_collection.vectors.params_iter().map(|p| p.0);
-            if let Some(duplicate_name) = dense_names.find(|name| sparse_config.contains_key(*name))
-            {
+        // Ensure dense and sparse vector names are unique.
+        if let Some(sparse) = &create_collection.sparse_vectors {
+            let dense_names = create_collection.vectors.params_iter().map(|p| p.0);
+            if let Some(duplicate) = dense_names.find(|n| sparse.contains_key(*n)) {
                 return Err(StorageError::bad_input(format!(
-                    "Dense and sparse vector names must be unique - duplicate found with '{duplicate_name}'",
+                    "Dense and sparse vector names must be unique - duplicate found with '{duplicate}'"
                 )));
             }
         }
-
         Ok(Self {
             collection_name,
             create_collection,
@@ -226,35 +215,75 @@ impl CreateCollectionOperation {
     }
 }
 
-/// Operation for updating parameters of the existing collection
+/// Use config of an existing collection to create a new collection.
+impl From<CollectionConfigInternal> for CreateCollection {
+    fn from(value: CollectionConfigInternal) -> Self {
+        let CollectionConfigInternal {
+            params:
+                CollectionParams {
+                    vectors,
+                    shard_number,
+                    sharding_method,
+                    replication_factor,
+                    write_consistency_factor,
+                    read_fan_out_factor: _,
+                    on_disk_payload,
+                    sparse_vectors,
+                },
+                hnsw_config,
+                optimizer_config,
+                wal_config,
+                quantization_config,
+                strict_mode_config,
+                uuid,
+            } = value;
+
+            Self {
+                vectors,
+                shard_number: Some(shard_number.get()),
+                sharding_method,
+                replication_factor: Some(replication_factor.get()),
+                write_consistency_factor: Some(write_consistency_factor.get()),
+                on_disk_payload: Some(on_disk_payload),
+                hnsw_config: Some(hnsw_config.into()),
+                wal_config: Some(wal_config.into()),
+                optimizers_config: Some(optimizer_config.into()),
+                init_from: None,
+                quantization_config,
+                sparse_vectors,
+                strict_mode_config,
+                uuid,
+            }
+        }
+    }
+
+/// Update collection parameters.
 #[derive(Debug, Deserialize, Serialize, JsonSchema, Validate, PartialEq, Eq, Hash, Clone)]
 #[serde(rename_all = "snake_case")]
 pub struct UpdateCollection {
-    /// Map of vector data parameters to update for each named vector.
-    /// To update parameters in a collection having a single unnamed vector, use an empty string as name.
+    /// Vector parameters to update.
     #[validate(nested)]
     pub vectors: Option<VectorsConfigDiff>,
-    /// Custom params for Optimizers.  If none - it is left unchanged.
-    /// This operation is blocking, it will only proceed once all current optimizations are complete
+    /// Optimizer configuration.
     #[serde(alias = "optimizer_config")]
-    pub optimizers_config: Option<OptimizersConfigDiff>, // TODO: Allow updates for other configuration params as well
-    /// Collection base params. If none - it is left unchanged.
+    #[validate(nested)]
+    pub optimizers_config: Option<OptimizersConfigDiff>,
+    /// Base collection parameters.
     pub params: Option<CollectionParamsDiff>,
-    /// HNSW parameters to update for the collection index. If none - it is left unchanged.
+    /// HNSW configuration.
     #[validate(nested)]
     pub hnsw_config: Option<HnswConfigDiff>,
-    /// Quantization parameters to update. If none - it is left unchanged.
+    /// Quantisation configuration.
     #[serde(default, alias = "quantization")]
     #[validate(nested)]
     pub quantization_config: Option<QuantizationConfigDiff>,
-    /// Map of sparse vector data parameters to update for each sparse vector.
+    /// Sparse vector configuration.
     #[validate(nested)]
     pub sparse_vectors: Option<SparseVectorsConfig>,
     #[validate(nested)]
     pub strict_mode_config: Option<StrictModeConfig>,
 }
 
-/// Operation for updating parameters of the existing collection
 #[derive(Debug, Deserialize, Serialize, PartialEq, Eq, Hash, Clone)]
 #[serde(rename_all = "snake_case")]
 pub struct UpdateCollectionOperation {
@@ -269,9 +298,9 @@ impl UpdateCollectionOperation {
             collection_name,
             update_collection: UpdateCollection {
                 vectors: None,
-                hnsw_config: None,
-                params: None,
                 optimizers_config: None,
+                params: None,
+                hnsw_config: None,
                 quantization_config: None,
                 sparse_vectors: None,
                 strict_mode_config: None,
@@ -288,10 +317,6 @@ impl UpdateCollectionOperation {
         }
     }
 
-    pub fn take_shard_replica_changes(&mut self) -> Option<Vec<replica_set::Change>> {
-        self.shard_replica_changes.take()
-    }
-
     pub fn set_shard_replica_changes(&mut self, changes: Vec<replica_set::Change>) {
         if changes.is_empty() {
             self.shard_replica_changes = None;
@@ -299,74 +324,63 @@ impl UpdateCollectionOperation {
             self.shard_replica_changes = Some(changes);
         }
     }
+
+    pub fn take_shard_replica_changes(&mut self) -> Option<Vec<replica_set::Change>> {
+        self.shard_replica_changes.take()
+    }
 }
 
-/// Operation for performing changes of collection aliases.
-/// Alias changes are atomic, meaning that no collection modifications can happen between
-/// alias operations.
+/// Change the aliases of a collection.
 #[derive(Debug, Deserialize, Serialize, JsonSchema, Validate, PartialEq, Eq, Hash, Clone)]
 #[serde(rename_all = "snake_case")]
 pub struct ChangeAliasesOperation {
     pub actions: Vec<AliasOperations>,
 }
 
-/// Operation for deleting collection with given name
+/// Delete a collection.
 #[derive(Debug, Deserialize, Serialize, PartialEq, Eq, Hash, Clone)]
-#[serde(rename_all = "snake_case")]
 pub struct DeleteCollectionOperation(pub String);
 
-#[derive(Clone, Debug, Eq, PartialEq, Hash, Deserialize, Serialize)]
-pub enum ReshardingOperation {
-    Start(ReshardKey),
-    CommitRead(ReshardKey),
-    CommitWrite(ReshardKey),
-    Finish(ReshardKey),
-    Abort(ReshardKey),
-}
-
+/// Operations for shard transfers.
 #[derive(Debug, Deserialize, Serialize, PartialEq, Eq, Hash, Clone)]
 pub enum ShardTransferOperations {
+    /// Start a transfer.
     Start(ShardTransfer),
-    /// Restart an existing transfer with a new configuration
-    ///
-    /// If the given transfer is ongoing, it is aborted and restarted with the new configuration.
+    /// Restart an existing transfer with a new configuration.
     Restart(ShardTransferRestart),
+    /// Finish a transfer.
     Finish(ShardTransfer),
-    /// Deprecated since Qdrant 1.9.0, used in Qdrant 1.7.0 and 1.8.0
+    /// Deprecated since Qdrant 1.9.0, used in Qdrant 1.7.0 and 1.8.0.
     ///
-    /// Used in `ShardTransferMethod::Snapshot`
+    /// Used for `ShardTransferMethod::Snapshot`.
     ///
-    /// Called when the snapshot has successfully been recovered on the remote, brings the transfer
-    /// to the next stage.
+    /// Called when the snapshot has been recovered on the remote, moving the
+    /// transfer to the next stage.
     SnapshotRecovered(ShardTransferKey),
-    /// Used in `ShardTransferMethod::Snapshot` and `ShardTransferMethod::WalDelta`
+    /// Used for `ShardTransferMethod::Snapshot` and `ShardTransferMethod::WalDelta`.
     ///
-    /// Called when the first stage of the transfer has been successfully finished, brings the
-    /// transfer to the next stage.
+    /// Called when the first stage of the transfer has been
+    /// successfully finished, moving the transfer to the next stage.
     RecoveryToPartial(ShardTransferKey),
+    /// Abort a transfer.
     Abort {
         transfer: ShardTransferKey,
         reason: String,
     },
 }
 
-/// Sets the state of shard replica
+/// Ensure the replica is in a valid state before applying a transition.
 #[derive(Debug, Deserialize, Serialize, PartialEq, Eq, Hash, Clone)]
 pub struct SetShardReplicaState {
     pub collection_name: String,
     pub shard_id: ShardId,
     pub peer_id: PeerId,
-    /// If `Active` then the replica is up to date and can receive updates and answer requests
     pub state: ReplicaState,
-    /// If `Some` then check that the replica is in this state before changing it
-    /// If `None` then the replica can be in any state
-    /// This is useful for example when we want to make sure
-    /// we only make transition from `Initializing` to `Active`, and not from `Dead` to `Active`.
-    /// If `from_state` does not match the current state of the replica, then the operation will be dismissed.
     #[serde(default)]
     pub from_state: Option<ReplicaState>,
 }
 
+/// Create a shard key.
 #[derive(Debug, Deserialize, Serialize, PartialEq, Eq, Hash, Clone)]
 pub struct CreateShardKey {
     pub collection_name: String,
@@ -374,12 +388,14 @@ pub struct CreateShardKey {
     pub placement: ShardsPlacement,
 }
 
+/// Drop a shard key.
 #[derive(Debug, Deserialize, Serialize, PartialEq, Eq, Hash, Clone)]
 pub struct DropShardKey {
     pub collection_name: String,
     pub shard_key: ShardKey,
 }
 
+/// Create payload index for a collection field.
 #[derive(Debug, Deserialize, Serialize, PartialEq, Eq, Hash, Clone)]
 pub struct CreatePayloadIndex {
     pub collection_name: String,
@@ -387,13 +403,24 @@ pub struct CreatePayloadIndex {
     pub field_schema: PayloadFieldSchema,
 }
 
+/// Drop a payload index from a collection field.
 #[derive(Debug, Deserialize, Serialize, PartialEq, Eq, Hash, Clone)]
 pub struct DropPayloadIndex {
     pub collection_name: String,
     pub field_name: PayloadKeyType,
 }
 
-/// Enumeration of all possible collection update operations
+/// Resharding operations.
+#[derive(Clone, Debug, PartialEq, Eq, Hash, Deserialize, Serialize)]
+pub enum ReshardingOperation {
+    Start(ReshardKey),
+    CommitRead(ReshardKey),
+    CommitWrite(ReshardKey),
+    Finish(ReshardKey),
+    Abort(ReshardKey),
+}
+
+/// All possible collection operations.
 #[derive(Debug, Deserialize, Serialize, PartialEq, Eq, Hash, Clone)]
 #[serde(rename_all = "snake_case")]
 pub enum CollectionMetaOperations {
@@ -401,56 +428,12 @@ pub enum CollectionMetaOperations {
     UpdateCollection(UpdateCollectionOperation),
     DeleteCollection(DeleteCollectionOperation),
     ChangeAliases(ChangeAliasesOperation),
-    Resharding(CollectionId, ReshardingOperation),
     TransferShard(CollectionId, ShardTransferOperations),
     SetShardReplicaState(SetShardReplicaState),
     CreateShardKey(CreateShardKey),
     DropShardKey(DropShardKey),
     CreatePayloadIndex(CreatePayloadIndex),
     DropPayloadIndex(DropPayloadIndex),
-    Nop { token: usize }, // Empty operation
-}
-
-/// Use config of the existing collection to generate a create collection operation
-/// for the new collection
-impl From<CollectionConfigInternal> for CreateCollection {
-    fn from(value: CollectionConfigInternal) -> Self {
-        let CollectionConfigInternal {
-            params,
-            hnsw_config,
-            optimizer_config,
-            wal_config,
-            quantization_config,
-            strict_mode_config,
-            uuid,
-        } = value;
-
-        let CollectionParams {
-            vectors,
-            shard_number,
-            sharding_method,
-            replication_factor,
-            write_consistency_factor,
-            read_fan_out_factor: _,
-            on_disk_payload,
-            sparse_vectors,
-        } = params;
-
-        Self {
-            vectors,
-            shard_number: Some(shard_number.get()),
-            sharding_method,
-            replication_factor: Some(replication_factor.get()),
-            write_consistency_factor: Some(write_consistency_factor.get()),
-            on_disk_payload: Some(on_disk_payload),
-            hnsw_config: Some(hnsw_config.into()),
-            wal_config: Some(wal_config.into()),
-            optimizers_config: Some(optimizer_config.into()),
-            init_from: None,
-            quantization_config,
-            sparse_vectors,
-            strict_mode_config,
-            uuid,
-        }
-    }
+    Resharding(CollectionId, ReshardingOperation),
+    Nop { token: usize },
 }
\ No newline at end of file
