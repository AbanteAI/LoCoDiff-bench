--- aider_tests_basic_test_reasoning.py_expectedoutput.txt (expected)+++ aider_tests_basic_test_reasoning.py_extracted.txt (actual)@@ -141,7 +141,7 @@         with (
             patch.object(model, "send_completion", return_value=(mock_hash, chunks)),
             patch.object(model, "token_count", return_value=10),
-        ):  # Mock token count to avoid serialization issues
+        ):
             # Set mdstream directly on the coder object
             coder.mdstream = mock_mdstream
 
@@ -304,16 +304,16 @@         # Create chunks to simulate streaming with think tags
         chunks = [
             # Start with open think tag
-            MockStreamingChunk(content="<think>\n", reasoning_content=None),
+            MockStreamingChunk(content="<think>\n"),
             # Reasoning content inside think tags
-            MockStreamingChunk(content="My step-by-step ", reasoning_content=None),
-            MockStreamingChunk(content="reasoning process\n", reasoning_content=None),
+            MockStreamingChunk(content="My step-by-step "),
+            MockStreamingChunk(content="reasoning process\n"),
             # Close think tag
-            MockStreamingChunk(content="</think>\n\n", reasoning_content=None),
+            MockStreamingChunk(content="</think>\n\n"),
             # Main content
-            MockStreamingChunk(content="Final ", reasoning_content=None),
-            MockStreamingChunk(content="answer ", reasoning_content=None),
-            MockStreamingChunk(content="after reasoning", reasoning_content=None),
+            MockStreamingChunk(content="Final "),
+            MockStreamingChunk(content="answer "),
+            MockStreamingChunk(content="after reasoning"),
             # End the response
             MockStreamingChunk(finish_reason="stop"),
         ]
@@ -323,7 +323,10 @@         mock_hash.hexdigest.return_value = "mock_hash_digest"
 
         # Mock the model's send_completion to return the hash and completion
-        with patch.object(model, "send_completion", return_value=(mock_hash, chunks)):
+        with (
+            patch.object(model, "send_completion", return_value=(mock_hash, chunks)),
+            patch.object(model, "token_count", return_value=10),
+        ):
             # Set mdstream directly on the coder object
             coder.mdstream = mock_mdstream
 
@@ -533,7 +536,7 @@         with (
             patch.object(model, "send_completion", return_value=(mock_hash, chunks)),
             patch.object(model, "token_count", return_value=10),
-        ):  # Mock token count to avoid serialization issues
+        ):
             # Set mdstream directly on the coder object
             coder.mdstream = mock_mdstream
 
@@ -586,11 +589,17 @@ 
         # Mock the completion response
         mock_response = MagicMock()
-        mock_response.choices = [MagicMock(message=MagicMock(content="""Here is some text
+        mock_response.choices = [
+            MagicMock(
+                message=MagicMock(
+                    content="""Here is some text
 <think>
 This reasoning should be removed
 </think>
-And this text should remain"""))]
+And this text should remain"""
+                )
+            )
+        ]
         mock_completion.return_value = mock_response
 
         messages = [{"role": "user", "content": "test"}]
