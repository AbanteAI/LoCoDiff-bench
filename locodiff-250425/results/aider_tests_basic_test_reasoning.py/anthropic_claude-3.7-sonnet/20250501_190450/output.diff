
index 80b84077..539eed31 100644
--- a/aider_tests_basic_test_reasoning.py_expectedoutput.txt (expected):tmp/tmpjt_yjd4x_expected.txt	
+++ b/aider_tests_basic_test_reasoning.py_extracted.txt (actual):tmp/tmp26adota0_actual.txt	
@@ -245,6 +245,10 @@ class TestReasoning(unittest.TestCase):
             self.assertIn(reasoning_content, output)
             self.assertIn(main_content, output)
 
+            # Verify that partial_response_content only contains the main content
+            coder.remove_reasoning_content()
+            self.assertEqual(coder.partial_response_content.strip(), main_content.strip())
+
             # Ensure proper order: reasoning first, then main content
             reasoning_pos = output.find(reasoning_content)
             main_pos = output.find(main_content)
@@ -252,10 +256,6 @@ class TestReasoning(unittest.TestCase):
                 reasoning_pos, main_pos, "Reasoning content should appear before main content"
             )
 
-            # Verify that partial_response_content only contains the main content
-            coder.remove_reasoning_content()
-            self.assertEqual(coder.partial_response_content.strip(), main_content.strip())
-
     def test_send_with_think_tags_stream(self):
         """Test that streaming with <think> tags is properly processed and formatted."""
         # Setup IO with pretty output for streaming
@@ -323,7 +323,10 @@ class TestReasoning(unittest.TestCase):
         mock_hash.hexdigest.return_value = "mock_hash_digest"
 
         # Mock the model's send_completion to return the hash and completion
-        with patch.object(model, "send_completion", return_value=(mock_hash, chunks)):
+        with (
+            patch.object(model, "send_completion", return_value=(mock_hash, chunks)),
+            patch.object(model, "token_count", return_value=10),
+        ):  # Mock token count to avoid serialization issues
             # Set mdstream directly on the coder object
             coder.mdstream = mock_mdstream
 
