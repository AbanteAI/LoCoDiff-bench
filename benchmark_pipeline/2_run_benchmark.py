#!/usr/bin/env python3
"""
Runs LoCoDiff benchmark cases against a specified model via OpenRouter.

Purpose:
  This script executes the benchmark cases generated by `1_generate_prompts.py`.
  For each benchmark case (identified by a unique prefix):
    1. Reads the corresponding prompt file.
    2. Sends the prompt content to the specified language model via the
       OpenRouter API.
    3. Retrieves the model's response.
    4. Extracts the code content from the response (expected to be within
       triple backticks).
    5. Compares the extracted code to the expected output file.
    6. Records the results (success/failure, cost, token usage, diff, raw
       response, extracted output) in a structured results directory.

  It supports concurrent execution, checks for previously run results to avoid
  re-running, and provides options to limit the number of new runs.

Arguments:
  --model (required): The model identifier string for OpenRouter
                      (e.g., 'openai/gpt-4o', 'google/gemini-pro').
  --benchmark-run-dir (required): Path to the directory containing the benchmark run data
                                  (subdirectories: 'prompts/', 'results/').
  --num-runs (optional): Maximum number of *new* benchmark cases to run.
                         - 0 (default): Show status of existing runs, run none.
                         - -1: Run all benchmark cases not already run.
                         - N > 0: Run up to N new benchmark cases.
  --concurrency (optional): Number of benchmark cases to run concurrently using
                            asyncio (default: 1).

Inputs:
  - Prompt files (`*_prompt.txt`) located in `<benchmark_run_dir>/prompts/`. Generated by
    `1_generate_prompts.py`.
  - Expected output files (`*_expectedoutput.txt`) located in `<benchmark_run_dir>/prompts/`.
    Generated by `1_generate_prompts.py`.
  - `OPENROUTER_API_KEY` environment variable: Required for authenticating with
    the OpenRouter API. The script loads this from a `.env` file in the project
    root if present.
  - Existing run results within `<benchmark_run_dir>/results/`: The script checks this directory
    to identify benchmark cases that have already been run for the specified model,
    using the latest timestamped result for status checks.
  - Command-line arguments.

Outputs:
  - Creates a structured directory hierarchy within `<benchmark_run_dir>/results/`:
    `<benchmark_run_dir>/results/[benchmark_case_prefix]/[sanitized_model_name]/[timestamp]/`
    for each executed run. `[sanitized_model_name]` is the model identifier
    with characters like '/' replaced by '_'. `[timestamp]` is in YYYYMMDD_HHMMSS format.
  - Inside each timestamped run directory, the following files are created:
    - `metadata.json`: Contains detailed information about the run, including
      model name, timestamp, success status, error messages (if any), cost,
      token counts (prompt, completion, total, native), generation ID, file paths, etc.
    - `raw_response.txt`: The complete, unmodified response received from the model API.
    - `extracted_output.txt`: The code content extracted from the raw response
      (typically from within ``` blocks). Only created if extraction is possible.
    - `output.diff`: A file showing the differences (unified diff format) between
      the `expected_output.txt` and the `extracted_output.txt`. If outputs match,
      it contains a "No differences found" message.
  - Prints real-time status updates to the console for each benchmark case being run
    (starting, success/failure, cost).
  - Prints a summary table at the end showing overall statistics for the run
    (attempted, successful, failed counts, costs).

File Modifications:
  - Creates the results directory structure under `<benchmark_run_dir>/results/`.
  - Creates `metadata.json`, `raw_response.txt`, `extracted_output.txt`, and
    `output.diff` files within each specific run's timestamped directory.
  - Does *not* modify files in `<benchmark_run_dir>/prompts/` (the input prompts/expected outputs).
  - Does *not* modify files in `cached-repos`.
  - Reads the `.env` file to load the API key but does *not* modify it.
"""

import argparse
import asyncio
import difflib
import glob
import json
import os
import re
import sys
from collections import defaultdict
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, Optional

import aiohttp  # For async requests
import openai
from dotenv import load_dotenv


# --- Model Interaction ---

# Global async client instance
_ASYNC_CLIENT = None


def _get_async_openai_client():
    """Initializes and returns the async OpenAI client for OpenRouter."""
    global _ASYNC_CLIENT
    if _ASYNC_CLIENT is None:
        load_dotenv()
        api_key = os.getenv("OPENROUTER_API_KEY")
        if not api_key:
            raise ValueError(
                "OPENROUTER_API_KEY not found in environment variables. "
                "Ensure it's set in a .env file or exported."
            )
        _ASYNC_CLIENT = openai.AsyncOpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=api_key,
        )
    return _ASYNC_CLIENT


async def get_model_response_openrouter(
    prompt_content: str, model_name: str
) -> tuple[str | None, str | None, str | None]:
    """
    Sends a prompt to a specified model via OpenRouter asynchronously.

    Args:
        prompt_content: The full content of the prompt to send to the model.
        model_name: The identifier of the model on OpenRouter (e.g., 'openai/gpt-4o').

    Returns:
        A tuple containing:
        - The content of the model's response message (str) if successful, else None.
        - The generation ID (str) if available, else None.
        - An error message (str) if an API error occurred, else None.

    Raises:
        ValueError: If the OPENROUTER_API_KEY environment variable is not set (raised by _get_async_openai_client).
    """
    client = _get_async_openai_client()
    error_message = None

    try:
        completion = await client.chat.completions.create(
            model=model_name,
            messages=[
                {
                    "role": "user",
                    "content": prompt_content,
                },
            ],
            # Optional: Add other parameters like temperature, max_tokens if needed
            # temperature=0.7,
            # max_tokens=2000,
        )

        response_content = ""
        generation_id = None

        # Check for API-level errors returned in the response body (e.g., credit limits)
        # Use getattr for safer access to potentially dynamic attributes
        error_payload = getattr(completion, "error", None)
        if error_payload:
            # Try to serialize the full error payload for more detail
            try:
                if isinstance(error_payload, dict):
                    error_details = json.dumps(error_payload)
                else:
                    error_details = str(error_payload)
                error_message = f"Provider error in response body: {error_details}"
            except Exception as serialize_err:
                # Fallback if serialization fails
                error_message = f"Provider error in response body (serialization failed: {serialize_err}): {str(error_payload)}"

            print(f"OpenRouter API reported an error: {error_message}")
            return None, None, error_message

        # Extract content if successful and no error in body
        if completion.choices and completion.choices[0].message:
            response_content = completion.choices[0].message.content or ""

        # Extract generation ID
        if hasattr(completion, "id") and isinstance(completion.id, str):
            generation_id = completion.id
        else:
            # Log the full response if ID extraction fails, might reveal structure changes
            print(
                f"Warning: Could not extract generation ID from OpenRouter response object: {completion}"
            )

        return response_content, generation_id, None  # Success

    except openai.APIError as e:
        # This catches errors where the API call itself failed (e.g., 4xx/5xx status codes)
        # Use getattr for status_code and body as they might not be statically typed
        status_code = getattr(e, "status_code", "Unknown")
        base_error_message = f"OpenRouter API Error: Status {status_code} - {e.message}"
        detailed_error_message = base_error_message  # Start with base message

        # Attempt to extract more detail from the body
        body = getattr(e, "body", None)
        if body:
            try:
                if isinstance(body, dict):
                    # Try to get nested message first
                    nested_message = body.get("error", {}).get("message")
                    if nested_message and nested_message != e.message:
                        detailed_error_message = (
                            f"{base_error_message} | Detail: {nested_message}"
                        )
                    # Include full body if it might be useful and isn't just repeating the message
                    body_str = json.dumps(body)
                    if body_str not in detailed_error_message:  # Avoid redundancy
                        detailed_error_message += f" | Body: {body_str}"
                else:
                    # If body is not a dict, include its string representation if informative
                    body_str = str(body)
                    if body_str and body_str not in detailed_error_message:
                        detailed_error_message += f" | Body: {body_str}"
            except Exception as serialize_err:
                detailed_error_message += (
                    f" (Failed to serialize body: {serialize_err})"
                )

        print(detailed_error_message)  # Print the most detailed message obtained
        return None, None, detailed_error_message  # Return the detailed message
    except Exception as e:
        error_message = f"Unexpected Error during API call: {type(e).__name__}: {e}"
        print(error_message)
        # Log traceback for unexpected errors
        # import traceback
        # traceback.print_exc()
        return None, None, error_message


async def get_generation_stats_openrouter(generation_id: str) -> dict | None:
    """
    Queries the OpenRouter Generation Stats API asynchronously for cost and token information.

    Args:
        generation_id: The ID of the generation to query (e.g., "gen-12345").

    Returns:
        A dictionary containing statistics like cost and token counts, or None if
        the query fails or the API key is missing.
        Example return format:
        {
            'cost_usd': float,
            'prompt_tokens': int,
            'completion_tokens': int,
            'total_tokens': int,
                    'native_prompt_tokens': int | None,
                    'native_completion_tokens': int | None,
                    'native_finish_reason': str | None
                }

    Raises:
        ValueError: If the OPENROUTER_API_KEY environment variable is not set.
    """
    load_dotenv()
    api_key = os.getenv("OPENROUTER_API_KEY")
    if not api_key:
        raise ValueError(
            "OPENROUTER_API_KEY not found in environment variables for stats query."
        )

    stats_url = f"https://openrouter.ai/api/v1/generation?id={generation_id}"
    headers = {"Authorization": f"Bearer {api_key}"}
    max_retries = 3
    retry_delay_seconds = 1

    for attempt in range(max_retries):
        try:
            # Use aiohttp for the async request
            async with aiohttp.ClientSession() as session:
                async with session.get(stats_url, headers=headers) as response:
                    # Check for 404 specifically for retry
                    if response.status == 404:
                        print(
                            f"Attempt {attempt + 1}/{max_retries}: Stats not found (404) for {generation_id}. Retrying in {retry_delay_seconds}s..."
                        )
                        if attempt < max_retries - 1:
                            await asyncio.sleep(retry_delay_seconds)
                            continue  # Go to next retry iteration
                        else:
                            print(
                                f"Max retries reached for {generation_id}. Giving up."
                            )
                            return None  # Failed after retries

                    # Raise HTTPError for other bad responses (4xx or 5xx, excluding 404 handled above)
                    response.raise_for_status()
                    response_data = await response.json()

            # If successful (status 200 and no exception raised), process data
            if "data" in response_data and response_data["data"] is not None:
                stats_data = response_data["data"]
                # Extract relevant fields, providing defaults or None if missing
                cost_usd = stats_data.get("total_cost", 0.0)
                prompt_tokens = stats_data.get("tokens_prompt", 0)
                completion_tokens = stats_data.get("tokens_completion", 0)
                native_prompt_tokens = stats_data.get(
                    "native_tokens_prompt"
                )  # Can be None
                native_completion_tokens = stats_data.get(
                    "native_tokens_completion"
                )  # Can be None
                native_finish_reason = stats_data.get(
                    "native_finish_reason"
                )  # Can be None

                return {
                    "cost_usd": float(cost_usd) if cost_usd is not None else 0.0,
                    "prompt_tokens": int(prompt_tokens)
                    if prompt_tokens is not None
                    else 0,
                    "completion_tokens": int(completion_tokens)
                    if completion_tokens is not None
                    else 0,
                    "total_tokens": (
                        int(prompt_tokens or 0) + int(completion_tokens or 0)
                    ),
                    "native_prompt_tokens": int(native_prompt_tokens)
                    if native_prompt_tokens is not None
                    else None,
                    "native_completion_tokens": int(native_completion_tokens)
                    if native_completion_tokens is not None
                    else None,
                    "native_finish_reason": str(native_finish_reason)
                    if native_finish_reason is not None
                    else None,
                }
            else:
                print(
                    f"Warning: 'data' field missing or null in OpenRouter stats response for ID {generation_id}."
                )
                print(f"Full response: {response_data}")
                return None  # Indicate stats could not be retrieved (even on success status)

        except aiohttp.ClientResponseError as e:
            # Catch non-404 HTTP errors after raise_for_status
            print(
                f"HTTP error querying OpenRouter generation stats API (Attempt {attempt + 1}/{max_retries}): {e.status} {e.message}"
            )
            # Don't retry on non-404 errors
            return None
        except aiohttp.ClientError as e:
            # Catch other aiohttp client errors (e.g., connection issues)
            print(
                f"Client error querying OpenRouter generation stats API (Attempt {attempt + 1}/{max_retries}): {e}"
            )
            # Don't retry on general client errors
            return None
        except json.JSONDecodeError as e:
            print(
                f"Error decoding JSON response from OpenRouter stats API (Attempt {attempt + 1}/{max_retries}): {e}"
            )
            # Don't retry on JSON errors
            return None
        except Exception as e:
            print(
                f"An unexpected error occurred during the async stats API call (Attempt {attempt + 1}/{max_retries}): {e}"
            )
            # Log traceback for unexpected errors
            # import traceback
            # traceback.print_exc()
            # Don't retry on unexpected errors
            return None

    # Should be unreachable if logic is correct, but as a fallback
    return None


# --- Benchmark Logic ---


def sanitize_filename(name: str) -> str:
    """Removes characters that are problematic for filenames/paths."""
    # Replace slashes with underscores
    name = name.replace(os.path.sep, "_").replace("/", "_")
    # Remove other potentially problematic characters
    name = re.sub(r'[<>:"|?*]', "", name)
    return name


def extract_code_from_backticks(text: str) -> str | None:
    """
    Extracts content between the first and the last triple backticks (```).
    Handles optional language identifiers after the first backticks and strips
    leading/trailing whitespace from the extracted content.
    """
    try:
        # Find the start of the first ``` block
        start_outer = text.find("```")
        if start_outer == -1:
            return None  # No opening backticks found

        # Find the end of the first ``` marker (including optional language and newline)
        # Use regex to find the end position after ```, optional language, and optional newline
        start_inner_match = re.search(r"```(?:\w+)?\s*?\n?", text[start_outer:])
        if start_inner_match:
            # Calculate the index in the original string where the actual content begins
            start_inner = start_outer + start_inner_match.end()
        else:
            # Fallback if regex fails (e.g., ``` immediately followed by content without newline)
            # Find the end of the initial ``` marker itself
            start_inner = start_outer + 3  # Length of ```

        # Find the start of the last ``` block using rfind
        end_outer = text.rfind("```")

        # Find the start of the last ``` block using rfind
        end_outer = text.rfind("```")

        # Check if the last ``` was found and if it's after the first ``` marker ended
        if end_outer == -1 or end_outer < start_inner:
            # No closing backticks found, or they are invalid.
            # Be generous: extract from the start marker to the end of the string.
            print(
                "Warning: Closing backticks not found or invalid. Extracting from start marker to end."
            )
            extracted_content = text[start_inner:]
        else:
            # Both opening and closing backticks are valid
            # Extract the content between the end of the first marker and the start of the last marker
            extracted_content = text[start_inner:end_outer]

        return extracted_content.strip()

    except Exception as e:
        # Log unexpected errors during extraction
        print(f"Error during backtick extraction: {e}")
        return None


def find_benchmark_cases(prompts_dir: str) -> list[str]:
    """
    Finds all unique benchmark case prefixes defined in the benchmark metadata
    (located in prompts_dir). Falls back to scanning prompts_dir if metadata
    is missing or invalid.

    Args:
        prompts_dir: The directory containing prompt files and metadata.json
                     (e.g., <benchmark_run_dir>/prompts/).
    """
    prefixes = set()
    metadata_path = os.path.join(prompts_dir, "metadata.json")
    metadata_loaded = False

    if os.path.exists(metadata_path):
        try:
            with open(metadata_path, "r", encoding="utf-8") as mf:
                metadata = json.load(mf)
            # Expect metadata to be a list of run dictionaries
            if isinstance(metadata, list):
                for run_data in metadata:
                    # Check for the key used in the new format
                    cases_key = "benchmark_cases_added"
                    if cases_key not in run_data and "benchmark_cases" in run_data:
                        # Support legacy key for backward compatibility if needed
                        cases_key = "benchmark_cases"
                        print(
                            f"Note: Found legacy 'benchmark_cases' key in run: {run_data.get('run_timestamp_utc', 'Unknown timestamp')}"
                        )

                    if isinstance(run_data, dict) and cases_key in run_data:
                        if isinstance(run_data[cases_key], list):
                            for case in run_data[cases_key]:
                                if (
                                    isinstance(case, dict)
                                    and "benchmark_case_prefix" in case
                                ):
                                    prefixes.add(case["benchmark_case_prefix"])
                        else:
                            print(
                                f"Warning: '{cases_key}' in metadata run is not a list."
                            )
                    else:
                        print(
                            f"Warning: Invalid run structure or missing '{cases_key}' key in metadata."
                        )
                metadata_loaded = True  # Mark metadata as successfully processed
            else:
                print(
                    f"Warning: Metadata file {metadata_path} is not a list. Expected list of runs."
                )
        except (json.JSONDecodeError, IOError) as e:
            print(
                f"Warning: Error reading/parsing {metadata_path}: {e}. Falling back to directory scan."
            )
        except Exception as e:
            print(
                f"Warning: Unexpected error processing {metadata_path}: {e}. Falling back to directory scan."
            )

    # Fallback or verification: Scan prompts directory if metadata loading failed or as a supplement
    if not metadata_loaded:
        print(f"Scanning directory {prompts_dir} for prompt files as fallback...")
        try:
            prompt_files = glob.glob(os.path.join(prompts_dir, "*_prompt.txt"))
            for f_path in prompt_files:
                basename = os.path.basename(f_path)
                prefix = basename[:-11]  # Remove '_prompt.txt'
                prefixes.add(prefix)
            if not prefixes:
                print("Warning: Fallback directory scan found no prompt files.")
        except OSError as e:
            print(f"Warning: Error during fallback directory scan: {e}")

    if not prefixes:
        print(
            "Error: No benchmark cases found either in metadata or by scanning the directory."
        )
        # Consider raising an error or returning empty list depending on desired behavior
        # return [] # Return empty list

    return sorted(list(prefixes))


def _save_text_file(filepath: Path, content: str):
    """Helper to save text content to a file, creating parent dirs."""
    try:
        filepath.parent.mkdir(parents=True, exist_ok=True)
        filepath.write_text(content, encoding="utf-8")
    except IOError as e:
        # Raise the error to be caught by the caller
        raise IOError(f"Failed to write file {filepath}: {e}") from e


async def run_single_benchmark(
    benchmark_case_prefix: str,
    model: str,
    prompts_dir: Path,  # Changed from benchmark_dir
    results_base_dir: Path,
    semaphore: asyncio.Semaphore,
) -> Dict[str, Any]:
    """
    Runs a single benchmark case asynchronously, handling errors and saving metadata.

    Args:
        benchmark_case_prefix: The unique identifier for the benchmark case.
        model: The model identifier string.
        prompts_dir: Path object to the directory containing prompt/expected files.
        results_base_dir: Path object to the base directory for saving results.
        semaphore: Asyncio semaphore for concurrency control.

    Returns:
        A dictionary containing the run metadata.
    """
    async with semaphore:
        prompt_filename = f"{benchmark_case_prefix}_prompt.txt"
        expected_filename = f"{benchmark_case_prefix}_expectedoutput.txt"
        prompt_filepath = prompts_dir / prompt_filename  # Changed from benchmark_dir
        expected_filepath = (
            prompts_dir / expected_filename
        )  # Changed from benchmark_dir

        print(f"Starting benchmark: {benchmark_case_prefix} with model {model}")

        # Initialize metadata - will be updated throughout the process
        run_metadata: Dict[str, Any] = {
            "model": model,
            "benchmark_case": benchmark_case_prefix,
            "prompts_dir": str(prompts_dir),  # Changed from benchmark_dir
            "prompt_file": str(prompt_filepath),
            "expected_file": str(expected_filepath),
            "timestamp_utc": datetime.now(timezone.utc).isoformat(),
            "success": False,
            "error": None,
            "api_error": False,
            "raw_response_length": 0,
            "extracted_output_length": None,
            "expected_output_length": 0,
            "results_dir": None,
            "generation_id": None,
            "cost_usd": None,
            "prompt_tokens": None,
            "completion_tokens": None,
            "total_tokens": None,
            "native_prompt_tokens": None,
            "native_completion_tokens": None,
            "native_finish_reason": None,
            "stats_error": None,
        }

        results_dir: Optional[Path] = None
        save_results = True  # Flag to control saving results/metadata

        try:
            # --- 1. Define Results Directory Path ---
            timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
            sanitized_model_name = sanitize_filename(model)
            results_dir = (
                results_base_dir
                / benchmark_case_prefix
                / sanitized_model_name
                / timestamp_str
            )
            run_metadata["results_dir_planned"] = str(results_dir)

            # --- 2. Read Input Files ---
            if not prompt_filepath.is_file():
                raise FileNotFoundError(f"Prompt file not found: {prompt_filepath}")
            prompt_content = prompt_filepath.read_text(encoding="utf-8")

            if not expected_filepath.is_file():
                raise FileNotFoundError(
                    f"Expected output file not found: {expected_filepath}"
                )
            expected_content = expected_filepath.read_text(encoding="utf-8")
            run_metadata["expected_output_length"] = len(expected_content)

            # --- 3. Call Model API (Async) ---
            (
                raw_model_response,
                generation_id,
                api_error_message,
            ) = await get_model_response_openrouter(prompt_content, model)

            # --- 4. Handle API Errors ---
            if api_error_message:
                # Record API error but prevent saving results/metadata
                run_metadata["error"] = api_error_message
                run_metadata["api_error"] = True
                save_results = False
                print(
                    f"⚠️ API Error: {benchmark_case_prefix} - Error: {api_error_message} - Skipping results saving."
                )
                # No need to raise, error is recorded, finally block won't save
                return run_metadata  # Return early as no results will be saved

            # --- 5. Process Successful API Response ---
            assert raw_model_response is not None
            run_metadata["raw_response_length"] = len(raw_model_response)
            run_metadata["generation_id"] = generation_id

            # --- 6. Get Generation Stats (Async) ---
            if generation_id:
                await asyncio.sleep(0.5)  # Delay for stats availability
                stats = await get_generation_stats_openrouter(generation_id)
                if stats:
                    run_metadata.update(stats)
                else:
                    run_metadata["stats_error"] = (
                        "Failed to retrieve stats from OpenRouter API"
                    )
            else:
                run_metadata["stats_error"] = (
                    "No generation ID received from chat completion"
                )

            # --- 7. Create Results Directory (Only if API call succeeded) ---
            assert results_dir is not None
            results_dir.mkdir(parents=True, exist_ok=True)
            run_metadata["results_dir"] = str(results_dir)  # Record actual path

            # --- 8. Save Raw Response ---
            _save_text_file(results_dir / "raw_response.txt", raw_model_response)

            # --- 9. Extract Content ---
            extracted_content = extract_code_from_backticks(raw_model_response)
            if extracted_content is None:
                run_metadata["error"] = "Extraction backticks not found"
                # Continue to save metadata and diff (which will show full expected)
            else:
                run_metadata["extracted_output_length"] = len(extracted_content)
                _save_text_file(results_dir / "extracted_output.txt", extracted_content)

                # --- 10. Compare Extracted vs Expected ---
                extracted_stripped = extracted_content.strip()
                expected_stripped = expected_content.strip()
                if extracted_stripped == expected_stripped:
                    run_metadata["success"] = True
                else:
                    # Only set error if not already set by extraction failure
                    if run_metadata["error"] is None:
                        run_metadata["error"] = "Output mismatch"

            # --- 11. Generate and Save Diff File ---
            # Always generate diff, even if extraction failed (diff against empty string)
            diff_path = results_dir / "output.diff"
            extracted_for_diff = (extracted_content or "").strip()  # Use empty if None
            expected_for_diff = expected_content.strip()

            if run_metadata["success"]:
                diff_content = "No differences found.\n"
            else:
                diff_lines = difflib.unified_diff(
                    expected_for_diff.splitlines(keepends=True),
                    extracted_for_diff.splitlines(keepends=True),
                    fromfile=f"{expected_filename} (expected)",
                    tofile=f"{benchmark_case_prefix}_extracted.txt (actual)",
                    lineterm="",
                )
                diff_content = "".join(diff_lines)
                if (
                    not diff_content
                ):  # Handle case where both are empty or identical after strip
                    diff_content = (
                        "No differences found (after stripping whitespace).\n"
                    )

            _save_text_file(diff_path, diff_content)  # Will raise IOError on failure

        # --- Error Handling for Processing Steps ---
        except FileNotFoundError as e:
            run_metadata["error"] = f"File Error: {e}"
            save_results = False  # Don't save metadata if input files missing
            print(
                f"File Error for {benchmark_case_prefix}: {e} - Skipping results saving."
            )
        except IOError as e:  # Catch file writing errors from _save_text_file
            run_metadata["error"] = f"IOError: {e}"
            # Allow metadata saving if results_dir exists, but log the error
            print(
                f"IO Error for {benchmark_case_prefix}: {e} - Attempting to save metadata."
            )
        except ValueError as e:  # Catches missing API key
            run_metadata["error"] = f"Config Error: {e}"
            run_metadata["api_error"] = True
            save_results = False
            print(
                f"Config Error for {benchmark_case_prefix}: {e} - Skipping results saving."
            )
        except Exception as e:  # Catch unexpected errors during processing
            run_metadata["error"] = f"Runtime Error: {type(e).__name__}: {e}"
            print(
                f"Runtime Error for {benchmark_case_prefix}: {e} - Attempting to save metadata."
            )
            # import traceback # Uncomment for debugging
            # run_metadata["traceback"] = traceback.format_exc()

        # --- Final Actions: Save Metadata and Print Status ---
        finally:
            # Save metadata only if results dir exists and saving is enabled
            if save_results and results_dir and results_dir.exists():
                metadata_path = results_dir / "metadata.json"
                try:
                    run_metadata.pop(
                        "results_dir_planned", None
                    )  # Clean up planned path
                    with open(metadata_path, "w", encoding="utf-8") as f_meta:
                        json.dump(run_metadata, f_meta, indent=4)
                except (IOError, TypeError) as meta_e:
                    print(
                        f"❌ Critical Error: Failed to save metadata.json for {benchmark_case_prefix} in {results_dir}: {meta_e}"
                    )
                    # Optionally re-raise or handle more gracefully
                    # For now, just print error, the run_metadata dict is still returned
                except Exception as meta_e:
                    print(
                        f"❌ Critical Error: Unexpected error saving metadata.json for {benchmark_case_prefix} in {results_dir}: {meta_e}"
                    )

            # Print final status for this case (unless it was an API error handled earlier)
            if not run_metadata.get("api_error", False):
                cost_str = (
                    f"Cost: ${run_metadata.get('cost_usd', 0.0):.6f}"
                    if run_metadata.get("cost_usd") is not None
                    else "Cost: N/A"
                )
                if run_metadata["success"]:
                    print(f"✅ Success: {benchmark_case_prefix} - {cost_str}")
                else:
                    error_msg = run_metadata.get("error", "Unknown processing error")
                    print(
                        f"❌ Failure: {benchmark_case_prefix} - Error: {error_msg} - {cost_str}"
                    )

            return run_metadata  # Return the final metadata dictionary


def _get_latest_run_info(
    benchmark_case_prefix: str, model_name: str, results_base_dir: Path
) -> tuple[Path | None, float]:
    """
    Finds the latest run directory for a case/model and returns its path and cost.
    # Helper function for checking existing results.

    Returns:
        A tuple containing:
        - The Path object for the latest run directory, or None if no run exists.
        - The cost_usd from the latest run's metadata (float), or 0.0 if no run,
          metadata is missing/unreadable, or cost is not recorded.
    """
    sanitized_model_name = sanitize_filename(model_name)
    pattern = f"{benchmark_case_prefix}/{sanitized_model_name}/*"
    potential_dirs = list(results_base_dir.glob(pattern))

    latest_dir: Optional[Path] = None
    latest_timestamp = ""

    for result_dir in potential_dirs:
        if not result_dir.is_dir():
            continue
        dir_name = result_dir.name
        # Check if it looks like a timestamp directory and find the latest
        if re.match(r"\d{8}_\d{6}", dir_name):
            if dir_name > latest_timestamp:
                latest_timestamp = dir_name
                latest_dir = result_dir

    if latest_dir is None:
        return None, 0.0  # No run found

    # Found a latest run, try to get cost
    metadata_path = latest_dir / "metadata.json"
    if metadata_path.exists():
        try:
            with open(metadata_path, "r", encoding="utf-8") as f:
                metadata = json.load(f)
            # Get cost, default to 0.0 if key missing or value is None/invalid
            cost = float(metadata.get("cost_usd", 0.0) or 0.0)
            return latest_dir, cost  # Return cost derived from metadata
        except (json.JSONDecodeError, IOError, ValueError, TypeError) as e:
            print(
                f"Warning: Could not read/parse metadata or cost for {latest_dir}: {e}"
            )
            return latest_dir, 0.0  # Return 0.0 cost if metadata parsing failed
    else:
        # Metadata file doesn't exist
        return latest_dir, 0.0  # Return 0.0 cost if no metadata file


async def main():
    parser = argparse.ArgumentParser(
        description="Run benchmark cases against a model using OpenRouter, using a unified benchmark run directory."
    )
    # Removed default paths for benchmark/results dirs

    parser.add_argument(
        "--model",
        required=True,
        help="Model identifier for OpenRouter (e.g., 'openai/gpt-4o').",
    )
    parser.add_argument(
        "--benchmark-run-dir",
        type=Path,
        required=True,
        help="Path to the directory containing the benchmark run data (subdirectories: 'prompts/', 'results/').",
    )
    # Removed --benchmark-dir and --results-dir arguments
    parser.add_argument(
        "--num-runs",
        type=int,
        default=0,
        help="Maximum number of new benchmarks to run. Set to -1 to run all remaining. (default: 0 - just show status).",
    )
    parser.add_argument(
        "--concurrency",
        type=int,
        default=1,
        help="Number of benchmarks to run concurrently (default: 1).",
    )

    args = parser.parse_args()

    # Define prompts and results directories based on the run dir
    prompts_dir = args.benchmark_run_dir / "prompts"
    results_base_dir = args.benchmark_run_dir / "results"

    # Ensure the base results directory exists
    results_base_dir.mkdir(parents=True, exist_ok=True)

    print("--- Starting Benchmark Run ---")
    print(f"Model: {args.model}")
    print(f"Benchmark Run Directory: {args.benchmark_run_dir}")
    print(f"  (Prompts Source: {prompts_dir})")
    print(f"  (Results Target: {results_base_dir})")
    print(f"Concurrency: {args.concurrency}")
    print(f"Max New Runs: {'All Remaining' if args.num_runs == -1 else args.num_runs}")
    print("-" * 30)

    # Find cases using the prompts directory
    prompts_dir_str = str(prompts_dir)
    all_cases = find_benchmark_cases(prompts_dir_str)
    if not all_cases:
        print(f"Error: No benchmark cases found in '{prompts_dir_str}'.")
        return 1

    print(f"Found {len(all_cases)} total benchmark cases.")

    already_run_cases = set()
    total_previous_cost = 0.0
    print("Checking for existing results and calculating previous costs...")
    for case_prefix in all_cases:
        # Use the helper function to find the latest run and its cost within results_base_dir
        latest_run_dir, cost = _get_latest_run_info(
            case_prefix,
            args.model,
            results_base_dir,  # Use derived results path
        )
        if latest_run_dir is not None:  # Check if a run directory was found
            already_run_cases.add(case_prefix)
            total_previous_cost += cost

    print(
        f"{len(already_run_cases)}/{len(all_cases)} cases have already been run for model '{args.model}'."
    )
    print(f"Total cost of previously run cases: ${total_previous_cost:.6f}")
    print("-" * 30)

    # Determine cases to run (those not in the already_run_cases set)
    cases_to_run_all = [case for case in all_cases if case not in already_run_cases]

    if not cases_to_run_all:
        print("No remaining benchmark cases to run for this model.")
        print("--- Benchmark Run Complete ---")
        return 0

    # Determine cases to run based on limit
    if args.num_runs == 0:
        cases_to_run_limited = []
        print(
            "Running in informational mode (num-runs=0). No new benchmarks will be executed."
        )
    elif args.num_runs == -1:
        cases_to_run_limited = cases_to_run_all
        print(
            f"Preparing to run all {len(cases_to_run_limited)} remaining benchmarks..."
        )
    else:
        cases_to_run_limited = cases_to_run_all[: args.num_runs]
        if not cases_to_run_limited:
            print("Limit specified, but no remaining cases to run within that limit.")
        else:
            print(
                f"Preparing to run up to {args.num_runs} new benchmarks ({len(cases_to_run_limited)} available within limit)..."
            )

    if not cases_to_run_limited:
        print("--- Benchmark Run Complete ---")
        return 0

    semaphore = asyncio.Semaphore(args.concurrency)
    tasks = [
        asyncio.create_task(
            run_single_benchmark(
                case,
                args.model,
                prompts_dir,  # Pass derived prompts Path object
                results_base_dir,  # Pass derived results Path object
                semaphore,
            )
        )
        for case in cases_to_run_limited
    ]

    # Run tasks and collect results
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # Process results
    success_count = 0
    failure_counts_by_type = defaultdict(int)
    api_error_count = 0
    system_error_count = 0
    total_new_cost = 0.0

    for result in results:
        if isinstance(result, Exception):
            print(
                f"❌ System Error: An unexpected error occurred during task execution: {result}"
            )
            system_error_count += 1
        elif isinstance(result, dict):
            # Accumulate cost only if not an API error and cost is present
            if not result.get("api_error") and result.get("cost_usd") is not None:
                total_new_cost += float(result.get("cost_usd", 0.0))

            # Categorize result
            if result.get("api_error"):
                api_error_count += 1
            elif result.get("success"):
                success_count += 1
            else:
                error_msg = result.get("error", "Unknown processing error")
                # Simplify error messages for grouping
                if "File Error:" in error_msg:
                    error_type = "File Error"
                elif "IOError:" in error_msg:
                    error_type = "IO Error"
                elif "Runtime Error:" in error_msg:
                    error_type = "Runtime Error"
                elif "Extraction backticks not found" in error_msg:
                    error_type = "Extraction Error"
                elif "Output mismatch" in error_msg:
                    error_type = "Output Mismatch"
                else:
                    error_type = error_msg
                failure_counts_by_type[error_type] += 1
        else:
            print(f"❌ System Error: Unexpected result type from task: {type(result)}")
            system_error_count += 1

    # Print Summary
    print("\n--- Benchmark Run Summary ---")
    print(f"Model: {args.model}")
    print(f"Attempted in this run: {len(results)} benchmarks")
    print(f"  ✅ Successful: {success_count}")
    if failure_counts_by_type:
        print("  --- Failures by Type ---")
        for error_type, count in sorted(failure_counts_by_type.items()):
            print(f"    ❌ {error_type}: {count}")
        print("  ------------------------")
    else:
        print("  ❌ Failed (Processing Errors): 0")
    print(f"  ⚠️ API Errors (Config/Credits/Rate Limits/etc.): {api_error_count}")
    if system_error_count > 0:
        print(f"  🔥 System Errors (Unexpected Task Failures): {system_error_count}")
    print("-" * 20)
    print(f"Cost of this run (successful/failed runs only): ${total_new_cost:.6f}")
    print(f"Total cost of previous runs: ${total_previous_cost:.6f}")
    print(
        f"Overall total cost (previous + current): ${total_previous_cost + total_new_cost:.6f}"
    )
    print("--- Benchmark Run Complete ---")

    # Determine exit code
    total_failures = sum(failure_counts_by_type.values())
    return 1 if (total_failures + api_error_count + system_error_count) > 0 else 0


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
