#!/usr/bin/env python3
"""
Runs LoCoDiff benchmark cases against a specified model via OpenRouter.

Purpose:
  This script executes the benchmark cases generated by `1_generate_prompts.py`.
  For each benchmark case (identified by a unique prefix):
    1. Reads the corresponding prompt file.
    2. Sends the prompt content to the specified language model via the
       OpenRouter API.
    3. Retrieves the model's response.
    4. Extracts the code content from the response (expected to be within
       triple backticks).
    5. Compares the extracted code to the expected output file.
    6. Records the results (success/failure, cost, token usage, diff, raw
       response, extracted output) in a structured results directory.

  It supports concurrent execution, checks for previously run results to avoid
  re-running, and provides options to limit the number of new runs.

Arguments:
  --model (required): The model identifier string for OpenRouter
                      (e.g., 'openai/gpt-4o', 'google/gemini-pro').
  --benchmark-dir (optional): Directory containing the generated prompt and
                              expected output files (default: 'generated_prompts').
  --results-dir (optional): Base directory where run results will be saved
                            (default: 'benchmark_results').
  --num-runs (optional): Maximum number of *new* benchmark cases to run.
                         - 0 (default): Show status of existing runs, run none.
                         - -1: Run all benchmark cases not already run.
                         - N > 0: Run up to N new benchmark cases.
  --concurrency (optional): Number of benchmark cases to run concurrently using
                            asyncio (default: 1).

Inputs:
  - Prompt files (`*_prompt.txt`) located in `benchmark-dir`. Generated by
    `1_generate_prompts.py`.
  - Expected output files (`*_expectedoutput.txt`) located in `benchmark-dir`.
    Generated by `1_generate_prompts.py`.
  - `OPENROUTER_API_KEY` environment variable: Required for authenticating with
    the OpenRouter API. The script loads this from a `.env` file in the project
    root if present.
  - Existing run results within `results-dir`: The script checks this directory
    to identify benchmark cases that have already been run for the specified model,
    using the latest timestamped result for status checks.
  - Command-line arguments.

Outputs:
  - Creates a structured directory hierarchy within `results-dir`:
    `results-dir/[benchmark_case_prefix]/[sanitized_model_name]/[timestamp]/`
    for each executed run. `[sanitized_model_name]` is the model identifier
    with characters like '/' replaced by '_'. `[timestamp]` is in YYYYMMDD_HHMMSS format.
  - Inside each timestamped run directory, the following files are created:
    - `metadata.json`: Contains detailed information about the run, including
      model name, timestamp, success status, error messages (if any), cost,
      token counts (prompt, completion, total, native), generation ID, file paths, etc.
    - `raw_response.txt`: The complete, unmodified response received from the model API.
    - `extracted_output.txt`: The code content extracted from the raw response
      (typically from within ``` blocks). Only created if extraction is possible.
    - `output.diff`: A file showing the differences (unified diff format) between
      the `expected_output.txt` and the `extracted_output.txt`. If outputs match,
      it contains a "No differences found" message.
  - Prints real-time status updates to the console for each benchmark case being run
    (starting, success/failure, cost).
  - Prints a summary table at the end showing overall statistics for the run
    (attempted, successful, failed counts, costs).

File Modifications:
  - Creates the results directory structure under `results-dir`.
  - Creates `metadata.json`, `raw_response.txt`, `extracted_output.txt`, and
    `output.diff` files within each specific run's timestamped directory.
  - Does *not* modify files in `benchmark-dir` (the input prompts/expected outputs).
  - Does *not* modify files in `cached-repos`.
  - Reads the `.env` file to load the API key but does *not* modify it.
"""

import argparse
import asyncio
import difflib
import glob
import json
import os
import re
import sys
from datetime import datetime

import aiohttp  # For async requests
import openai
from dotenv import load_dotenv


# --- Model Interaction ---

# Global async client instance
_ASYNC_CLIENT = None


def _get_async_openai_client():
    """Initializes and returns the async OpenAI client for OpenRouter."""
    global _ASYNC_CLIENT
    if _ASYNC_CLIENT is None:
        load_dotenv()
        api_key = os.getenv("OPENROUTER_API_KEY")
        if not api_key:
            raise ValueError(
                "OPENROUTER_API_KEY not found in environment variables. "
                "Ensure it's set in a .env file or exported."
            )
        _ASYNC_CLIENT = openai.AsyncOpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=api_key,
        )
    return _ASYNC_CLIENT


async def get_model_response_openrouter(
    prompt_content: str, model_name: str
) -> tuple[str | None, str | None, str | None]:
    """
    Sends a prompt to a specified model via OpenRouter asynchronously.

    Args:
        prompt_content: The full content of the prompt to send to the model.
        model_name: The identifier of the model on OpenRouter (e.g., 'openai/gpt-4o').

    Returns:
        A tuple containing:
        - The content of the model's response message (str) if successful, else None.
        - The generation ID (str) if available, else None.
        - An error message (str) if an API error occurred, else None.

    Raises:
        ValueError: If the OPENROUTER_API_KEY environment variable is not set (raised by _get_async_openai_client).
    """
    client = _get_async_openai_client()
    error_message = None

    try:
        completion = await client.chat.completions.create(
            model=model_name,
            messages=[
                {
                    "role": "user",
                    "content": prompt_content,
                },
            ],
            # Optional: Add other parameters like temperature, max_tokens if needed
            # temperature=0.7,
            # max_tokens=2000,
        )

        response_content = ""
        generation_id = None

        # Check for API-level errors returned in the response body (e.g., credit limits)
        # Use getattr for safer access to potentially dynamic attributes
        error_payload = getattr(completion, "error", None)
        if error_payload:
            # Try to serialize the full error payload for more detail
            try:
                if isinstance(error_payload, dict):
                    error_details = json.dumps(error_payload)
                else:
                    error_details = str(error_payload)
                error_message = f"Provider error in response body: {error_details}"
            except Exception as serialize_err:
                # Fallback if serialization fails
                error_message = f"Provider error in response body (serialization failed: {serialize_err}): {str(error_payload)}"

            print(f"OpenRouter API reported an error: {error_message}")
            return None, None, error_message

        # Extract content if successful and no error in body
        if completion.choices and completion.choices[0].message:
            response_content = completion.choices[0].message.content or ""

        # Extract generation ID
        if hasattr(completion, "id") and isinstance(completion.id, str):
            generation_id = completion.id
        else:
            # Log the full response if ID extraction fails, might reveal structure changes
            print(
                f"Warning: Could not extract generation ID from OpenRouter response object: {completion}"
            )

        return response_content, generation_id, None  # Success

    except openai.APIError as e:
        # This catches errors where the API call itself failed (e.g., 4xx/5xx status codes)
        # Use getattr for status_code and body as they might not be statically typed
        status_code = getattr(e, "status_code", "Unknown")
        base_error_message = f"OpenRouter API Error: Status {status_code} - {e.message}"
        detailed_error_message = base_error_message  # Start with base message

        # Attempt to extract more detail from the body
        body = getattr(e, "body", None)
        if body:
            try:
                if isinstance(body, dict):
                    # Try to get nested message first
                    nested_message = body.get("error", {}).get("message")
                    if nested_message and nested_message != e.message:
                        detailed_error_message = (
                            f"{base_error_message} | Detail: {nested_message}"
                        )
                    # Include full body if it might be useful and isn't just repeating the message
                    body_str = json.dumps(body)
                    if body_str not in detailed_error_message:  # Avoid redundancy
                        detailed_error_message += f" | Body: {body_str}"
                else:
                    # If body is not a dict, include its string representation if informative
                    body_str = str(body)
                    if body_str and body_str not in detailed_error_message:
                        detailed_error_message += f" | Body: {body_str}"
            except Exception as serialize_err:
                detailed_error_message += (
                    f" (Failed to serialize body: {serialize_err})"
                )

        print(detailed_error_message)  # Print the most detailed message obtained
        return None, None, detailed_error_message  # Return the detailed message
    except Exception as e:
        error_message = f"Unexpected Error during API call: {type(e).__name__}: {e}"
        print(error_message)
        # Log traceback for unexpected errors
        # import traceback
        # traceback.print_exc()
        return None, None, error_message


async def get_generation_stats_openrouter(generation_id: str) -> dict | None:
    """
    Queries the OpenRouter Generation Stats API asynchronously for cost and token information.

    Args:
        generation_id: The ID of the generation to query (e.g., "gen-12345").

    Returns:
        A dictionary containing statistics like cost and token counts, or None if
        the query fails or the API key is missing.
        Example return format:
        {
            'cost_usd': float,
            'prompt_tokens': int,
            'completion_tokens': int,
            'total_tokens': int,
                    'native_prompt_tokens': int | None,
                    'native_completion_tokens': int | None,
                    'native_finish_reason': str | None
                }

    Raises:
        ValueError: If the OPENROUTER_API_KEY environment variable is not set.
    """
    load_dotenv()
    api_key = os.getenv("OPENROUTER_API_KEY")
    if not api_key:
        raise ValueError(
            "OPENROUTER_API_KEY not found in environment variables for stats query."
        )

    stats_url = f"https://openrouter.ai/api/v1/generation?id={generation_id}"
    headers = {"Authorization": f"Bearer {api_key}"}
    max_retries = 3
    retry_delay_seconds = 1

    for attempt in range(max_retries):
        try:
            # Use aiohttp for the async request
            async with aiohttp.ClientSession() as session:
                async with session.get(stats_url, headers=headers) as response:
                    # Check for 404 specifically for retry
                    if response.status == 404:
                        print(
                            f"Attempt {attempt + 1}/{max_retries}: Stats not found (404) for {generation_id}. Retrying in {retry_delay_seconds}s..."
                        )
                        if attempt < max_retries - 1:
                            await asyncio.sleep(retry_delay_seconds)
                            continue  # Go to next retry iteration
                        else:
                            print(
                                f"Max retries reached for {generation_id}. Giving up."
                            )
                            return None  # Failed after retries

                    # Raise HTTPError for other bad responses (4xx or 5xx, excluding 404 handled above)
                    response.raise_for_status()
                    response_data = await response.json()

            # If successful (status 200 and no exception raised), process data
            if "data" in response_data and response_data["data"] is not None:
                stats_data = response_data["data"]
                # Extract relevant fields, providing defaults or None if missing
                cost_usd = stats_data.get("total_cost", 0.0)
                prompt_tokens = stats_data.get("tokens_prompt", 0)
                completion_tokens = stats_data.get("tokens_completion", 0)
                native_prompt_tokens = stats_data.get(
                    "native_tokens_prompt"
                )  # Can be None
                native_completion_tokens = stats_data.get(
                    "native_tokens_completion"
                )  # Can be None
                native_finish_reason = stats_data.get(
                    "native_finish_reason"
                )  # Can be None

                return {
                    "cost_usd": float(cost_usd) if cost_usd is not None else 0.0,
                    "prompt_tokens": int(prompt_tokens)
                    if prompt_tokens is not None
                    else 0,
                    "completion_tokens": int(completion_tokens)
                    if completion_tokens is not None
                    else 0,
                    "total_tokens": (
                        int(prompt_tokens or 0) + int(completion_tokens or 0)
                    ),
                    "native_prompt_tokens": int(native_prompt_tokens)
                    if native_prompt_tokens is not None
                    else None,
                    "native_completion_tokens": int(native_completion_tokens)
                    if native_completion_tokens is not None
                    else None,
                    "native_finish_reason": str(native_finish_reason)
                    if native_finish_reason is not None
                    else None,
                }
            else:
                print(
                    f"Warning: 'data' field missing or null in OpenRouter stats response for ID {generation_id}."
                )
                print(f"Full response: {response_data}")
                return None  # Indicate stats could not be retrieved (even on success status)

        except aiohttp.ClientResponseError as e:
            # Catch non-404 HTTP errors after raise_for_status
            print(
                f"HTTP error querying OpenRouter generation stats API (Attempt {attempt + 1}/{max_retries}): {e.status} {e.message}"
            )
            # Don't retry on non-404 errors
            return None
        except aiohttp.ClientError as e:
            # Catch other aiohttp client errors (e.g., connection issues)
            print(
                f"Client error querying OpenRouter generation stats API (Attempt {attempt + 1}/{max_retries}): {e}"
            )
            # Don't retry on general client errors
            return None
        except json.JSONDecodeError as e:
            print(
                f"Error decoding JSON response from OpenRouter stats API (Attempt {attempt + 1}/{max_retries}): {e}"
            )
            # Don't retry on JSON errors
            return None
        except Exception as e:
            print(
                f"An unexpected error occurred during the async stats API call (Attempt {attempt + 1}/{max_retries}): {e}"
            )
            # Log traceback for unexpected errors
            # import traceback
            # traceback.print_exc()
            # Don't retry on unexpected errors
            return None

    # Should be unreachable if logic is correct, but as a fallback
    return None


# --- Benchmark Logic ---


def sanitize_filename(name: str) -> str:
    """Removes characters that are problematic for filenames/paths."""
    # Replace slashes with underscores
    name = name.replace(os.path.sep, "_").replace("/", "_")
    # Remove other potentially problematic characters
    name = re.sub(r'[<>:"|?*]', "", name)
    return name


def extract_code_from_backticks(text: str) -> str | None:
    """
    Extracts content between the first and the last triple backticks (```).
    Handles optional language identifiers after the first backticks and strips
    leading/trailing whitespace from the extracted content.
    """
    try:
        # Find the start of the first ``` block
        start_outer = text.find("```")
        if start_outer == -1:
            return None  # No opening backticks found

        # Find the end of the first ``` marker (including optional language and newline)
        # Use regex to find the end position after ```, optional language, and optional newline
        start_inner_match = re.search(r"```(?:\w+)?\s*?\n?", text[start_outer:])
        if start_inner_match:
            # Calculate the index in the original string where the actual content begins
            start_inner = start_outer + start_inner_match.end()
        else:
            # Fallback if regex fails (e.g., ``` immediately followed by content without newline)
            # Find the end of the initial ``` marker itself
            start_inner = start_outer + 3  # Length of ```

        # Find the start of the last ``` block using rfind
        end_outer = text.rfind("```")

        # Find the start of the last ``` block using rfind
        end_outer = text.rfind("```")

        # Check if the last ``` was found and if it's after the first ``` marker ended
        if end_outer == -1 or end_outer < start_inner:
            # No closing backticks found, or they are invalid.
            # Be generous: extract from the start marker to the end of the string.
            print(
                "Warning: Closing backticks not found or invalid. Extracting from start marker to end."
            )
            extracted_content = text[start_inner:]
        else:
            # Both opening and closing backticks are valid
            # Extract the content between the end of the first marker and the start of the last marker
            extracted_content = text[start_inner:end_outer]

        return extracted_content.strip()

    except Exception as e:
        # Log unexpected errors during extraction
        print(f"Error during backtick extraction: {e}")
        return None


def find_benchmark_cases(benchmark_dir: str) -> list[str]:
    """
    Finds all unique benchmark case prefixes defined in the benchmark metadata.
    Falls back to scanning directory if metadata is missing or invalid.
    """
    prefixes = set()
    metadata_path = os.path.join(benchmark_dir, "metadata.json")
    metadata_loaded = False

    if os.path.exists(metadata_path):
        try:
            with open(metadata_path, "r", encoding="utf-8") as f:
                metadata = json.load(f)
                if isinstance(metadata, list):
                    # New-style metadata (list of runs)
                    for run in metadata:
                        if isinstance(run, dict):
                            # Extract from benchmark_cases
                            if "benchmark_cases" in run:
                                for case in run["benchmark_cases"]:
                                    if (
                                        isinstance(case, dict)
                                        and "benchmark_case_prefix" in case
                                    ):
                                        prefixes.add(case["benchmark_case_prefix"])
                            # Extract from benchmark_cases_added
                            if "benchmark_cases_added" in run:
                                for case in run["benchmark_cases_added"]:
                                    if (
                                        isinstance(case, dict)
                                        and "benchmark_case_prefix" in case
                                    ):
                                        prefixes.add(case["benchmark_case_prefix"])
                elif isinstance(metadata, dict):
                    # Old-style metadata (single run object)
                    if "benchmark_cases" in metadata:
                        for case in metadata["benchmark_cases"]:
                            if (
                                isinstance(case, dict)
                                and "benchmark_case_prefix" in case
                            ):
                                prefixes.add(case["benchmark_case_prefix"])
                metadata_loaded = bool(prefixes)  # True if we found any prefixes
        except (json.JSONDecodeError, TypeError, OSError) as e:
            print(f"Error loading metadata file: {e}")

    # Fallback: Scan directory for prompt files if metadata didn't yield prefixes
    if not metadata_loaded:
        print(
            "Warning: Metadata file not found or invalid. Scanning directory for prompt files."
        )
        pattern = os.path.join(benchmark_dir, "*_prompt.txt")
        for path in glob.glob(pattern):
            basename = os.path.basename(path)
            # Remove _prompt.txt to get prefix
            if basename.endswith("_prompt.txt"):
                prefix = basename[:-11]  # Remove _prompt.txt suffix
                prefixes.add(prefix)

    if not prefixes:
        print(f"Warning: No benchmark cases found in {benchmark_dir}")

    return sorted(list(prefixes))  # Sort for deterministic behavior


def check_existing_runs(
    case_prefix: str, model_name: str, results_dir: str
) -> tuple[bool, str | None, str | None]:
    """
    Checks if a benchmark case has already been run for a given model.

    Args:
        case_prefix: The benchmark case prefix to check.
        model_name: The model name to check for.
        results_dir: Directory containing results.

    Returns:
        A tuple containing:
        - Boolean indicating if the case has already been run.
        - Path to the most recent run directory, or None if not run.
        - Status string ('success', 'failure', or None if not run).
    """
    safe_model_name = sanitize_filename(model_name)
    model_dir = os.path.join(results_dir, case_prefix, safe_model_name)

    if not os.path.exists(model_dir) or not os.path.isdir(model_dir):
        return False, None, None

    # Find all timestamp subdirectories
    timestamp_dirs = [
        d for d in os.listdir(model_dir) if os.path.isdir(os.path.join(model_dir, d))
    ]
    if not timestamp_dirs:
        return False, None, None

    # Sort by timestamp descending (most recent first)
    timestamp_dirs.sort(reverse=True)
    latest_dir = os.path.join(model_dir, timestamp_dirs[0])

    # Check if run was successful by looking at metadata
    metadata_path = os.path.join(latest_dir, "metadata.json")
    if os.path.exists(metadata_path):
        try:
            with open(metadata_path, "r", encoding="utf-8") as f:
                metadata = json.load(f)
                success = metadata.get("success", False)
                return True, latest_dir, "success" if success else "failure"
        except (json.JSONDecodeError, TypeError, OSError):
            # If metadata.json exists but can't be read, assume failure
            return True, latest_dir, "failure"
    else:
        # If run dir exists but no metadata, assume it's a failed/incomplete run
        return True, latest_dir, "failure"


async def run_benchmark_case(
    case_prefix: str,
    model_name: str,
    benchmark_dir: str,
    results_dir: str,
    existing_runs_info: dict,
    run_attempt_counter: dict,
) -> dict:
    """
    Runs a single benchmark case against the specified model.

    Args:
        case_prefix: The benchmark case prefix identifier.
        model_name: The model name for OpenRouter.
        benchmark_dir: Directory containing benchmark prompts/expected files.
        results_dir: Base directory to save results.
        existing_runs_info: Dictionary of existing run info for this model.
        run_attempt_counter: Counter dictionary for tracking run attempts.

    Returns:
        A dictionary containing run results and metadata.
    """
    prompt_path = os.path.join(benchmark_dir, f"{case_prefix}_prompt.txt")
    expected_path = os.path.join(benchmark_dir, f"{case_prefix}_expectedoutput.txt")

    # Check if files exist
    if not os.path.exists(prompt_path):
        return {
            "case_prefix": case_prefix,
            "success": False,
            "error": f"Prompt file not found: {prompt_path}",
            "skipped": True,
        }
    if not os.path.exists(expected_path):
        return {
            "case_prefix": case_prefix,
            "success": False,
            "error": f"Expected output file not found: {expected_path}",
            "skipped": True,
        }

    # Skip if already run successfully
    if case_prefix in existing_runs_info:
        status = existing_runs_info[case_prefix]["status"]
        run_dir = existing_runs_info[case_prefix]["run_dir"]
        if status == "success":
            # Load some basic info from previous successful run for the summary
            metadata_path = os.path.join(run_dir, "metadata.json")
            try:
                with open(metadata_path, "r", encoding="utf-8") as f:
                    metadata = json.load(f)
                    return {
                        "case_prefix": case_prefix,
                        "success": True,
                        "skipped": True,
                        "already_run": True,
                        "run_dir": run_dir,
                        "cost_usd": metadata.get("cost_usd", 0.0),
                        "total_tokens": metadata.get("total_tokens", 0),
                    }
            except (json.JSONDecodeError, TypeError, OSError) as e:
                print(f"Error loading metadata from existing run: {e}")
                # Continue with the run anyway

    # Update counters
    run_attempt_counter["total"] += 1

    # Create timestamp for this run
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    safe_model_name = sanitize_filename(model_name)
    run_dir = os.path.join(results_dir, case_prefix, safe_model_name, timestamp)
    os.makedirs(run_dir, exist_ok=True)

    # Read prompt and expected output
    try:
        with open(prompt_path, "r", encoding="utf-8") as f:
            prompt_content = f.read()
        with open(expected_path, "r", encoding="utf-8") as f:
            expected_content = f.read()
    except (OSError, UnicodeDecodeError) as e:
        run_attempt_counter["error"] += 1
        return {
            "case_prefix": case_prefix,
            "success": False,
            "error": f"Error reading input files: {e}",
            "timestamp": timestamp,
            "run_dir": run_dir,
            "skipped": False,
        }

    # 1. Make API request to OpenRouter
    print(f"Running benchmark for {case_prefix} with model {model_name}...")
    run_results = {
        "case_prefix": case_prefix,
        "model_name": model_name,
        "timestamp": timestamp,
        "benchmark_dir": benchmark_dir,
        "results_dir": results_dir,
        "run_dir": run_dir,
        "prompt_path": prompt_path,
        "expected_path": expected_path,
        "skipped": False,
    }

    try:
        # Make the request
        (
            response_content,
            generation_id,
            error_message,
        ) = await get_model_response_openrouter(prompt_content, model_name)

        # Set up initial results
        run_results["api_request_successful"] = response_content is not None
        run_results["generation_id"] = generation_id
        if error_message:
            run_results["api_error"] = error_message
            run_results["success"] = False
            run_attempt_counter["api_failure"] += 1

        # Save raw response (even if API reported an error)
        raw_response_path = os.path.join(run_dir, "raw_response.txt")
        with open(raw_response_path, "w", encoding="utf-8") as f:
            if response_content:
                f.write(response_content)
            else:
                f.write(f"API Error: {error_message}")
        run_results["raw_response_path"] = raw_response_path

        # If API call failed, return early with error info
        if response_content is None:
            # Write metadata (partial)
            metadata_path = os.path.join(run_dir, "metadata.json")
            with open(metadata_path, "w", encoding="utf-8") as f:
                json.dump(run_results, f, indent=2)
            return run_results

        # 2. Extract code from response if successful
        extracted_content = extract_code_from_backticks(response_content)
        if extracted_content is None:
            run_results["extraction_error"] = (
                "Failed to extract code from response (no triple backticks found)"
            )
            run_results["success"] = False
            run_attempt_counter["extraction_failure"] += 1
        else:
            # Save extracted content
            extracted_path = os.path.join(run_dir, "extracted_output.txt")
            with open(extracted_path, "w", encoding="utf-8") as f:
                f.write(extracted_content)
            run_results["extracted_path"] = extracted_path
            run_results["extraction_successful"] = True

            # 3. Compare extracted to expected
            diff_path = os.path.join(run_dir, "output.diff")
            with open(diff_path, "w", encoding="utf-8") as f:
                if extracted_content == expected_content:
                    f.write(
                        "No differences found. Output matches expected content exactly.\n"
                    )
                    run_results["success"] = True
                    run_attempt_counter["success"] += 1
                else:
                    # Create a unified diff
                    diff_lines = difflib.unified_diff(
                        expected_content.splitlines(keepends=True),
                        extracted_content.splitlines(keepends=True),
                        fromfile="expected_output.txt",
                        tofile="model_output.txt",
                    )
                    diff_text = "".join(diff_lines)
                    f.write(diff_text)
                    run_results["success"] = False
                    run_attempt_counter["diff_failure"] += 1
            run_results["diff_path"] = diff_path

        # 4. Get token usage and cost stats if generation_id is available
        if generation_id:
            stats = await get_generation_stats_openrouter(generation_id)
            if stats:
                run_results.update(stats)
                print(
                    f"  Cost: ${stats.get('cost_usd', 0):.6f}, "
                    f"Tokens: {stats.get('total_tokens', 0)} "
                    f"({'success' if run_results.get('success', False) else 'failure'})"
                )
            else:
                print("  Warning: Failed to retrieve cost/token statistics")

        # 5. Write final metadata
        metadata_path = os.path.join(run_dir, "metadata.json")
        with open(metadata_path, "w", encoding="utf-8") as f:
            json.dump(run_results, f, indent=2)

        return run_results

    except Exception as e:
        # Handle unexpected errors during benchmark execution
        error_message = (
            f"Unexpected error during benchmark execution: {type(e).__name__}: {e}"
        )
        print(f"  Error: {error_message}")

        run_results["success"] = False
        run_results["error"] = error_message
        run_attempt_counter["error"] += 1

        # Save metadata even in case of unexpected errors
        metadata_path = os.path.join(run_dir, "metadata.json")
        with open(metadata_path, "w", encoding="utf-8") as f:
            json.dump(run_results, f, indent=2)

        return run_results


async def run_benchmark(
    model_name: str,
    benchmark_dir: str,
    results_dir: str,
    num_runs: int,
    concurrency: int,
) -> tuple[list[dict], dict]:
    """
    Runs the specified benchmark cases against a model.

    Args:
        model_name: The model identifier for OpenRouter.
        benchmark_dir: Directory containing benchmark prompts.
        results_dir: Directory to store results.
        num_runs: Number of new runs to execute (0=none, -1=all).
        concurrency: Number of concurrent runs to execute.

    Returns:
        A tuple containing:
        - A list of result dictionaries for each run.
        - A summary dictionary with counts and stats.
    """
    # Ensure results directory exists
    os.makedirs(results_dir, exist_ok=True)

    # Find all benchmark cases
    all_case_prefixes = find_benchmark_cases(benchmark_dir)
    if not all_case_prefixes:
        print(f"No benchmark cases found in {benchmark_dir}")
        return [], {"total_cases": 0}

    print(f"Found {len(all_case_prefixes)} benchmark cases.")

    # Check for existing run results
    existing_runs_info = {}
    existing_success_count = 0
    existing_failure_count = 0

    for case_prefix in all_case_prefixes:
        already_run, run_dir, status = check_existing_runs(
            case_prefix, model_name, results_dir
        )
        if already_run:
            existing_runs_info[case_prefix] = {"run_dir": run_dir, "status": status}
            if status == "success":
                existing_success_count += 1
            else:
                existing_failure_count += 1

    print(
        f"Existing runs for {model_name}: "
        f"{existing_success_count} successful, "
        f"{existing_failure_count} failed, "
        f"{len(all_case_prefixes) - existing_success_count - existing_failure_count} never run."
    )

    if num_runs == 0:
        print("No new runs requested. Exiting.")
        return (
            [],
            {
                "total_cases": len(all_case_prefixes),
                "existing_success": existing_success_count,
                "existing_failure": existing_failure_count,
                "never_run": len(all_case_prefixes)
                - existing_success_count
                - existing_failure_count,
                "new_runs_requested": 0,
            },
        )

    # Determine which cases to run
    cases_to_run = []

    if num_runs == -1:
        # Run all cases that haven't been successful
        for case_prefix in all_case_prefixes:
            if (
                case_prefix not in existing_runs_info
                or existing_runs_info[case_prefix]["status"] != "success"
            ):
                cases_to_run.append(case_prefix)
    else:
        # Run a specific number of previously unsuccessful cases
        for case_prefix in all_case_prefixes:
            if (
                case_prefix not in existing_runs_info
                or existing_runs_info[case_prefix]["status"] != "success"
            ):
                cases_to_run.append(case_prefix)
                if len(cases_to_run) >= num_runs:
                    break

    total_to_run = len(cases_to_run)
    print(f"Will run benchmark on {total_to_run} cases with concurrency={concurrency}.")

    if total_to_run == 0:
        print("No new cases to run. All requested cases already successful.")
        return (
            [],
            {
                "total_cases": len(all_case_prefixes),
                "existing_success": existing_success_count,
                "existing_failure": existing_failure_count,
                "never_run": len(all_case_prefixes)
                - existing_success_count
                - existing_failure_count,
                "new_runs_requested": num_runs,
                "new_runs_actual": 0,
            },
        )

    # Tracking for this run
    run_attempt_counter = {
        "total": 0,
        "success": 0,
        "diff_failure": 0,
        "extraction_failure": 0,
        "api_failure": 0,
        "error": 0,
    }

    # Run benchmark cases (concurrently)
    results = []
    semaphore = asyncio.Semaphore(concurrency)

    async def run_with_semaphore(case_prefix):
        async with semaphore:
            return await run_benchmark_case(
                case_prefix,
                model_name,
                benchmark_dir,
                results_dir,
                existing_runs_info,
                run_attempt_counter,
            )

    # Create tasks for all benchmark cases
    tasks = [run_with_semaphore(case_prefix) for case_prefix in cases_to_run]

    # Gather all results
    results = await asyncio.gather(*tasks)

    # Calculate total cost
    total_cost = 0.0
    for result in results:
        if not result.get("skipped", False) and "cost_usd" in result:
            total_cost += result["cost_usd"]

    summary = {
        "total_cases": len(all_case_prefixes),
        "existing_success": existing_success_count,
        "existing_failure": existing_failure_count,
        "never_run": len(all_case_prefixes)
        - existing_success_count
        - existing_failure_count,
        "new_runs_requested": num_runs,
        "new_runs_actual": run_attempt_counter["total"],
        "new_success": run_attempt_counter["success"],
        "new_failures": {
            "diff_failure": run_attempt_counter["diff_failure"],
            "extraction_failure": run_attempt_counter["extraction_failure"],
            "api_failure": run_attempt_counter["api_failure"],
            "other_error": run_attempt_counter["error"],
        },
        "total_cost_usd": total_cost,
    }

    return results, summary


def main():
    parser = argparse.ArgumentParser(
        description="Run LoCoDiff benchmark cases against a specified model via OpenRouter"
    )
    parser.add_argument(
        "--model",
        required=True,
        help="Model identifier for OpenRouter (e.g., 'openai/gpt-4o')",
    )
    parser.add_argument(
        "--benchmark-dir",
        default="generated_prompts",
        help="Directory containing benchmark prompt/expected files",
    )
    parser.add_argument(
        "--results-dir",
        default="benchmark_results",
        help="Directory to save results",
    )
    parser.add_argument(
        "--num-runs",
        type=int,
        default=0,
        help="Maximum number of new benchmark cases to run (0=none/report only, -1=all, N=up to N new)",
    )
    parser.add_argument(
        "--concurrency",
        type=int,
        default=1,
        help="Number of benchmark cases to run concurrently",
    )
    args = parser.parse_args()

    # Check if OpenRouter API key is available
    load_dotenv()
    if not os.getenv("OPENROUTER_API_KEY"):
        print(
            "Error: OPENROUTER_API_KEY not found in environment variables or .env file."
        )
        return 1

    # Run the benchmark asynchronously
    results, summary = asyncio.run(
        run_benchmark(
            args.model,
            args.benchmark_dir,
            args.results_dir,
            args.num_runs,
            args.concurrency,
        )
    )

    # Print summary
    print("\n--- Benchmark Summary ---")
    print(f"Model: {args.model}")
    print(f"Total benchmark cases: {summary['total_cases']}")
    print("\nExisting runs before this session:")
    print(f"  Previously successful: {summary['existing_success']}")
    print(f"  Previously failed: {summary['existing_failure']}")
    print(f"  Never run before: {summary['never_run']}")

    if "new_runs_actual" in summary:
        print("\nNew runs in this session:")
        print(f"  Requested: {summary['new_runs_requested']}")
        print(f"  Actually run: {summary['new_runs_actual']}")
        if summary["new_runs_actual"] > 0:
            print(f"  Successful: {summary['new_success']}")
            print(
                f"  Failed: {summary['new_runs_actual'] - summary['new_success']} ("
                f"diff: {summary['new_failures']['diff_failure']}, "
                f"extraction: {summary['new_failures']['extraction_failure']}, "
                f"API: {summary['new_failures']['api_failure']}, "
                f"other: {summary['new_failures']['other_error']})"
            )
            print(f"  Total cost for new runs: ${summary['total_cost_usd']:.8f}")

    # Overall success rate
    total_success = summary["existing_success"] + summary.get("new_success", 0)
    print(
        f"\nOverall success rate: {total_success}/{summary['total_cases']} "
        f"({total_success / summary['total_cases'] * 100:.1f}%)"
    )

    return 0


if __name__ == "__main__":
    sys.exit(main())
