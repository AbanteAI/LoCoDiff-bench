# LoCoDiff Benchmark Explorer

A simple Flask web application to explore the results generated by the LoCoDiff benchmark scripts.

## Setup

1.  **Install Dependencies:**
    Make sure you have `uv` installed. Then, install the required Python packages from the project root directory:
    ```bash
    uv pip install -r requirements.txt
    ```
    *(This assumes you have already run `./.mentat/setup.sh` at least once, which also installs requirements.)*

2.  **Generate Benchmark Data (if needed):**
    Ensure you have run the benchmark steps:
    *   `./clone_repos.py -r <org/repo1> <org/repo2> ...`
    *   `./generate_prompts.py -e .py .js ...`
    *   `./run_benchmark.py --model <model_identifier> --num-runs -1` (e.g., `openai/gpt-4o`)
    *   `./analyze_results.py` (This generates the plot used by the explorer)

## Running the Explorer

From the root directory of the `LoCoDiff-bench` repository:

```bash
flask --app explorer_app/app.py run --host=0.0.0.0 --port=5001
```

*   `--app explorer_app/app.py`: Specifies the Flask application file.
*   `run`: Starts the development server.
*   `--host=0.0.0.0`: Makes the server accessible on your network (optional). Use `127.0.0.1` or omit for local access only.
*   `--port=5001`: Specifies the port to run on (optional, defaults to 5000).

Then, open your web browser and navigate to `http://localhost:5001` (or `http://<your-ip-address>:5001` if using `0.0.0.0`).

## Features

*   **Overview Page:** Shows summary statistics, lists models with results, and displays the overall success rate plot.
*   **Model Results Page:** Lists all benchmark runs for a specific model, grouped by prompt token bucket. Shows success/failure status and links to detailed views.
*   **Case Details Page:** Displays the full details for a single benchmark run, including:
    *   Run metadata (success, cost, tokens, errors, etc.)
    *   Prompt content
    *   Expected output
    *   Raw model response
    *   Extracted output (if successful)
    *   Diff between expected and extracted output
