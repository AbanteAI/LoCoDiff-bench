<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Prompt: benchmark/problem_stats.py - Grok 4</title>
    <link rel="stylesheet" href="../../../../styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
</head>
<body>
    <header>
        <h1>Prompt: benchmark/problem_stats.py</h1>
        <h2>Model: Grok 4</h2>
        <p><a href="../../../cases/x-ai_grok-4-07-09/aider_benchmark_problem_stats.py.html">Back to Case</a> | <a href="../../../cases.html">All Cases</a> | <a href="../../../index.html">Home</a></p>
    </header>
    <main>
        <section>
            <h2>Prompt Content</h2>
            <pre><code class="language-plaintext"># Instructions

You are being benchmarked. You will see the output of a git log command, and from that must infer the current state of a file. Think carefully, as you must output the exact state of the file to earn full marks.

**Important:** Your goal is to reproduce the file's content *exactly* as it exists at the final commit, even if the code appears broken, buggy, or contains obvious errors. Do **not** try to "fix" the code. Attempting to correct issues will result in a poor score, as this benchmark evaluates your ability to reproduce the precise state of the file based on its history.

# Required Response Format

Wrap the content of the file in triple backticks (```). Any text outside the final closing backticks will be ignored. End your response after outputting the closing backticks.

# Example Response

```python
#!/usr/bin/env python
print('Hello, world!')
```

# File History

> git log -p --cc --topo-order --reverse -- benchmark/problem_stats.py

commit 66e597a05c6e8ae5547f04b8b4550eac6e994543
Author: Paul Gauthier <paul@aider.chat>
Date:   Tue Dec 17 14:06:52 2024 -0800

    feat: Add problem stats benchmark

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
new file mode 100644
index 00000000..e69de29b

commit 9cc674c283d93c1f752380f620e221439b3cb88e
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Tue Dec 17 14:06:53 2024 -0800

    feat: Add script to analyze exercise solution stats

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index e69de29b..daa971fc 100644
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -0,0 +1,82 @@
+#!/usr/bin/env python
+
+import yaml
+from pathlib import Path
+from collections import defaultdict
+import json
+
+def load_results(dirname):
+    """Load all result files from a benchmark directory"""
+    dirname = Path(dirname)
+    benchmark_dir = Path("tmp.benchmarks") / dirname
+    if not benchmark_dir.exists():
+        return None
+    
+    all_results = []
+    for fname in benchmark_dir.glob("*/.aider.results.json"):
+        try:
+            results = json.loads(fname.read_text())
+            all_results.append(results)
+        except json.JSONDecodeError:
+            print(f"Failed to parse {fname}")
+            continue
+    return all_results
+
+def analyze_exercise_solutions():
+    # Load the leaderboard data
+    with open("aider/website/_data/edit_leaderboard.yml") as f:
+        leaderboard = yaml.safe_load(f)
+    
+    # Track which models solved each exercise
+    exercise_solutions = defaultdict(list)
+    
+    for entry in leaderboard:
+        dirname = entry["dirname"]
+        model = entry["model"]
+        
+        results = load_results(dirname)
+        if not results:
+            print(f"Could not load results for {dirname}")
+            continue
+            
+        for result in results:
+            testcase = result.get("testcase")
+            if not testcase:
+                continue
+                
+            # Consider it solved if the last test attempt passed
+            tests_outcomes = result.get("tests_outcomes", [])
+            if tests_outcomes and tests_outcomes[-1]:
+                exercise_solutions[testcase].append(model)
+    
+    # Print statistics
+    print("\nExercise Solution Statistics:")
+    print("-" * 40)
+    
+    # Sort by number of models that solved each exercise
+    sorted_exercises = sorted(
+        exercise_solutions.items(),
+        key=lambda x: len(x[1]),
+        reverse=True
+    )
+    
+    for testcase, models in sorted_exercises:
+        print(f"{testcase}: solved by {len(models)} models")
+        #print(f"  Models: {', '.join(models)}")
+    
+    print("\nSummary:")
+    print(f"Total exercises solved at least once: {len(exercise_solutions)}")
+    never_solved = 133 - len(exercise_solutions)
+    print(f"Never solved by any model: {never_solved}")
+    
+    # Distribution of solutions
+    solved_by_counts = defaultdict(int)
+    for models in exercise_solutions.values():
+        solved_by_counts[len(models)] += 1
+    
+    print("\nDistribution of solutions:")
+    for count in sorted(solved_by_counts.keys()):
+        print(f"Solved by {count} models: {solved_by_counts[count]} exercises")
+
+if __name__ == "__main__":
+    analyze_exercise_solutions()

commit 7bfc2e0e7450cfdd38e090cdab2b96fee706e654
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Tue Dec 17 14:06:56 2024 -0800

    style: Run linter on benchmark script

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index daa971fc..0729f247 100644
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -1,9 +1,11 @@
 #!/usr/bin/env python
 
-import yaml
-from pathlib import Path
-from collections import defaultdict
 import json
+from collections import defaultdict
+from pathlib import Path
+
+import yaml
+
 
 def load_results(dirname):
     """Load all result files from a benchmark directory"""
@@ -11,7 +13,7 @@ def load_results(dirname):
     benchmark_dir = Path("tmp.benchmarks") / dirname
     if not benchmark_dir.exists():
         return None
-    
+
     all_results = []
     for fname in benchmark_dir.glob("*/.aider.results.json"):
         try:
@@ -22,61 +24,59 @@ def load_results(dirname):
             continue
     return all_results
 
+
 def analyze_exercise_solutions():
     # Load the leaderboard data
     with open("aider/website/_data/edit_leaderboard.yml") as f:
         leaderboard = yaml.safe_load(f)
-    
+
     # Track which models solved each exercise
     exercise_solutions = defaultdict(list)
-    
+
     for entry in leaderboard:
         dirname = entry["dirname"]
         model = entry["model"]
-        
+
         results = load_results(dirname)
         if not results:
             print(f"Could not load results for {dirname}")
             continue
-            
+
         for result in results:
             testcase = result.get("testcase")
             if not testcase:
                 continue
-                
+
             # Consider it solved if the last test attempt passed
             tests_outcomes = result.get("tests_outcomes", [])
             if tests_outcomes and tests_outcomes[-1]:
                 exercise_solutions[testcase].append(model)
-    
+
     # Print statistics
     print("\nExercise Solution Statistics:")
     print("-" * 40)
-    
+
     # Sort by number of models that solved each exercise
-    sorted_exercises = sorted(
-        exercise_solutions.items(),
-        key=lambda x: len(x[1]),
-        reverse=True
-    )
-    
+    sorted_exercises = sorted(exercise_solutions.items(), key=lambda x: len(x[1]), reverse=True)
+
     for testcase, models in sorted_exercises:
         print(f"{testcase}: solved by {len(models)} models")
-        #print(f"  Models: {', '.join(models)}")
-    
+        # print(f"  Models: {', '.join(models)}")
+
     print("\nSummary:")
     print(f"Total exercises solved at least once: {len(exercise_solutions)}")
     never_solved = 133 - len(exercise_solutions)
     print(f"Never solved by any model: {never_solved}")
-    
+
     # Distribution of solutions
     solved_by_counts = defaultdict(int)
     for models in exercise_solutions.values():
         solved_by_counts[len(models)] += 1
-    
+
     print("\nDistribution of solutions:")
     for count in sorted(solved_by_counts.keys()):
         print(f"Solved by {count} models: {solved_by_counts[count]} exercises")
 
+
 if __name__ == "__main__":
     analyze_exercise_solutions()

commit c69ffe02f8b672533fef00146959bbbd0df5d010
Author: Paul Gauthier <paul@aider.chat>
Date:   Tue Dec 17 14:08:46 2024 -0800

    chore: Make problem_stats.py executable

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
old mode 100644
new mode 100755

commit 0ae53ce1a1bfc343258e728d61a8e62e1b177e92
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Tue Dec 17 14:08:47 2024 -0800

    feat: Output per-exercise stats, sort by solvers

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 0729f247..810b48f4 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -52,7 +52,7 @@ def analyze_exercise_solutions():
             if tests_outcomes and tests_outcomes[-1]:
                 exercise_solutions[testcase].append(model)
 
-    # Print statistics
+    # Print per-exercise statistics
     print("\nExercise Solution Statistics:")
     print("-" * 40)
 
@@ -60,8 +60,7 @@ def analyze_exercise_solutions():
     sorted_exercises = sorted(exercise_solutions.items(), key=lambda x: len(x[1]), reverse=True)
 
     for testcase, models in sorted_exercises:
-        print(f"{testcase}: solved by {len(models)} models")
-        # print(f"  Models: {', '.join(models)}")
+        print(f"{testcase}: {len(models)} solved")
 
     print("\nSummary:")
     print(f"Total exercises solved at least once: {len(exercise_solutions)}")

commit 54c15538923fb71ff69e35759b421ba897fc1a69
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Tue Dec 17 14:09:17 2024 -0800

    refactor: Remove distribution of solutions table

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 810b48f4..ba18cf44 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -67,15 +67,5 @@ def analyze_exercise_solutions():
     never_solved = 133 - len(exercise_solutions)
     print(f"Never solved by any model: {never_solved}")
 
-    # Distribution of solutions
-    solved_by_counts = defaultdict(int)
-    for models in exercise_solutions.values():
-        solved_by_counts[len(models)] += 1
-
-    print("\nDistribution of solutions:")
-    for count in sorted(solved_by_counts.keys()):
-        print(f"Solved by {count} models: {solved_by_counts[count]} exercises")
-
-
 if __name__ == "__main__":
     analyze_exercise_solutions()

commit 24599aa64f4a8c2fbe9b983cbae24c011189978b
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Tue Dec 17 14:09:20 2024 -0800

    style: Run linter on problem_stats.py

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index ba18cf44..4469fc5b 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -67,5 +67,6 @@ def analyze_exercise_solutions():
     never_solved = 133 - len(exercise_solutions)
     print(f"Never solved by any model: {never_solved}")
 
+
 if __name__ == "__main__":
     analyze_exercise_solutions()

commit 1d7cb0c119864199065d06cf3415f64ec3874228
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Tue Dec 17 14:10:00 2024 -0800

    feat: Format problem stats output as a table with percentages

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 4469fc5b..7b53d7e4 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -59,8 +59,14 @@ def analyze_exercise_solutions():
     # Sort by number of models that solved each exercise
     sorted_exercises = sorted(exercise_solutions.items(), key=lambda x: len(x[1]), reverse=True)
 
+    # Calculate max length for alignment
+    max_name_len = max(len(testcase) for testcase, _ in sorted_exercises)
+    total_models = len({model for models in exercise_solutions.values() for model in models})
+
     for testcase, models in sorted_exercises:
-        print(f"{testcase}: {len(models)} solved")
+        num_solved = len(models)
+        percent = (num_solved / total_models) * 100
+        print(f"{testcase:<{max_name_len}} : {num_solved:>3} solved ({percent:>5.1f}%)")
 
     print("\nSummary:")
     print(f"Total exercises solved at least once: {len(exercise_solutions)}")

commit 91f5fca5e98e73521672c4402e0ab71aa216b436
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Tue Dec 17 14:10:47 2024 -0800

    feat: Include never solved exercises in stats

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 7b53d7e4..15da88c3 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -30,9 +30,18 @@ def analyze_exercise_solutions():
     with open("aider/website/_data/edit_leaderboard.yml") as f:
         leaderboard = yaml.safe_load(f)
 
-    # Track which models solved each exercise
+    # Get all exercise names from a complete run
+    all_exercises = set()
     exercise_solutions = defaultdict(list)
 
+    # Find a complete run to get all exercise names
+    for entry in leaderboard:
+        dirname = entry["dirname"]
+        results = load_results(dirname)
+        if results and len(results) == 133:  # Complete run
+            all_exercises = {result["testcase"] for result in results}
+            break
+
     for entry in leaderboard:
         dirname = entry["dirname"]
         model = entry["model"]
@@ -56,11 +65,16 @@ def analyze_exercise_solutions():
     print("\nExercise Solution Statistics:")
     print("-" * 40)
 
+    # Add exercises that were never solved
+    for exercise in all_exercises:
+        if exercise not in exercise_solutions:
+            exercise_solutions[exercise] = []
+
     # Sort by number of models that solved each exercise
     sorted_exercises = sorted(exercise_solutions.items(), key=lambda x: len(x[1]), reverse=True)
 
     # Calculate max length for alignment
-    max_name_len = max(len(testcase) for testcase, _ in sorted_exercises)
+    max_name_len = max(len(testcase) for testcase in all_exercises)
     total_models = len({model for models in exercise_solutions.values() for model in models})
 
     for testcase, models in sorted_exercises:

commit 00d7c3a05ae7b17ce55748739995f8d9f1536917
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Tue Dec 17 14:13:16 2024 -0800

    feat: Add --topn argument to limit models by pass rate

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 15da88c3..ad4790f5 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -1,5 +1,6 @@
 #!/usr/bin/env python
 
+import argparse
 import json
 from collections import defaultdict
 from pathlib import Path
@@ -25,11 +26,16 @@ def load_results(dirname):
     return all_results
 
 
-def analyze_exercise_solutions():
+def analyze_exercise_solutions(topn=None):
     # Load the leaderboard data
     with open("aider/website/_data/edit_leaderboard.yml") as f:
         leaderboard = yaml.safe_load(f)
 
+    # Sort models by pass rate to get top N if specified
+    if topn:
+        leaderboard.sort(key=lambda x: float(x.get('pass_rate_2', '0').rstrip('%')), reverse=True)
+        leaderboard = leaderboard[:topn]
+
     # Get all exercise names from a complete run
     all_exercises = set()
     exercise_solutions = defaultdict(list)
@@ -89,4 +95,8 @@ def analyze_exercise_solutions():
 
 
 if __name__ == "__main__":
-    analyze_exercise_solutions()
+    parser = argparse.ArgumentParser()
+    parser.add_argument('--topn', type=int, help='Only consider top N models by pass rate')
+    args = parser.parse_args()
+    
+    analyze_exercise_solutions(args.topn)

commit 3a0be0cca9b0dde014489892aca0fd1ecb9113ab
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Tue Dec 17 14:13:19 2024 -0800

    style: Apply linter formatting

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index ad4790f5..80c5316a 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -33,7 +33,7 @@ def analyze_exercise_solutions(topn=None):
 
     # Sort models by pass rate to get top N if specified
     if topn:
-        leaderboard.sort(key=lambda x: float(x.get('pass_rate_2', '0').rstrip('%')), reverse=True)
+        leaderboard.sort(key=lambda x: float(x.get("pass_rate_2", "0").rstrip("%")), reverse=True)
         leaderboard = leaderboard[:topn]
 
     # Get all exercise names from a complete run
@@ -96,7 +96,7 @@ def analyze_exercise_solutions(topn=None):
 
 if __name__ == "__main__":
     parser = argparse.ArgumentParser()
-    parser.add_argument('--topn', type=int, help='Only consider top N models by pass rate')
+    parser.add_argument("--topn", type=int, help="Only consider top N models by pass rate")
     args = parser.parse_args()
-    
+
     analyze_exercise_solutions(args.topn)

commit 674e3846e23f14ddec9a21e7790d3dfc38c0064d
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Tue Dec 17 14:13:43 2024 -0800

    fix: Correctly sort leaderboard by pass rate

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 80c5316a..f5c2a4e6 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -33,7 +33,7 @@ def analyze_exercise_solutions(topn=None):
 
     # Sort models by pass rate to get top N if specified
     if topn:
-        leaderboard.sort(key=lambda x: float(x.get("pass_rate_2", "0").rstrip("%")), reverse=True)
+        leaderboard.sort(key=lambda x: float(x.get("pass_rate_2", 0)), reverse=True)
         leaderboard = leaderboard[:topn]
 
     # Get all exercise names from a complete run

commit 7f16757bbe26c76a538b9da5f52e3becc32b6afe
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Tue Dec 17 14:15:02 2024 -0800

    fix: Handle missing results in topn leaderboard calculation

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index f5c2a4e6..4924fa37 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -31,11 +31,22 @@ def analyze_exercise_solutions(topn=None):
     with open("aider/website/_data/edit_leaderboard.yml") as f:
         leaderboard = yaml.safe_load(f)
 
-    # Sort models by pass rate to get top N if specified
+    # Filter out entries that don't load and sort by pass rate
+    valid_entries = []
+    for entry in leaderboard:
+        dirname = entry["dirname"]
+        results = load_results(dirname)
+        if results:
+            valid_entries.append((entry, results))
+
+    # Sort by pass rate and take top N if specified
+    valid_entries.sort(key=lambda x: float(x[0].get("pass_rate_2", 0)), reverse=True)
     if topn:
-        leaderboard.sort(key=lambda x: float(x.get("pass_rate_2", 0)), reverse=True)
-        leaderboard = leaderboard[:topn]
+        valid_entries = valid_entries[:topn]
 
+    # Unpack the filtered and sorted entries
+    leaderboard = [entry for entry, _ in valid_entries]
+    
     # Get all exercise names from a complete run
     all_exercises = set()
     exercise_solutions = defaultdict(list)
@@ -81,7 +92,7 @@ def analyze_exercise_solutions(topn=None):
 
     # Calculate max length for alignment
     max_name_len = max(len(testcase) for testcase in all_exercises)
-    total_models = len({model for models in exercise_solutions.values() for model in models})
+    total_models = len(leaderboard)
 
     for testcase, models in sorted_exercises:
         num_solved = len(models)

commit cac5d8e7169e67b1857b24ab3f6ec2de9ae14e25
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Tue Dec 17 14:15:06 2024 -0800

    style: Apply linter formatting

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 4924fa37..d0e09edc 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -46,7 +46,7 @@ def analyze_exercise_solutions(topn=None):
 
     # Unpack the filtered and sorted entries
     leaderboard = [entry for entry, _ in valid_entries]
-    
+
     # Get all exercise names from a complete run
     all_exercises = set()
     exercise_solutions = defaultdict(list)

commit 59308c20c651daa64d9f4ed700ed76598eca74a8
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Tue Dec 17 14:15:40 2024 -0800

    feat: Number exercises in the table

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index d0e09edc..910bfc3b 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -94,10 +94,10 @@ def analyze_exercise_solutions(topn=None):
     max_name_len = max(len(testcase) for testcase in all_exercises)
     total_models = len(leaderboard)
 
-    for testcase, models in sorted_exercises:
+    for i, (testcase, models) in enumerate(sorted_exercises, 1):
         num_solved = len(models)
         percent = (num_solved / total_models) * 100
-        print(f"{testcase:<{max_name_len}} : {num_solved:>3} solved ({percent:>5.1f}%)")
+        print(f"{i:>3}. {testcase:<{max_name_len}} : {num_solved:>3} solved ({percent:>5.1f}%)")
 
     print("\nSummary:")
     print(f"Total exercises solved at least once: {len(exercise_solutions)}")

commit 5c55453a0eee092e3ed8d4ce31aac08b9c285891
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Tue Dec 17 18:14:48 2024 -0800

    fix: Correctly calculate and display never solved exercises

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 910bfc3b..46f43e60 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -78,6 +78,9 @@ def analyze_exercise_solutions(topn=None):
             if tests_outcomes and tests_outcomes[-1]:
                 exercise_solutions[testcase].append(model)
 
+    # Calculate never solved exercises
+    never_solved = len(all_exercises - set(exercise_solutions.keys()))
+
     # Print per-exercise statistics
     print("\nExercise Solution Statistics:")
     print("-" * 40)
@@ -100,8 +103,8 @@ def analyze_exercise_solutions(topn=None):
         print(f"{i:>3}. {testcase:<{max_name_len}} : {num_solved:>3} solved ({percent:>5.1f}%)")
 
     print("\nSummary:")
-    print(f"Total exercises solved at least once: {len(exercise_solutions)}")
-    never_solved = 133 - len(exercise_solutions)
+    solved_at_least_once = len([ex for ex, models in exercise_solutions.items() if models])
+    print(f"Total exercises solved at least once: {solved_at_least_once}")
     print(f"Never solved by any model: {never_solved}")
 
 

commit a19f1fbc67ba003d9cac4daf941648d0ae356f54
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 12:21:13 2024 -0800

    feat: Allow specifying dirs on cmd line for problem_stats

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 46f43e60..023a11b6 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -8,6 +8,13 @@ from pathlib import Path
 import yaml
 
 
+def get_dirs_from_leaderboard():
+    # Load the leaderboard data
+    with open("aider/website/_data/edit_leaderboard.yml") as f:
+        leaderboard = yaml.safe_load(f)
+    return [(entry["dirname"], entry["model"]) for entry in leaderboard]
+
+
 def load_results(dirname):
     """Load all result files from a benchmark directory"""
     dirname = Path(dirname)
@@ -26,44 +33,44 @@ def load_results(dirname):
     return all_results
 
 
-def analyze_exercise_solutions(topn=None):
-    # Load the leaderboard data
-    with open("aider/website/_data/edit_leaderboard.yml") as f:
-        leaderboard = yaml.safe_load(f)
+def analyze_exercise_solutions(dirs=None, topn=None):
+    if dirs is None:
+        # Use leaderboard data if no directories specified
+        dir_entries = get_dirs_from_leaderboard()
+    else:
+        # Use provided directories, with dirname as model name
+        dir_entries = [(d, d) for d in dirs]
 
     # Filter out entries that don't load and sort by pass rate
     valid_entries = []
-    for entry in leaderboard:
-        dirname = entry["dirname"]
+    for dirname, model in dir_entries:
         results = load_results(dirname)
         if results:
-            valid_entries.append((entry, results))
+            # Calculate pass rate for sorting when using custom dirs
+            if dirs is not None:
+                pass_rate = sum(1 for r in results if r.get("tests_outcomes", []) and r["tests_outcomes"][-1]) / len(results)
+            else:
+                # Use existing pass rate from leaderboard
+                pass_rate = next((entry["pass_rate_2"] for entry in yaml.safe_load(open("aider/website/_data/edit_leaderboard.yml")) 
+                                if entry["dirname"] == dirname), 0)
+            valid_entries.append(((dirname, model), results, float(pass_rate)))
 
     # Sort by pass rate and take top N if specified
-    valid_entries.sort(key=lambda x: float(x[0].get("pass_rate_2", 0)), reverse=True)
+    valid_entries.sort(key=lambda x: x[2], reverse=True)
     if topn:
         valid_entries = valid_entries[:topn]
 
-    # Unpack the filtered and sorted entries
-    leaderboard = [entry for entry, _ in valid_entries]
-
     # Get all exercise names from a complete run
     all_exercises = set()
     exercise_solutions = defaultdict(list)
 
     # Find a complete run to get all exercise names
-    for entry in leaderboard:
-        dirname = entry["dirname"]
-        results = load_results(dirname)
+    for (dirname, model), results, _ in valid_entries:
         if results and len(results) == 133:  # Complete run
             all_exercises = {result["testcase"] for result in results}
             break
 
-    for entry in leaderboard:
-        dirname = entry["dirname"]
-        model = entry["model"]
-
-        results = load_results(dirname)
+    for (dirname, model), results, _ in valid_entries:
         if not results:
             print(f"Could not load results for {dirname}")
             continue
@@ -95,7 +102,7 @@ def analyze_exercise_solutions(topn=None):
 
     # Calculate max length for alignment
     max_name_len = max(len(testcase) for testcase in all_exercises)
-    total_models = len(leaderboard)
+    total_models = len(valid_entries)
 
     for i, (testcase, models) in enumerate(sorted_exercises, 1):
         num_solved = len(models)
@@ -111,6 +118,7 @@ def analyze_exercise_solutions(topn=None):
 if __name__ == "__main__":
     parser = argparse.ArgumentParser()
     parser.add_argument("--topn", type=int, help="Only consider top N models by pass rate")
+    parser.add_argument("dirs", nargs="*", help="Directories to analyze (optional, defaults to leaderboard entries)")
     args = parser.parse_args()
 
-    analyze_exercise_solutions(args.topn)
+    analyze_exercise_solutions(args.dirs if args.dirs else None, args.topn)

commit 0a3e0665ab58c9dc81bba35bb4392651174d4223
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 12:21:19 2024 -0800

    style: Apply linter formatting

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 023a11b6..eb15c926 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -48,11 +48,21 @@ def analyze_exercise_solutions(dirs=None, topn=None):
         if results:
             # Calculate pass rate for sorting when using custom dirs
             if dirs is not None:
-                pass_rate = sum(1 for r in results if r.get("tests_outcomes", []) and r["tests_outcomes"][-1]) / len(results)
+                pass_rate = sum(
+                    1 for r in results if r.get("tests_outcomes", []) and r["tests_outcomes"][-1]
+                ) / len(results)
             else:
                 # Use existing pass rate from leaderboard
-                pass_rate = next((entry["pass_rate_2"] for entry in yaml.safe_load(open("aider/website/_data/edit_leaderboard.yml")) 
-                                if entry["dirname"] == dirname), 0)
+                pass_rate = next(
+                    (
+                        entry["pass_rate_2"]
+                        for entry in yaml.safe_load(
+                            open("aider/website/_data/edit_leaderboard.yml")
+                        )
+                        if entry["dirname"] == dirname
+                    ),
+                    0,
+                )
             valid_entries.append(((dirname, model), results, float(pass_rate)))
 
     # Sort by pass rate and take top N if specified
@@ -118,7 +128,9 @@ def analyze_exercise_solutions(dirs=None, topn=None):
 if __name__ == "__main__":
     parser = argparse.ArgumentParser()
     parser.add_argument("--topn", type=int, help="Only consider top N models by pass rate")
-    parser.add_argument("dirs", nargs="*", help="Directories to analyze (optional, defaults to leaderboard entries)")
+    parser.add_argument(
+        "dirs", nargs="*", help="Directories to analyze (optional, defaults to leaderboard entries)"
+    )
     args = parser.parse_args()
 
     analyze_exercise_solutions(args.dirs if args.dirs else None, args.topn)

commit 6d74a564e621c6be56024597aee348a793d52239
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 12:25:33 2024 -0800

    feat: Handle new exercise dir layout and add language info

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index eb15c926..fe07903c 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -23,9 +23,13 @@ def load_results(dirname):
         return None
 
     all_results = []
-    for fname in benchmark_dir.glob("*/.aider.results.json"):
+    # Look in language subdirectories under exercises/practice
+    for fname in benchmark_dir.glob("*/exercises/practice/*/.aider.results.json"):
         try:
             results = json.loads(fname.read_text())
+            # Add language info to results
+            lang = fname.parts[-4]  # Get language from path
+            results['language'] = lang
             all_results.append(results)
         except json.JSONDecodeError:
             print(f"Failed to parse {fname}")
@@ -107,17 +111,32 @@ def analyze_exercise_solutions(dirs=None, topn=None):
         if exercise not in exercise_solutions:
             exercise_solutions[exercise] = []
 
-    # Sort by number of models that solved each exercise
-    sorted_exercises = sorted(exercise_solutions.items(), key=lambda x: len(x[1]), reverse=True)
-
-    # Calculate max length for alignment
+    # Group exercises by language
+    by_language = defaultdict(list)
+    for testcase in all_exercises:
+        # Find language for this testcase from results
+        lang = next((r['language'] for r in next(iter(valid_entries))[1] if r['testcase'] == testcase), 'unknown')
+        by_language[lang].append(testcase)
+
+    # Sort languages
+    sorted_languages = sorted(by_language.keys())
+    
+    # Calculate max lengths for alignment
     max_name_len = max(len(testcase) for testcase in all_exercises)
+    max_lang_len = max(len(lang) for lang in sorted_languages)
     total_models = len(valid_entries)
 
-    for i, (testcase, models) in enumerate(sorted_exercises, 1):
-        num_solved = len(models)
-        percent = (num_solved / total_models) * 100
-        print(f"{i:>3}. {testcase:<{max_name_len}} : {num_solved:>3} solved ({percent:>5.1f}%)")
+    # Print exercises grouped by language
+    for lang in sorted_languages:
+        print(f"\n{lang.upper()}:")
+        lang_exercises = [(ex, exercise_solutions[ex]) for ex in by_language[lang]]
+        # Sort by number of models that solved each exercise
+        lang_exercises.sort(key=lambda x: len(x[1]), reverse=True)
+        
+        for i, (testcase, models) in enumerate(lang_exercises, 1):
+            num_solved = len(models)
+            percent = (num_solved / total_models) * 100
+            print(f"{i:>3}. {testcase:<{max_name_len}} : {num_solved:>3} solved ({percent:>5.1f}%)")
 
     print("\nSummary:")
     solved_at_least_once = len([ex for ex, models in exercise_solutions.items() if models])

commit 687ba8c9a2a1d148745f2eaf14a496df7d4a360b
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 12:25:39 2024 -0800

    style: Apply linter fixes

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index fe07903c..81dacf5a 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -29,7 +29,7 @@ def load_results(dirname):
             results = json.loads(fname.read_text())
             # Add language info to results
             lang = fname.parts[-4]  # Get language from path
-            results['language'] = lang
+            results["language"] = lang
             all_results.append(results)
         except json.JSONDecodeError:
             print(f"Failed to parse {fname}")
@@ -115,12 +115,15 @@ def analyze_exercise_solutions(dirs=None, topn=None):
     by_language = defaultdict(list)
     for testcase in all_exercises:
         # Find language for this testcase from results
-        lang = next((r['language'] for r in next(iter(valid_entries))[1] if r['testcase'] == testcase), 'unknown')
+        lang = next(
+            (r["language"] for r in next(iter(valid_entries))[1] if r["testcase"] == testcase),
+            "unknown",
+        )
         by_language[lang].append(testcase)
 
     # Sort languages
     sorted_languages = sorted(by_language.keys())
-    
+
     # Calculate max lengths for alignment
     max_name_len = max(len(testcase) for testcase in all_exercises)
     max_lang_len = max(len(lang) for lang in sorted_languages)
@@ -132,7 +135,7 @@ def analyze_exercise_solutions(dirs=None, topn=None):
         lang_exercises = [(ex, exercise_solutions[ex]) for ex in by_language[lang]]
         # Sort by number of models that solved each exercise
         lang_exercises.sort(key=lambda x: len(x[1]), reverse=True)
-        
+
         for i, (testcase, models) in enumerate(lang_exercises, 1):
             num_solved = len(models)
             percent = (num_solved / total_models) * 100

commit 81d424f475a774b009b7a5d497babd01062a6d42
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 12:25:47 2024 -0800

    fix: Remove unused max_lang_len variable

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 81dacf5a..47f76658 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -126,7 +126,6 @@ def analyze_exercise_solutions(dirs=None, topn=None):
 
     # Calculate max lengths for alignment
     max_name_len = max(len(testcase) for testcase in all_exercises)
-    max_lang_len = max(len(lang) for lang in sorted_languages)
     total_models = len(valid_entries)
 
     # Print exercises grouped by language

commit 236a7f68e90351ba46dd4bc0aaaffd4215c5917f
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 12:26:51 2024 -0800

    fix: Handle empty results in problem_stats.py

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 47f76658..ac08daeb 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -78,11 +78,11 @@ def analyze_exercise_solutions(dirs=None, topn=None):
     all_exercises = set()
     exercise_solutions = defaultdict(list)
 
-    # Find a complete run to get all exercise names
+    # Get all unique exercise names from all results
+    all_exercises = set()
     for (dirname, model), results, _ in valid_entries:
-        if results and len(results) == 133:  # Complete run
-            all_exercises = {result["testcase"] for result in results}
-            break
+        if results:
+            all_exercises.update(result["testcase"] for result in results)
 
     for (dirname, model), results, _ in valid_entries:
         if not results:

commit 57a8eab1c34ceb77d90ce4ddc8f802dd5fdac9e2
Author: Paul Gauthier <paul@aider.chat>
Date:   Wed Dec 18 12:29:14 2024 -0800

    chore: Add dump for debugging

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index ac08daeb..78f072be 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -4,7 +4,7 @@ import argparse
 import json
 from collections import defaultdict
 from pathlib import Path
-
+from aider.dump import dump
 import yaml
 
 
@@ -48,6 +48,7 @@ def analyze_exercise_solutions(dirs=None, topn=None):
     # Filter out entries that don't load and sort by pass rate
     valid_entries = []
     for dirname, model in dir_entries:
+        dump(dirname, model)
         results = load_results(dirname)
         if results:
             # Calculate pass rate for sorting when using custom dirs

commit 7a27e2b94b01c9fa4f7862bfc0ffb9d2d650a560
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 12:29:15 2024 -0800

    fix: Handle missing testcase key in problem stats

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 78f072be..85f5fa19 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -83,7 +83,11 @@ def analyze_exercise_solutions(dirs=None, topn=None):
     all_exercises = set()
     for (dirname, model), results, _ in valid_entries:
         if results:
-            all_exercises.update(result["testcase"] for result in results)
+            for result in results:
+                try:
+                    all_exercises.add(result["testcase"])
+                except KeyError:
+                    print(f"Warning: Missing testcase in {dirname}")
 
     for (dirname, model), results, _ in valid_entries:
         if not results:

commit 291d8cd33526400383aa96e8031e8eb89cc31a8b
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 12:29:20 2024 -0800

    style: Run linter

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 85f5fa19..13ad95ab 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -4,9 +4,11 @@ import argparse
 import json
 from collections import defaultdict
 from pathlib import Path
-from aider.dump import dump
+
 import yaml
 
+from aider.dump import dump
+
 
 def get_dirs_from_leaderboard():
     # Load the leaderboard data

commit 65133b2aef85f2e94755e439777e99622ba75644
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 12:29:38 2024 -0800

    fix: Handle missing testcase key in problem stats

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 13ad95ab..adbf9721 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -122,10 +122,14 @@ def analyze_exercise_solutions(dirs=None, topn=None):
     by_language = defaultdict(list)
     for testcase in all_exercises:
         # Find language for this testcase from results
-        lang = next(
-            (r["language"] for r in next(iter(valid_entries))[1] if r["testcase"] == testcase),
-            "unknown",
-        )
+        lang = "unknown"
+        for r in next(iter(valid_entries))[1]:
+            try:
+                if r.get("testcase") == testcase:
+                    lang = r["language"]
+                    break
+            except KeyError:
+                continue
         by_language[lang].append(testcase)
 
     # Sort languages

commit 58812f7f1ff66790b2bb1399aeddebcc53f20eb6
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 12:31:50 2024 -0800

    refactor: Sort all exercises by solve rate, not by language

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index adbf9721..840094cb 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -118,10 +118,12 @@ def analyze_exercise_solutions(dirs=None, topn=None):
         if exercise not in exercise_solutions:
             exercise_solutions[exercise] = []
 
-    # Group exercises by language
-    by_language = defaultdict(list)
+    # Create list of (language, exercise) pairs with solution stats
+    exercise_stats = []
+    total_models = len(valid_entries)
+
     for testcase in all_exercises:
-        # Find language for this testcase from results
+        # Find language for this testcase
         lang = "unknown"
         for r in next(iter(valid_entries))[1]:
             try:
@@ -130,26 +132,22 @@ def analyze_exercise_solutions(dirs=None, topn=None):
                     break
             except KeyError:
                 continue
-        by_language[lang].append(testcase)
+        
+        models = exercise_solutions[testcase]
+        num_solved = len(models)
+        percent = (num_solved / total_models) * 100
+        exercise_stats.append((lang, testcase, num_solved, percent))
 
-    # Sort languages
-    sorted_languages = sorted(by_language.keys())
+    # Sort all exercises by solve rate
+    exercise_stats.sort(key=lambda x: x[2], reverse=True)
 
     # Calculate max lengths for alignment
-    max_name_len = max(len(testcase) for testcase in all_exercises)
-    total_models = len(valid_entries)
+    max_name_len = max(len(f"{lang}/{ex}") for lang, ex, _, _ in exercise_stats)
 
-    # Print exercises grouped by language
-    for lang in sorted_languages:
-        print(f"\n{lang.upper()}:")
-        lang_exercises = [(ex, exercise_solutions[ex]) for ex in by_language[lang]]
-        # Sort by number of models that solved each exercise
-        lang_exercises.sort(key=lambda x: len(x[1]), reverse=True)
-
-        for i, (testcase, models) in enumerate(lang_exercises, 1):
-            num_solved = len(models)
-            percent = (num_solved / total_models) * 100
-            print(f"{i:>3}. {testcase:<{max_name_len}} : {num_solved:>3} solved ({percent:>5.1f}%)")
+    # Print all exercises sorted by solve rate
+    print("\nAll Exercises (sorted by solve rate):")
+    for i, (lang, testcase, num_solved, percent) in enumerate(exercise_stats, 1):
+        print(f"{i:>3}. {lang}/{testcase:<{max_name_len}} : {num_solved:>3} solved ({percent:>5.1f}%)")
 
     print("\nSummary:")
     solved_at_least_once = len([ex for ex, models in exercise_solutions.items() if models])

commit 7ad0d46c11d476524438c837688c87646d0b11cd
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 12:31:54 2024 -0800

    style: Format problem_stats.py with linter

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 840094cb..464cca70 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -132,7 +132,7 @@ def analyze_exercise_solutions(dirs=None, topn=None):
                     break
             except KeyError:
                 continue
-        
+
         models = exercise_solutions[testcase]
         num_solved = len(models)
         percent = (num_solved / total_models) * 100
@@ -147,7 +147,9 @@ def analyze_exercise_solutions(dirs=None, topn=None):
     # Print all exercises sorted by solve rate
     print("\nAll Exercises (sorted by solve rate):")
     for i, (lang, testcase, num_solved, percent) in enumerate(exercise_stats, 1):
-        print(f"{i:>3}. {lang}/{testcase:<{max_name_len}} : {num_solved:>3} solved ({percent:>5.1f}%)")
+        print(
+            f"{i:>3}. {lang}/{testcase:<{max_name_len}} : {num_solved:>3} solved ({percent:>5.1f}%)"
+        )
 
     print("\nSummary:")
     solved_at_least_once = len([ex for ex, models in exercise_solutions.items() if models])

commit 42d8650058ecb0665d16e1ba212b7fc9108d77fc
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 12:33:41 2024 -0800

    fix: Remove "exercises/" prefix from testcase output

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 464cca70..6dadb9ea 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -136,6 +136,7 @@ def analyze_exercise_solutions(dirs=None, topn=None):
         models = exercise_solutions[testcase]
         num_solved = len(models)
         percent = (num_solved / total_models) * 100
+        testcase = testcase.replace("exercises/", "")  # Remove the exercises/ prefix
         exercise_stats.append((lang, testcase, num_solved, percent))
 
     # Sort all exercises by solve rate

commit a168403d683deb8bd5530bf2e048952dcd3033ee
Author: Paul Gauthier <paul@aider.chat>
Date:   Wed Dec 18 12:38:40 2024 -0800

    fix: Correctly extract language and testcase from results

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 6dadb9ea..1e857179 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -30,8 +30,9 @@ def load_results(dirname):
         try:
             results = json.loads(fname.read_text())
             # Add language info to results
-            lang = fname.parts[-4]  # Get language from path
+            lang = fname.parts[-5]  # Get language from path
             results["language"] = lang
+            dump(results)
             all_results.append(results)
         except json.JSONDecodeError:
             print(f"Failed to parse {fname}")
@@ -87,7 +88,7 @@ def analyze_exercise_solutions(dirs=None, topn=None):
         if results:
             for result in results:
                 try:
-                    all_exercises.add(result["testcase"])
+                    all_exercises.add(result["language"] + "/" + result["testcase"])
                 except KeyError:
                     print(f"Warning: Missing testcase in {dirname}")
 
@@ -100,7 +101,11 @@ def analyze_exercise_solutions(dirs=None, topn=None):
             testcase = result.get("testcase")
             if not testcase:
                 continue
+            lang = result.get("language")
+            if not lang:
+                continue
 
+            testcase = f"{lang}/{testcase}"
             # Consider it solved if the last test attempt passed
             tests_outcomes = result.get("tests_outcomes", [])
             if tests_outcomes and tests_outcomes[-1]:

commit b8647c04819af749f876690d0fd398c15ffccf02
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 12:38:41 2024 -0800

    feat: Show exercises solved by all and total breakdown

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 1e857179..c231c215 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -159,8 +159,14 @@ def analyze_exercise_solutions(dirs=None, topn=None):
 
     print("\nSummary:")
     solved_at_least_once = len([ex for ex, models in exercise_solutions.items() if models])
+    solved_by_none = never_solved
+    solved_by_all = len([ex for ex, models in exercise_solutions.items() if len(models) == total_models])
+    
     print(f"Total exercises solved at least once: {solved_at_least_once}")
-    print(f"Never solved by any model: {never_solved}")
+    print(f"Never solved by any model: {solved_by_none}")
+    print(f"Solved by all models: {solved_by_all}")
+    print(f"Total exercises: {len(all_exercises)} = {solved_by_none} (none) + {solved_by_all} (all) + "
+          f"{len(all_exercises) - solved_by_none - solved_by_all} (some)")
 
 
 if __name__ == "__main__":

commit 8302b351ddf074f4cfb4f213c5c07e6c46376f26
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 12:38:45 2024 -0800

    style: Fix line length in problem_stats.py

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index c231c215..992b4cc2 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -160,13 +160,17 @@ def analyze_exercise_solutions(dirs=None, topn=None):
     print("\nSummary:")
     solved_at_least_once = len([ex for ex, models in exercise_solutions.items() if models])
     solved_by_none = never_solved
-    solved_by_all = len([ex for ex, models in exercise_solutions.items() if len(models) == total_models])
-    
+    solved_by_all = len(
+        [ex for ex, models in exercise_solutions.items() if len(models) == total_models]
+    )
+
     print(f"Total exercises solved at least once: {solved_at_least_once}")
     print(f"Never solved by any model: {solved_by_none}")
     print(f"Solved by all models: {solved_by_all}")
-    print(f"Total exercises: {len(all_exercises)} = {solved_by_none} (none) + {solved_by_all} (all) + "
-          f"{len(all_exercises) - solved_by_none - solved_by_all} (some)")
+    print(
+        f"Total exercises: {len(all_exercises)} = {solved_by_none} (none) + {solved_by_all} (all) +"
+        f" {len(all_exercises) - solved_by_none - solved_by_all} (some)"
+    )
 
 
 if __name__ == "__main__":

commit d9e2471fcd63d4efff3e140656cd4fd3bda4afa7
Author: Paul Gauthier <paul@aider.chat>
Date:   Wed Dec 18 12:43:03 2024 -0800

    refactor: Remove unused dump calls in benchmark script

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 992b4cc2..9381e6b3 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -32,7 +32,6 @@ def load_results(dirname):
             # Add language info to results
             lang = fname.parts[-5]  # Get language from path
             results["language"] = lang
-            dump(results)
             all_results.append(results)
         except json.JSONDecodeError:
             print(f"Failed to parse {fname}")
@@ -51,7 +50,6 @@ def analyze_exercise_solutions(dirs=None, topn=None):
     # Filter out entries that don't load and sort by pass rate
     valid_entries = []
     for dirname, model in dir_entries:
-        dump(dirname, model)
         results = load_results(dirname)
         if results:
             # Calculate pass rate for sorting when using custom dirs

commit 26ccb23402f501d37dc1e95decfc1342416225eb
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 12:43:04 2024 -0800

    feat: Add table showing exercise solution distribution

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 9381e6b3..8a43309a 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -170,6 +170,17 @@ def analyze_exercise_solutions(dirs=None, topn=None):
         f" {len(all_exercises) - solved_by_none - solved_by_all} (some)"
     )
 
+    # Distribution table of how many models solved each exercise
+    print("\nDistribution of solutions:")
+    print("Models  Exercises")
+    print("-" * 20)
+    counts = [0] * (total_models + 1)
+    for ex, models in exercise_solutions.items():
+        counts[len(models)] += 1
+    
+    for i, count in enumerate(counts):
+        print(f"{i:>6d}  {count:>9d}")
+
 
 if __name__ == "__main__":
     parser = argparse.ArgumentParser()

commit 2c7d1897eb433acee8ab8681105a05862b802dcd
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 12:43:08 2024 -0800

    style: Fix linting issues in problem_stats.py

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 8a43309a..c4c261ec 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -177,7 +177,7 @@ def analyze_exercise_solutions(dirs=None, topn=None):
     counts = [0] * (total_models + 1)
     for ex, models in exercise_solutions.items():
         counts[len(models)] += 1
-    
+
     for i, count in enumerate(counts):
         print(f"{i:>6d}  {count:>9d}")
 

commit 366155b8283f969fbdb14bb62da8b103b0eae1bc
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 12:45:58 2024 -0800

    fix: Correct language lookup in problem_stats.py

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index c4c261ec..3f40df0b 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -126,16 +126,8 @@ def analyze_exercise_solutions(dirs=None, topn=None):
     total_models = len(valid_entries)
 
     for testcase in all_exercises:
-        # Find language for this testcase
-        lang = "unknown"
-        for r in next(iter(valid_entries))[1]:
-            try:
-                if r.get("testcase") == testcase:
-                    lang = r["language"]
-                    break
-            except KeyError:
-                continue
-
+        # Language is already in the testcase string
+        lang = testcase.split('/')[0]  # First part is the language
         models = exercise_solutions[testcase]
         num_solved = len(models)
         percent = (num_solved / total_models) * 100

commit 8c1b147705fb63c3c5e7f70cef2a6e2daf709569
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 12:46:02 2024 -0800

    style: Fix string formatting in problem stats

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 3f40df0b..9e87d5a5 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -127,7 +127,7 @@ def analyze_exercise_solutions(dirs=None, topn=None):
 
     for testcase in all_exercises:
         # Language is already in the testcase string
-        lang = testcase.split('/')[0]  # First part is the language
+        lang = testcase.split("/")[0]  # First part is the language
         models = exercise_solutions[testcase]
         num_solved = len(models)
         percent = (num_solved / total_models) * 100

commit 20f5f3da2477732d0e9b0a9db54c9ae0df0bd245
Author: Paul Gauthier <paul@aider.chat>
Date:   Wed Dec 18 12:47:13 2024 -0800

    chore: Remove unused import

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 9e87d5a5..317c52f7 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -7,7 +7,7 @@ from pathlib import Path
 
 import yaml
 
-from aider.dump import dump
+from aider.dump import dump # noqa
 
 
 def get_dirs_from_leaderboard():

commit 6badf5ea1d3500777545c5cc03f7ff29e7a94b61
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 12:47:14 2024 -0800

    feat: Add cumulative sum column to distribution table

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 317c52f7..ccfd5ad0 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -164,14 +164,16 @@ def analyze_exercise_solutions(dirs=None, topn=None):
 
     # Distribution table of how many models solved each exercise
     print("\nDistribution of solutions:")
-    print("Models  Exercises")
-    print("-" * 20)
+    print("Models  Exercises  Cumulative")
+    print("-" * 35)
     counts = [0] * (total_models + 1)
     for ex, models in exercise_solutions.items():
         counts[len(models)] += 1
 
+    cumsum = 0
     for i, count in enumerate(counts):
-        print(f"{i:>6d}  {count:>9d}")
+        cumsum += count
+        print(f"{i:>6d}  {count:>9d}  {cumsum:>10d}")
 
 
 if __name__ == "__main__":

commit e88064fdc9335d65d48f526a6c3597be9ca4b71e
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 12:47:17 2024 -0800

    style: Run linter

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index ccfd5ad0..06cbf97e 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -7,7 +7,7 @@ from pathlib import Path
 
 import yaml
 
-from aider.dump import dump # noqa
+from aider.dump import dump  # noqa
 
 
 def get_dirs_from_leaderboard():

commit 14af6f1fba7e61cad5230082fdddb0f97ff88450
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 12:48:40 2024 -0800

    fix: Remove duplicate language prefix in problem stats

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 06cbf97e..251bf6ff 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -132,6 +132,9 @@ def analyze_exercise_solutions(dirs=None, topn=None):
         num_solved = len(models)
         percent = (num_solved / total_models) * 100
         testcase = testcase.replace("exercises/", "")  # Remove the exercises/ prefix
+        # Remove duplicate language prefix (e.g. javascript/javascript/ -> javascript/)
+        if testcase.startswith(f"{lang}/{lang}/"):
+            testcase = testcase[len(lang)+1:]
         exercise_stats.append((lang, testcase, num_solved, percent))
 
     # Sort all exercises by solve rate

commit 5dddaac0063a78eea1951e3c0e88207ef9663cef
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 12:48:44 2024 -0800

    style: Fix linting issues in problem_stats.py

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 251bf6ff..c53bdf09 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -134,7 +134,7 @@ def analyze_exercise_solutions(dirs=None, topn=None):
         testcase = testcase.replace("exercises/", "")  # Remove the exercises/ prefix
         # Remove duplicate language prefix (e.g. javascript/javascript/ -> javascript/)
         if testcase.startswith(f"{lang}/{lang}/"):
-            testcase = testcase[len(lang)+1:]
+            testcase = testcase[len(lang) + 1 :]
         exercise_stats.append((lang, testcase, num_solved, percent))
 
     # Sort all exercises by solve rate

commit 9e9cfb4600a05e386483bbd84ab67b28d5277c9d
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 12:49:23 2024 -0800

    fix: Calculate max name length after cleaning paths

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index c53bdf09..61b21cb4 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -140,8 +140,8 @@ def analyze_exercise_solutions(dirs=None, topn=None):
     # Sort all exercises by solve rate
     exercise_stats.sort(key=lambda x: x[2], reverse=True)
 
-    # Calculate max lengths for alignment
-    max_name_len = max(len(f"{lang}/{ex}") for lang, ex, _, _ in exercise_stats)
+    # Calculate max lengths for alignment after cleaning up paths
+    max_name_len = max(len(f"{lang}/{testcase}") for lang, testcase, _, _ in exercise_stats)
 
     # Print all exercises sorted by solve rate
     print("\nAll Exercises (sorted by solve rate):")

commit e5a693ab94bae0af55149b602779213efd9c5150
Author: Paul Gauthier <paul@aider.chat>
Date:   Wed Dec 18 12:55:42 2024 -0800

    fix: Correctly format testcase/language in stats

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 61b21cb4..34ea021c 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -86,7 +86,7 @@ def analyze_exercise_solutions(dirs=None, topn=None):
         if results:
             for result in results:
                 try:
-                    all_exercises.add(result["language"] + "/" + result["testcase"])
+                    all_exercises.add(result["testcase"] +"/"+ result["language"])
                 except KeyError:
                     print(f"Warning: Missing testcase in {dirname}")
 
@@ -103,7 +103,7 @@ def analyze_exercise_solutions(dirs=None, topn=None):
             if not lang:
                 continue
 
-            testcase = f"{lang}/{testcase}"
+            testcase = f"{testcase}/{lang}"
             # Consider it solved if the last test attempt passed
             tests_outcomes = result.get("tests_outcomes", [])
             if tests_outcomes and tests_outcomes[-1]:
@@ -147,7 +147,7 @@ def analyze_exercise_solutions(dirs=None, topn=None):
     print("\nAll Exercises (sorted by solve rate):")
     for i, (lang, testcase, num_solved, percent) in enumerate(exercise_stats, 1):
         print(
-            f"{i:>3}. {lang}/{testcase:<{max_name_len}} : {num_solved:>3} solved ({percent:>5.1f}%)"
+            f"{i:>3}. {testcase:<{max_name_len}} : {num_solved:>3} solved ({percent:>5.1f}%)"
         )
 
     print("\nSummary:")

commit e35909ac7d28b2ff36c2206396f300e4f4887ae9
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 12:55:43 2024 -0800

    refactor: Subsort exercises by name when solve rates are equal

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 34ea021c..26db9263 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -137,8 +137,8 @@ def analyze_exercise_solutions(dirs=None, topn=None):
             testcase = testcase[len(lang) + 1 :]
         exercise_stats.append((lang, testcase, num_solved, percent))
 
-    # Sort all exercises by solve rate
-    exercise_stats.sort(key=lambda x: x[2], reverse=True)
+    # Sort all exercises by solve rate, then by exercise name
+    exercise_stats.sort(key=lambda x: (-x[2], x[1]))  # -x[2] for descending solve rate, x[1] for ascending exercise name
 
     # Calculate max lengths for alignment after cleaning up paths
     max_name_len = max(len(f"{lang}/{testcase}") for lang, testcase, _, _ in exercise_stats)

commit c36087cc0c36b7068cb40046d7e4f1564ae5d02c
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 12:55:47 2024 -0800

    style: Apply linter formatting

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 26db9263..b5df09eb 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -86,7 +86,7 @@ def analyze_exercise_solutions(dirs=None, topn=None):
         if results:
             for result in results:
                 try:
-                    all_exercises.add(result["testcase"] +"/"+ result["language"])
+                    all_exercises.add(result["testcase"] + "/" + result["language"])
                 except KeyError:
                     print(f"Warning: Missing testcase in {dirname}")
 
@@ -138,7 +138,9 @@ def analyze_exercise_solutions(dirs=None, topn=None):
         exercise_stats.append((lang, testcase, num_solved, percent))
 
     # Sort all exercises by solve rate, then by exercise name
-    exercise_stats.sort(key=lambda x: (-x[2], x[1]))  # -x[2] for descending solve rate, x[1] for ascending exercise name
+    exercise_stats.sort(
+        key=lambda x: (-x[2], x[1])
+    )  # -x[2] for descending solve rate, x[1] for ascending exercise name
 
     # Calculate max lengths for alignment after cleaning up paths
     max_name_len = max(len(f"{lang}/{testcase}") for lang, testcase, _, _ in exercise_stats)
@@ -146,9 +148,7 @@ def analyze_exercise_solutions(dirs=None, topn=None):
     # Print all exercises sorted by solve rate
     print("\nAll Exercises (sorted by solve rate):")
     for i, (lang, testcase, num_solved, percent) in enumerate(exercise_stats, 1):
-        print(
-            f"{i:>3}. {testcase:<{max_name_len}} : {num_solved:>3} solved ({percent:>5.1f}%)"
-        )
+        print(f"{i:>3}. {testcase:<{max_name_len}} : {num_solved:>3} solved ({percent:>5.1f}%)")
 
     print("\nSummary:")
     solved_at_least_once = len([ex for ex, models in exercise_solutions.items() if models])

commit f3be2fa66bcf17156c2b84224d474158e3944489
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 13:03:27 2024 -0800

    feat: Add hard set analysis to problem_stats.py

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index b5df09eb..094f5f96 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -178,6 +178,45 @@ def analyze_exercise_solutions(dirs=None, topn=None):
         cumsum += count
         print(f"{i:>6d}  {count:>9d}  {cumsum:>10d}")
 
+    # Collect the hard set (exercises solved by 4 or fewer models)
+    print("\nHard Set Analysis (exercises solved by ≤4 models):")
+    print("-" * 60)
+    hard_set = {ex for ex, models in exercise_solutions.items() if len(models) <= 4}
+    print(f"Total hard set exercises: {len(hard_set)}")
+
+    # For each model, compute performance on hard set
+    model_hard_stats = []
+    for (dirname, model), results, _ in valid_entries:
+        if not results:
+            continue
+        
+        solved_hard = 0
+        for result in results:
+            testcase = result.get("testcase")
+            if not testcase:
+                continue
+            lang = result.get("language")
+            if not lang:
+                continue
+            
+            testcase = f"{testcase}/{lang}"
+            if testcase in hard_set:
+                tests_outcomes = result.get("tests_outcomes", [])
+                if tests_outcomes and tests_outcomes[-1]:
+                    solved_hard += 1
+        
+        pct = (solved_hard / len(hard_set)) * 100
+        model_hard_stats.append((model, solved_hard, pct))
+
+    # Sort by number solved
+    model_hard_stats.sort(key=lambda x: x[1], reverse=True)
+    
+    print("\nModel performance on hard set:")
+    print(f"{'Model':<30} {'Solved':<8} {'Percent':>7}")
+    print("-" * 50)
+    for model, solved, pct in model_hard_stats:
+        print(f"{model:<30} {solved:>6d}   {pct:>6.1f}%")
+
 
 if __name__ == "__main__":
     parser = argparse.ArgumentParser()

commit 04916a6e97ddb50b4ec0465071e05834d5b133f4
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 13:03:31 2024 -0800

    style: Run linter on problem_stats.py

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 094f5f96..1ceb270b 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -189,7 +189,7 @@ def analyze_exercise_solutions(dirs=None, topn=None):
     for (dirname, model), results, _ in valid_entries:
         if not results:
             continue
-        
+
         solved_hard = 0
         for result in results:
             testcase = result.get("testcase")
@@ -198,19 +198,19 @@ def analyze_exercise_solutions(dirs=None, topn=None):
             lang = result.get("language")
             if not lang:
                 continue
-            
+
             testcase = f"{testcase}/{lang}"
             if testcase in hard_set:
                 tests_outcomes = result.get("tests_outcomes", [])
                 if tests_outcomes and tests_outcomes[-1]:
                     solved_hard += 1
-        
+
         pct = (solved_hard / len(hard_set)) * 100
         model_hard_stats.append((model, solved_hard, pct))
 
     # Sort by number solved
     model_hard_stats.sort(key=lambda x: x[1], reverse=True)
-    
+
     print("\nModel performance on hard set:")
     print(f"{'Model':<30} {'Solved':<8} {'Percent':>7}")
     print("-" * 50)

commit 051cabed69bf49761959b560cd8a22ab60739390
Author: Paul Gauthier <paul@aider.chat>
Date:   Wed Dec 18 13:06:02 2024 -0800

    style: Adjust model column width in problem stats

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 1ceb270b..75a76728 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -212,10 +212,10 @@ def analyze_exercise_solutions(dirs=None, topn=None):
     model_hard_stats.sort(key=lambda x: x[1], reverse=True)
 
     print("\nModel performance on hard set:")
-    print(f"{'Model':<30} {'Solved':<8} {'Percent':>7}")
+    print(f"{'Model':<55} {'Solved':<8} {'Percent':>7}")
     print("-" * 50)
     for model, solved, pct in model_hard_stats:
-        print(f"{model:<30} {solved:>6d}   {pct:>6.1f}%")
+        print(f"{model:<55} {solved:>6d}   {pct:>6.1f}%")
 
 
 if __name__ == "__main__":

commit e6bfc1c2fcca145aa29dcda404acdb9c99a88d22
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 13:06:03 2024 -0800

    refactor: Use constant for hard set threshold

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 75a76728..dbd3004e 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -3,6 +3,9 @@
 import argparse
 import json
 from collections import defaultdict
+from typing import List, Optional
+
+HARD_SET_NUM = 4  # Number of models that defines the hard set threshold
 from pathlib import Path
 
 import yaml
@@ -178,10 +181,10 @@ def analyze_exercise_solutions(dirs=None, topn=None):
         cumsum += count
         print(f"{i:>6d}  {count:>9d}  {cumsum:>10d}")
 
-    # Collect the hard set (exercises solved by 4 or fewer models)
-    print("\nHard Set Analysis (exercises solved by ≤4 models):")
+    # Collect the hard set (exercises solved by HARD_SET_NUM or fewer models)
+    print(f"\nHard Set Analysis (exercises solved by ≤{HARD_SET_NUM} models):")
     print("-" * 60)
-    hard_set = {ex for ex, models in exercise_solutions.items() if len(models) <= 4}
+    hard_set = {ex for ex, models in exercise_solutions.items() if len(models) <= HARD_SET_NUM}
     print(f"Total hard set exercises: {len(hard_set)}")
 
     # For each model, compute performance on hard set

commit d4b62608a98904c05b448196765bf0d288d306fa
Author: Paul Gauthier <paul@aider.chat>
Date:   Wed Dec 18 13:08:47 2024 -0800

    chore: Move HARD_SET_NUM definition

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index dbd3004e..ccb531bb 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -5,13 +5,14 @@ import json
 from collections import defaultdict
 from typing import List, Optional
 
-HARD_SET_NUM = 4  # Number of models that defines the hard set threshold
 from pathlib import Path
 
 import yaml
 
 from aider.dump import dump  # noqa
 
+HARD_SET_NUM = 4  # Number of models that defines the hard set threshold
+
 
 def get_dirs_from_leaderboard():
     # Load the leaderboard data

commit 3e4500f9fdebd01c120d759b92ac0a837960aa45
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 13:08:48 2024 -0800

    feat: Print hard set problem counts by language

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index ccb531bb..b8f3a3c1 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -188,6 +188,19 @@ def analyze_exercise_solutions(dirs=None, topn=None):
     hard_set = {ex for ex, models in exercise_solutions.items() if len(models) <= HARD_SET_NUM}
     print(f"Total hard set exercises: {len(hard_set)}")
 
+    # Count problems by language in hard set
+    lang_counts = defaultdict(int)
+    for exercise in hard_set:
+        lang = exercise.split('/')[1]  # Get language from path
+        lang_counts[lang] += 1
+
+    print("\nHard set problems by language:")
+    print(f"{'Language':<12} {'Count':>5}")
+    print("-" * 18)
+    for lang, count in sorted(lang_counts.items()):
+        print(f"{lang:<12} {count:>5}")
+    print()
+
     # For each model, compute performance on hard set
     model_hard_stats = []
     for (dirname, model), results, _ in valid_entries:

commit 01088e214c6bce19f2064776a8195964eec3f6d4
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 13:08:51 2024 -0800

    style: Run linter on problem_stats.py

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index b8f3a3c1..06c9caf0 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -3,9 +3,8 @@
 import argparse
 import json
 from collections import defaultdict
-from typing import List, Optional
-
 from pathlib import Path
+from typing import List, Optional
 
 import yaml
 
@@ -191,7 +190,7 @@ def analyze_exercise_solutions(dirs=None, topn=None):
     # Count problems by language in hard set
     lang_counts = defaultdict(int)
     for exercise in hard_set:
-        lang = exercise.split('/')[1]  # Get language from path
+        lang = exercise.split("/")[1]  # Get language from path
         lang_counts[lang] += 1
 
     print("\nHard set problems by language:")

commit 817cb0d36323758896fc6a9cb9dd5c23f70c93cf
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 13:09:13 2024 -0800

    fix: Remove unused List, Optional imports

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 06c9caf0..e147fb4b 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -4,7 +4,6 @@ import argparse
 import json
 from collections import defaultdict
 from pathlib import Path
-from typing import List, Optional
 
 import yaml
 

commit 34da3dd3d78bc92349e10413fe41efabaa2859df
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 13:09:42 2024 -0800

    feat: Show percent of unsolved problems per language

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index e147fb4b..d81f76bc 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -186,17 +186,26 @@ def analyze_exercise_solutions(dirs=None, topn=None):
     hard_set = {ex for ex, models in exercise_solutions.items() if len(models) <= HARD_SET_NUM}
     print(f"Total hard set exercises: {len(hard_set)}")
 
-    # Count problems by language in hard set
-    lang_counts = defaultdict(int)
+    # Count total problems and hard set problems by language
+    lang_totals = defaultdict(int)
+    lang_hard_counts = defaultdict(int)
+    
+    for exercise in all_exercises:
+        lang = exercise.split("/")[1]  # Get language from path
+        lang_totals[lang] += 1
+        
     for exercise in hard_set:
         lang = exercise.split("/")[1]  # Get language from path
-        lang_counts[lang] += 1
+        lang_hard_counts[lang] += 1
 
     print("\nHard set problems by language:")
-    print(f"{'Language':<12} {'Count':>5}")
-    print("-" * 18)
-    for lang, count in sorted(lang_counts.items()):
-        print(f"{lang:<12} {count:>5}")
+    print(f"{'Language':<12} {'Count':>5} {'Percent':>8}")
+    print("-" * 28)
+    for lang in sorted(lang_totals.keys()):
+        count = lang_hard_counts[lang]
+        total = lang_totals[lang]
+        pct = (count / total) * 100
+        print(f"{lang:<12} {count:>5} {pct:>7.1f}%")
     print()
 
     # For each model, compute performance on hard set

commit 78e643970d0877077d1dac635805b544aca0a943
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 13:09:46 2024 -0800

    style: Fix linting issues

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index d81f76bc..68c2535e 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -189,11 +189,11 @@ def analyze_exercise_solutions(dirs=None, topn=None):
     # Count total problems and hard set problems by language
     lang_totals = defaultdict(int)
     lang_hard_counts = defaultdict(int)
-    
+
     for exercise in all_exercises:
         lang = exercise.split("/")[1]  # Get language from path
         lang_totals[lang] += 1
-        
+
     for exercise in hard_set:
         lang = exercise.split("/")[1]  # Get language from path
         lang_hard_counts[lang] += 1

commit b71c9d539e7379afe7e51c453e676460413f7e20
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 13:11:20 2024 -0800

    feat: Calculate and display unsolved problem percentages by language

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 68c2535e..921cfd3f 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -186,23 +186,21 @@ def analyze_exercise_solutions(dirs=None, topn=None):
     hard_set = {ex for ex, models in exercise_solutions.items() if len(models) <= HARD_SET_NUM}
     print(f"Total hard set exercises: {len(hard_set)}")
 
-    # Count total problems and hard set problems by language
+    # Count total problems and unsolved problems by language
     lang_totals = defaultdict(int)
-    lang_hard_counts = defaultdict(int)
+    lang_unsolved = defaultdict(int)
 
     for exercise in all_exercises:
         lang = exercise.split("/")[1]  # Get language from path
         lang_totals[lang] += 1
+        if not exercise_solutions[exercise]:  # No models solved this exercise
+            lang_unsolved[lang] += 1
 
-    for exercise in hard_set:
-        lang = exercise.split("/")[1]  # Get language from path
-        lang_hard_counts[lang] += 1
-
-    print("\nHard set problems by language:")
+    print("\nUnsolved problems by language:")
     print(f"{'Language':<12} {'Count':>5} {'Percent':>8}")
     print("-" * 28)
     for lang in sorted(lang_totals.keys()):
-        count = lang_hard_counts[lang]
+        count = lang_unsolved[lang]
         total = lang_totals[lang]
         pct = (count / total) * 100
         print(f"{lang:<12} {count:>5} {pct:>7.1f}%")

commit 3069db0cfd5f228936c6800d9d6d9f0ccf33202d
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 13:13:51 2024 -0800

    feat: Add --copy-hard-set switch to copy hard set problems

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 921cfd3f..e7b84df7 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -2,6 +2,7 @@
 
 import argparse
 import json
+import shutil
 from collections import defaultdict
 from pathlib import Path
 
@@ -41,7 +42,7 @@ def load_results(dirname):
     return all_results
 
 
-def analyze_exercise_solutions(dirs=None, topn=None):
+def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
     if dirs is None:
         # Use leaderboard data if no directories specified
         dir_entries = get_dirs_from_leaderboard()
@@ -239,6 +240,34 @@ def analyze_exercise_solutions(dirs=None, topn=None):
     for model, solved, pct in model_hard_stats:
         print(f"{model:<55} {solved:>6d}   {pct:>6.1f}%")
 
+    if copy_hard_set:
+        # Create hard set directory
+        src_dir = Path("tmp.benchmarks/exercism")
+        dst_dir = Path("tmp.benchmarks/exercism-hard-set")
+        
+        if dst_dir.exists():
+            print(f"\nError: Destination directory {dst_dir} already exists")
+            return
+
+        print(f"\nCopying hard set problems to {dst_dir}...")
+        
+        # Get the base names of hard set problems
+        hard_set_bases = {exercise.split('/')[0] for exercise in hard_set}
+        
+        # Copy each hard set problem's directory
+        for lang_dir in src_dir.glob("*/exercises/practice"):
+            if not lang_dir.is_dir():
+                continue
+                
+            for problem_dir in lang_dir.glob("*"):
+                if problem_dir.name in hard_set_bases:
+                    rel_path = problem_dir.relative_to(src_dir)
+                    dst_path = dst_dir / rel_path
+                    dst_path.parent.mkdir(parents=True, exist_ok=True)
+                    shutil.copytree(problem_dir, dst_path)
+        
+        print("Done copying hard set problems")
+
 
 if __name__ == "__main__":
     parser = argparse.ArgumentParser()
@@ -246,6 +275,11 @@ if __name__ == "__main__":
     parser.add_argument(
         "dirs", nargs="*", help="Directories to analyze (optional, defaults to leaderboard entries)"
     )
+    parser.add_argument(
+        "--copy-hard-set",
+        action="store_true",
+        help="Copy hard set problems to tmp.benchmarks/exercism-hard-set",
+    )
     args = parser.parse_args()
 
-    analyze_exercise_solutions(args.dirs if args.dirs else None, args.topn)
+    analyze_exercise_solutions(args.dirs if args.dirs else None, args.topn, args.copy_hard_set)

commit 892fd5a6ef4a1b8534c5eefa768fa1cccae418f3
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 13:13:55 2024 -0800

    style: Run linter

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index e7b84df7..e5b5f439 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -244,28 +244,28 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
         # Create hard set directory
         src_dir = Path("tmp.benchmarks/exercism")
         dst_dir = Path("tmp.benchmarks/exercism-hard-set")
-        
+
         if dst_dir.exists():
             print(f"\nError: Destination directory {dst_dir} already exists")
             return
 
         print(f"\nCopying hard set problems to {dst_dir}...")
-        
+
         # Get the base names of hard set problems
-        hard_set_bases = {exercise.split('/')[0] for exercise in hard_set}
-        
+        hard_set_bases = {exercise.split("/")[0] for exercise in hard_set}
+
         # Copy each hard set problem's directory
         for lang_dir in src_dir.glob("*/exercises/practice"):
             if not lang_dir.is_dir():
                 continue
-                
+
             for problem_dir in lang_dir.glob("*"):
                 if problem_dir.name in hard_set_bases:
                     rel_path = problem_dir.relative_to(src_dir)
                     dst_path = dst_dir / rel_path
                     dst_path.parent.mkdir(parents=True, exist_ok=True)
                     shutil.copytree(problem_dir, dst_path)
-        
+
         print("Done copying hard set problems")
 
 

commit 82f21b67347b1cce10cf51a625efe2a19ca9e289
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 13:16:04 2024 -0800

    fix: Copy hard set problems by language and exercise

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index e5b5f439..11384b07 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -251,22 +251,25 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
 
         print(f"\nCopying hard set problems to {dst_dir}...")
 
-        # Get the base names of hard set problems
-        hard_set_bases = {exercise.split("/")[0] for exercise in hard_set}
+        # Create a set of (exercise, language) pairs from hard_set
+        hard_set_pairs = {tuple(exercise.split("/")) for exercise in hard_set}
 
         # Copy each hard set problem's directory
+        num_copied = 0
         for lang_dir in src_dir.glob("*/exercises/practice"):
             if not lang_dir.is_dir():
                 continue
-
+                
+            lang = lang_dir.parts[-3]  # Get language from path
             for problem_dir in lang_dir.glob("*"):
-                if problem_dir.name in hard_set_bases:
+                if (problem_dir.name, lang) in hard_set_pairs:
                     rel_path = problem_dir.relative_to(src_dir)
                     dst_path = dst_dir / rel_path
                     dst_path.parent.mkdir(parents=True, exist_ok=True)
                     shutil.copytree(problem_dir, dst_path)
+                    num_copied += 1
 
-        print("Done copying hard set problems")
+        print(f"Copied {num_copied} hard set problems")
 
 
 if __name__ == "__main__":

commit 8f84df44ab8e5b104d6155f6dd7221e90c3edc4d
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 13:16:08 2024 -0800

    style: Run linter

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 11384b07..d841eaed 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -259,7 +259,7 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
         for lang_dir in src_dir.glob("*/exercises/practice"):
             if not lang_dir.is_dir():
                 continue
-                
+
             lang = lang_dir.parts[-3]  # Get language from path
             for problem_dir in lang_dir.glob("*"):
                 if (problem_dir.name, lang) in hard_set_pairs:

commit 7410c6216cb2d1eb61173a9273132ab2f18b90e0
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 13:16:41 2024 -0800

    feat: Count and display copied exercises per language

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index d841eaed..aaee36a6 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -255,11 +255,11 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
         hard_set_pairs = {tuple(exercise.split("/")) for exercise in hard_set}
 
         # Copy each hard set problem's directory
-        num_copied = 0
+        copied_by_lang = defaultdict(int)
         for lang_dir in src_dir.glob("*/exercises/practice"):
             if not lang_dir.is_dir():
                 continue
-
+                
             lang = lang_dir.parts[-3]  # Get language from path
             for problem_dir in lang_dir.glob("*"):
                 if (problem_dir.name, lang) in hard_set_pairs:
@@ -267,9 +267,12 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
                     dst_path = dst_dir / rel_path
                     dst_path.parent.mkdir(parents=True, exist_ok=True)
                     shutil.copytree(problem_dir, dst_path)
-                    num_copied += 1
+                    copied_by_lang[lang] += 1
 
-        print(f"Copied {num_copied} hard set problems")
+        total_copied = sum(copied_by_lang.values())
+        print(f"\nCopied {total_copied} hard set problems:")
+        for lang in sorted(copied_by_lang):
+            print(f"  {lang}: {copied_by_lang[lang]}")
 
 
 if __name__ == "__main__":

commit e1571dda9b7c0b35686a1642930497d76c359f40
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 13:16:45 2024 -0800

    style: Run linter

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index aaee36a6..065d0b1e 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -259,7 +259,7 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
         for lang_dir in src_dir.glob("*/exercises/practice"):
             if not lang_dir.is_dir():
                 continue
-                
+
             lang = lang_dir.parts[-3]  # Get language from path
             for problem_dir in lang_dir.glob("*"):
                 if (problem_dir.name, lang) in hard_set_pairs:

commit 50bb2cb1e664df6bd715661050080bbea3859a34
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 13:17:38 2024 -0800

    feat: Add total column to unsolved problems table

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 065d0b1e..fd9be348 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -198,13 +198,13 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
             lang_unsolved[lang] += 1
 
     print("\nUnsolved problems by language:")
-    print(f"{'Language':<12} {'Count':>5} {'Percent':>8}")
-    print("-" * 28)
+    print(f"{'Language':<12} {'Count':>5} {'Total':>7} {'Percent':>8}")
+    print("-" * 35)
     for lang in sorted(lang_totals.keys()):
         count = lang_unsolved[lang]
         total = lang_totals[lang]
         pct = (count / total) * 100
-        print(f"{lang:<12} {count:>5} {pct:>7.1f}%")
+        print(f"{lang:<12} {count:>5} {total:>7} {pct:>7.1f}%")
     print()
 
     # For each model, compute performance on hard set

commit 6ddb8a7d88b384eece891d7e80d28bedf16a2b5b
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Wed Dec 18 13:18:31 2024 -0800

    feat: Add hard set problem counts by language

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index fd9be348..f6957862 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -187,24 +187,28 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
     hard_set = {ex for ex, models in exercise_solutions.items() if len(models) <= HARD_SET_NUM}
     print(f"Total hard set exercises: {len(hard_set)}")
 
-    # Count total problems and unsolved problems by language
+    # Count total problems, unsolved problems, and hard set problems by language
     lang_totals = defaultdict(int)
     lang_unsolved = defaultdict(int)
+    lang_hard_set = defaultdict(int)
 
     for exercise in all_exercises:
         lang = exercise.split("/")[1]  # Get language from path
         lang_totals[lang] += 1
         if not exercise_solutions[exercise]:  # No models solved this exercise
             lang_unsolved[lang] += 1
+        if exercise in hard_set:  # Exercise is in the hard set
+            lang_hard_set[lang] += 1
 
-    print("\nUnsolved problems by language:")
-    print(f"{'Language':<12} {'Count':>5} {'Total':>7} {'Percent':>8}")
-    print("-" * 35)
+    print("\nUnsolved and hard set problems by language:")
+    print(f"{'Language':<12} {'Unsolved':>8} {'Hard Set':>9} {'Total':>7} {'Percent':>8}")
+    print("-" * 47)
     for lang in sorted(lang_totals.keys()):
         count = lang_unsolved[lang]
+        hard = lang_hard_set[lang]
         total = lang_totals[lang]
         pct = (count / total) * 100
-        print(f"{lang:<12} {count:>5} {total:>7} {pct:>7.1f}%")
+        print(f"{lang:<12} {count:>8} {hard:>9} {total:>7} {pct:>7.1f}%")
     print()
 
     # For each model, compute performance on hard set

commit a915c60999f42181b1af455310e729d2454a6af7
Author: Paul Gauthier <paul@aider.chat>
Date:   Wed Dec 18 13:36:37 2024 -0800

    feat: Add pass_num to benchmark results, fix hard set percent

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index f6957862..ca4e48ed 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -201,13 +201,13 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
             lang_hard_set[lang] += 1
 
     print("\nUnsolved and hard set problems by language:")
-    print(f"{'Language':<12} {'Unsolved':>8} {'Hard Set':>9} {'Total':>7} {'Percent':>8}")
+    print(f"{'Language':<12} {'Unsolved':>8} {'Hard Set':>9} {'Total':>7} {'%hardUnsolved':>8}")
     print("-" * 47)
     for lang in sorted(lang_totals.keys()):
         count = lang_unsolved[lang]
         hard = lang_hard_set[lang]
         total = lang_totals[lang]
-        pct = (count / total) * 100
+        pct = (count / hard) * 100
         print(f"{lang:<12} {count:>8} {hard:>9} {total:>7} {pct:>7.1f}%")
     print()
 

commit 5a0d4eff71f03f0cd12249b9e2dc744158b76061
Author: Paul Gauthier <paul@aider.chat>
Date:   Thu Dec 19 14:39:17 2024 -0800

    fix: Correctly handle zero hard set problems

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index ca4e48ed..375163b4 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -10,7 +10,7 @@ import yaml
 
 from aider.dump import dump  # noqa
 
-HARD_SET_NUM = 4  # Number of models that defines the hard set threshold
+HARD_SET_NUM = 3  # Number of models that defines the hard set threshold
 
 
 def get_dirs_from_leaderboard():
@@ -207,7 +207,7 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
         count = lang_unsolved[lang]
         hard = lang_hard_set[lang]
         total = lang_totals[lang]
-        pct = (count / hard) * 100
+        pct = (count / hard) * 100 if hard else -1
         print(f"{lang:<12} {count:>8} {hard:>9} {total:>7} {pct:>7.1f}%")
     print()
 

commit 14a8759b82a287079ddd409c3232c950f17e4013
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Thu Dec 19 14:39:18 2024 -0800

    feat: Disqualify exercises with >=4 parse errors

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 375163b4..b8718a5a 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -28,6 +28,8 @@ def load_results(dirname):
         return None
 
     all_results = []
+    parse_errors = []  # Track which exercises had parse errors for this model
+    
     # Look in language subdirectories under exercises/practice
     for fname in benchmark_dir.glob("*/exercises/practice/*/.aider.results.json"):
         try:
@@ -36,13 +38,21 @@ def load_results(dirname):
             lang = fname.parts[-5]  # Get language from path
             results["language"] = lang
             all_results.append(results)
+            
         except json.JSONDecodeError:
+            # Track the parse error for this exercise/model combination
+            lang = fname.parts[-5]
+            exercise = f"{fname.parts[-2]}/{lang}"  # Use directory name as testcase
+            parse_errors.append(exercise)
             print(f"Failed to parse {fname}")
             continue
-    return all_results
+            
+    return all_results, parse_errors
 
 
 def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
+    PARSE_ERROR_M = 4  # Threshold for number of parse errors to DQ an exercise
+
     if dirs is None:
         # Use leaderboard data if no directories specified
         dir_entries = get_dirs_from_leaderboard()
@@ -52,9 +62,13 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
 
     # Filter out entries that don't load and sort by pass rate
     valid_entries = []
+    parse_errors_by_model = {}  # Track which exercises had parse errors for each model
+    
     for dirname, model in dir_entries:
-        results = load_results(dirname)
-        if results:
+        results_data = load_results(dirname)
+        if results_data:
+            results, model_parse_errors = results_data
+            parse_errors_by_model[model] = set(model_parse_errors)
             # Calculate pass rate for sorting when using custom dirs
             if dirs is not None:
                 pass_rate = sum(
@@ -181,10 +195,30 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
         cumsum += count
         print(f"{i:>6d}  {count:>9d}  {cumsum:>10d}")
 
+    # Count parse errors per exercise
+    parse_error_counts = defaultdict(int)
+    for model_errors in parse_errors_by_model.values():
+        for exercise in model_errors:
+            parse_error_counts[exercise] += 1
+
+    # Find exercises to disqualify based on parse error threshold
+    disqualified_exercises = {
+        exercise for exercise, count in parse_error_counts.items() 
+        if count >= PARSE_ERROR_M
+    }
+
+    if disqualified_exercises:
+        print(f"\nDisqualified {len(disqualified_exercises)} exercises with {PARSE_ERROR_M}+ parse errors:")
+        for ex in sorted(disqualified_exercises):
+            print(f"  {ex} ({parse_error_counts[ex]} parse errors)")
+
     # Collect the hard set (exercises solved by HARD_SET_NUM or fewer models)
     print(f"\nHard Set Analysis (exercises solved by ≤{HARD_SET_NUM} models):")
     print("-" * 60)
-    hard_set = {ex for ex, models in exercise_solutions.items() if len(models) <= HARD_SET_NUM}
+    hard_set = {
+        ex for ex, models in exercise_solutions.items() 
+        if len(models) <= HARD_SET_NUM and ex not in disqualified_exercises
+    }
     print(f"Total hard set exercises: {len(hard_set)}")
 
     # Count total problems, unsolved problems, and hard set problems by language

commit bb711fe255bb9d59c4533d40754872d18f9e24d5
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Thu Dec 19 14:39:23 2024 -0800

    style: Run linter

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index b8718a5a..8a11237b 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -29,7 +29,7 @@ def load_results(dirname):
 
     all_results = []
     parse_errors = []  # Track which exercises had parse errors for this model
-    
+
     # Look in language subdirectories under exercises/practice
     for fname in benchmark_dir.glob("*/exercises/practice/*/.aider.results.json"):
         try:
@@ -38,7 +38,7 @@ def load_results(dirname):
             lang = fname.parts[-5]  # Get language from path
             results["language"] = lang
             all_results.append(results)
-            
+
         except json.JSONDecodeError:
             # Track the parse error for this exercise/model combination
             lang = fname.parts[-5]
@@ -46,7 +46,7 @@ def load_results(dirname):
             parse_errors.append(exercise)
             print(f"Failed to parse {fname}")
             continue
-            
+
     return all_results, parse_errors
 
 
@@ -63,7 +63,7 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
     # Filter out entries that don't load and sort by pass rate
     valid_entries = []
     parse_errors_by_model = {}  # Track which exercises had parse errors for each model
-    
+
     for dirname, model in dir_entries:
         results_data = load_results(dirname)
         if results_data:
@@ -203,12 +203,14 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
 
     # Find exercises to disqualify based on parse error threshold
     disqualified_exercises = {
-        exercise for exercise, count in parse_error_counts.items() 
-        if count >= PARSE_ERROR_M
+        exercise for exercise, count in parse_error_counts.items() if count >= PARSE_ERROR_M
     }
 
     if disqualified_exercises:
-        print(f"\nDisqualified {len(disqualified_exercises)} exercises with {PARSE_ERROR_M}+ parse errors:")
+        print(
+            f"\nDisqualified {len(disqualified_exercises)} exercises with {PARSE_ERROR_M}+ parse"
+            " errors:"
+        )
         for ex in sorted(disqualified_exercises):
             print(f"  {ex} ({parse_error_counts[ex]} parse errors)")
 
@@ -216,7 +218,8 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
     print(f"\nHard Set Analysis (exercises solved by ≤{HARD_SET_NUM} models):")
     print("-" * 60)
     hard_set = {
-        ex for ex, models in exercise_solutions.items() 
+        ex
+        for ex, models in exercise_solutions.items()
         if len(models) <= HARD_SET_NUM and ex not in disqualified_exercises
     }
     print(f"Total hard set exercises: {len(hard_set)}")

commit 7eb7533d422c0c3977fb24f0a7046ffecd75a009
Author: Paul Gauthier <paul@aider.chat>
Date:   Thu Dec 19 15:49:12 2024 -0800

    fix: Handle missing testcase in results and bad json

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 8a11237b..3178c1fb 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -32,19 +32,25 @@ def load_results(dirname):
 
     # Look in language subdirectories under exercises/practice
     for fname in benchmark_dir.glob("*/exercises/practice/*/.aider.results.json"):
+        error = False
         try:
             results = json.loads(fname.read_text())
-            # Add language info to results
-            lang = fname.parts[-5]  # Get language from path
-            results["language"] = lang
-            all_results.append(results)
+            error = 'testcase' not in results
+            if not error:
+                # Add language info to results
+                lang = fname.parts[-5]  # Get language from path
+                results["language"] = lang
+                all_results.append(results)
 
         except json.JSONDecodeError:
+            error = True
+
+        if error:
             # Track the parse error for this exercise/model combination
             lang = fname.parts[-5]
             exercise = f"{fname.parts[-2]}/{lang}"  # Use directory name as testcase
             parse_errors.append(exercise)
-            print(f"Failed to parse {fname}")
+            print(f"Bad results file {fname}")
             continue
 
     return all_results, parse_errors
@@ -105,7 +111,7 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
                 try:
                     all_exercises.add(result["testcase"] + "/" + result["language"])
                 except KeyError:
-                    print(f"Warning: Missing testcase in {dirname}")
+                    print(f"Warning: Missing testcase in {dirname}", json.dumps(result, indent=4))
 
     for (dirname, model), results, _ in valid_entries:
         if not results:
@@ -224,6 +230,9 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
     }
     print(f"Total hard set exercises: {len(hard_set)}")
 
+    dump(disqualified_exercises)
+    dump(hard_set)
+
     # Count total problems, unsolved problems, and hard set problems by language
     lang_totals = defaultdict(int)
     lang_unsolved = defaultdict(int)

commit 2d32f77ed0828c24d4335431738e2911bffaf251
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Thu Dec 19 15:49:13 2024 -0800

    feat: Print list of exercises never solved by any model

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 3178c1fb..7358a382 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -182,7 +182,12 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
 
     print(f"Total exercises solved at least once: {solved_at_least_once}")
     print(f"Never solved by any model: {solved_by_none}")
-    print(f"Solved by all models: {solved_by_all}")
+    if solved_by_none > 0:
+        print("\nExercises never solved by any model:")
+        unsolved = [ex for ex, models in exercise_solutions.items() if not models]
+        for ex in sorted(unsolved):
+            print(f"  {ex}")
+    print(f"\nSolved by all models: {solved_by_all}")
     print(
         f"Total exercises: {len(all_exercises)} = {solved_by_none} (none) + {solved_by_all} (all) +"
         f" {len(all_exercises) - solved_by_none - solved_by_all} (some)"

commit dddf192e5acf7aa5069391c16fa87a08684359f0
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Thu Dec 19 15:49:16 2024 -0800

    fix: Check for testcase key in results

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 7358a382..004e4f24 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -35,7 +35,7 @@ def load_results(dirname):
         error = False
         try:
             results = json.loads(fname.read_text())
-            error = 'testcase' not in results
+            error = "testcase" not in results
             if not error:
                 # Add language info to results
                 lang = fname.parts[-5]  # Get language from path

commit 6185ddf76a336586f6ce1d3ca1012cfb5e7c8d6e
Author: Paul Gauthier <paul@aider.chat>
Date:   Thu Dec 19 15:50:10 2024 -0800

    feat: Print never solved exercises and remove dumps

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 004e4f24..1e992555 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -181,6 +181,7 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
     )
 
     print(f"Total exercises solved at least once: {solved_at_least_once}")
+    # print out these never solved use lang/exercises/practice/ex ai!
     print(f"Never solved by any model: {solved_by_none}")
     if solved_by_none > 0:
         print("\nExercises never solved by any model:")
@@ -235,9 +236,6 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
     }
     print(f"Total hard set exercises: {len(hard_set)}")
 
-    dump(disqualified_exercises)
-    dump(hard_set)
-
     # Count total problems, unsolved problems, and hard set problems by language
     lang_totals = defaultdict(int)
     lang_unsolved = defaultdict(int)

commit 250e2ab6aae7050c59cdea86b699d57b0a9b7370
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Thu Dec 19 15:50:11 2024 -0800

    feat: Print never solved exercises with full path

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 1e992555..c09c9674 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -181,13 +181,16 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
     )
 
     print(f"Total exercises solved at least once: {solved_at_least_once}")
-    # print out these never solved use lang/exercises/practice/ex ai!
     print(f"Never solved by any model: {solved_by_none}")
     if solved_by_none > 0:
         print("\nExercises never solved by any model:")
         unsolved = [ex for ex, models in exercise_solutions.items() if not models]
         for ex in sorted(unsolved):
-            print(f"  {ex}")
+            # Split into language and exercise parts
+            lang, exercise = ex.split('/')
+            # Reconstruct path in desired format
+            formatted_path = f"{lang}/exercises/practice/{exercise}"
+            print(f"  {formatted_path}")
     print(f"\nSolved by all models: {solved_by_all}")
     print(
         f"Total exercises: {len(all_exercises)} = {solved_by_none} (none) + {solved_by_all} (all) +"

commit def2d4bac968e88d298cdcc0f7b9f1b368a9ecd5
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Thu Dec 19 15:50:14 2024 -0800

    style: Fix string formatting in problem stats

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index c09c9674..31f4d3e7 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -187,7 +187,7 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
         unsolved = [ex for ex, models in exercise_solutions.items() if not models]
         for ex in sorted(unsolved):
             # Split into language and exercise parts
-            lang, exercise = ex.split('/')
+            lang, exercise = ex.split("/")
             # Reconstruct path in desired format
             formatted_path = f"{lang}/exercises/practice/{exercise}"
             print(f"  {formatted_path}")

commit 4efdc8b4f7a665ec08cb6463bb6dc9cfc42f7164
Author: Paul Gauthier <paul@aider.chat>
Date:   Sat Dec 21 11:09:52 2024 -0800

    refactor: Rename benchmark dir, improve rsync, fix problem stats

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 31f4d3e7..eaace404 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -23,9 +23,12 @@ def get_dirs_from_leaderboard():
 def load_results(dirname):
     """Load all result files from a benchmark directory"""
     dirname = Path(dirname)
-    benchmark_dir = Path("tmp.benchmarks") / dirname
+
+    benchmark_dir = dirname
     if not benchmark_dir.exists():
-        return None
+        benchmark_dir = Path("tmp.benchmarks") / dirname
+        if not benchmark_dir.exists():
+            return None
 
     all_results = []
     parse_errors = []  # Track which exercises had parse errors for this model
@@ -70,8 +73,11 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
     valid_entries = []
     parse_errors_by_model = {}  # Track which exercises had parse errors for each model
 
+    dump(dir_entries)
+
     for dirname, model in dir_entries:
         results_data = load_results(dirname)
+
         if results_data:
             results, model_parse_errors = results_data
             parse_errors_by_model[model] = set(model_parse_errors)
@@ -299,7 +305,7 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
     if copy_hard_set:
         # Create hard set directory
         src_dir = Path("tmp.benchmarks/exercism")
-        dst_dir = Path("tmp.benchmarks/exercism-hard-set")
+        dst_dir = Path("tmp.benchmarks/exercism-polyglot")
 
         if dst_dir.exists():
             print(f"\nError: Destination directory {dst_dir} already exists")
@@ -340,7 +346,7 @@ if __name__ == "__main__":
     parser.add_argument(
         "--copy-hard-set",
         action="store_true",
-        help="Copy hard set problems to tmp.benchmarks/exercism-hard-set",
+        help="Copy hard set problems to tmp.benchmarks/exercism-polygot",
     )
     args = parser.parse_args()
 

commit 8eaefb57d33f7c85c6853aba0bdd57c800e5735f
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Sat Dec 28 11:45:41 2024 -0400

    feat: Add RevCumulative column to problem stats

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index eaace404..2ee8a089 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -205,16 +205,18 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
 
     # Distribution table of how many models solved each exercise
     print("\nDistribution of solutions:")
-    print("Models  Exercises  Cumulative")
-    print("-" * 35)
+    print("Models  Exercises  Cumulative  RevCumulative")
+    print("-" * 50)
     counts = [0] * (total_models + 1)
     for ex, models in exercise_solutions.items():
         counts[len(models)] += 1
 
     cumsum = 0
+    revcumsum = sum(counts)  # Start with total number of exercises
     for i, count in enumerate(counts):
         cumsum += count
-        print(f"{i:>6d}  {count:>9d}  {cumsum:>10d}")
+        print(f"{i:>6d}  {count:>9d}  {cumsum:>10d}  {revcumsum:>12d}")
+        revcumsum -= count  # Decrement the reverse cumulative sum
 
     # Count parse errors per exercise
     parse_error_counts = defaultdict(int)

commit 976722c1295fc1561dd320b1b4e17f5d222a51e9
Author: Paul Gauthier (aider) <paul@aider.chat>
Date:   Thu Feb 27 08:56:54 2025 -0800

    refactor: Update problem_stats.py to use polyglot_leaderboard.yml

diff --git a/benchmark/problem_stats.py b/benchmark/problem_stats.py
index 2ee8a089..202942f1 100755
--- a/benchmark/problem_stats.py
+++ b/benchmark/problem_stats.py
@@ -15,7 +15,7 @@ HARD_SET_NUM = 3  # Number of models that defines the hard set threshold
 
 def get_dirs_from_leaderboard():
     # Load the leaderboard data
-    with open("aider/website/_data/edit_leaderboard.yml") as f:
+    with open("aider/website/_data/polyglot_leaderboard.yml") as f:
         leaderboard = yaml.safe_load(f)
     return [(entry["dirname"], entry["model"]) for entry in leaderboard]
 
@@ -92,7 +92,7 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
                     (
                         entry["pass_rate_2"]
                         for entry in yaml.safe_load(
-                            open("aider/website/_data/edit_leaderboard.yml")
+                            open("aider/website/_data/polyglot_leaderboard.yml")
                         )
                         if entry["dirname"] == dirname
                     ),

</code></pre>
        </section>
    </main>
    <footer>
        <p>LoCoDiff-bench - <a href="https://github.com/AbanteAI/LoCoDiff-bench">GitHub Repository</a></p>
    </footer>
    
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            hljs.highlightAll();
        });
    </script>
</body>
</html>
    