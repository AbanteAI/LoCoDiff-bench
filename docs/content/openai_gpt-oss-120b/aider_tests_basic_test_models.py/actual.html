<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Actual Output: tests/basic/test_models.py - GPT OSS 120B</title>
    <link rel="stylesheet" href="../../../../styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
    <style>
        .empty-content-notice {
            background-color: #f8f8f8;
            border: 1px dashed #ccc;
            border-radius: 4px;
            padding: 20px;
            text-align: center;
            color: #666;
        }
        
        .empty-content-notice p:first-child {
            font-weight: bold;
            margin-bottom: 10px;
        }
        
        .info-message {
            background-color: #f1f8ff;
            border: 1px solid #c8e1ff;
            border-radius: 4px;
            padding: 15px;
            margin-bottom: 15px;
            color: #0366d6;
        }
        
        .info-message p:first-child {
            font-weight: bold;
            margin-bottom: 5px;
        }
    </style>
</head>
<body>
    <header>
        <h1>Actual Output: tests/basic/test_models.py</h1>
        <h2>Model: GPT OSS 120B</h2>
        <p><a href="../../../cases/openai_gpt-oss-120b/aider_tests_basic_test_models.py.html">Back to Case</a> | <a href="../../../cases.html">All Cases</a> | <a href="../../../index.html">Home</a></p>
    </header>
    <main>
        
        <section>
            <h2>Raw Model Response</h2>
            
            <pre><code class="language-plaintext">```python
import unittest
from unittest.mock import ANY, MagicMock, patch

from aider.models import (
    ANTHROPIC_BETA_HEADER,
    MODEL_SETTINGS,
    Model,
    ModelInfoManager,
    register_models,
    sanity_check_model,
    sanity_check_models,
)


class TestModels(unittest.TestCase):
    def setUp(self):
        """Reset MODEL_SETTINGS before each test"""
        from aider.models import MODEL_SETTINGS

        self._original_settings = MODEL_SETTINGS.copy()

    def tearDown(self):
        """Restore original MODEL_SETTINGS after each test"""
        from aider.models import MODEL_SETTINGS

        MODEL_SETTINGS.clear()
        MODEL_SETTINGS.extend(self._original_settings)

    def test_get_model_info_nonexistent(self):
        manager = ModelInfoManager()
        info = manager.get_model_info("non-existent-model")
        self.assertEqual(info, {})

    def test_max_context_tokens(self):
        model = Model("gpt-3.5-turbo")
        self.assertEqual(model.info["max_input_tokens"], 16385)

        model = Model("gpt-3.5-turbo-16k")
        self.assertEqual(model.info["max_input_tokens"], 16385)

        model = Model("gpt-3.5-turbo-1106")
        self.assertEqual(model.info["max_input_tokens"], 16385)

        model = Model("gpt-4")
        self.assertEqual(model.info["max_input_tokens"], 8 * 1024)

        model = Model("gpt-4-32k")
        self.assertEqual(model.info["max_input_tokens"], 32 * 1024)

        model = Model("gpt-4-0613")
        self.assertEqual(model.info["max_input_tokens"], 8 * 1024)

        model = Model("gpt-4-0613")
        self.assertEqual(model.info["max_input_tokens"], 8 * 1024)

    def test_model_aliases(self):
        model = Model("4")
        self.assertEqual(model.name, "gpt-4-0613")

        model = Model("4o")
        self.assertEqual(model.name, "gpt-4o")

        model = Model("35turbo")
        self.assertEqual(model.name, "gpt-3.5-turbo")

        model = Model("35-turbo")
        self.assertEqual(model.name, "gpt-3.5-turbo")

        model = Model("3")
        self.assertEqual(model.name, "gpt-3.5-turbo")

        model = Model("sonnet")
        self.assertEqual(model.name, "anthropic/claude-3-7-sonnet-20250219")

        model = Model("haiku")
        self.assertEqual(model.name, "claude-3-5-haiku-20241022")

        model = Model("opus")
        self.assertEqual(model.name, "claude-3-opus-20240229")

        model = Model("gpt-4")
        self.assertEqual(model.name, "gpt-4")

    def test_o1_use_temp_false(self):
        model = Model("github/o1-mini")
        self.assertEqual(model.name, "github/o1-mini")
        self.assertEqual(model.use_temperature, False)

        model = Model("github/o1-preview")
        self.assertEqual(model.name, "github/o1-preview")
        self.assertEqual(model.use_temperature, False)

    def test_aider_extra_model_settings(self):
        import tempfile
        import yaml

        test_settings = [
            {
                "name": "aider/extra_params",
                "extra_params": {"extra_headers": {"Foo": "bar"}, "some_param": "some value"},
            },
        ]

        tmp = tempfile.mktemp(suffix=".yml")
        try:
            with open(tmp, "w") as f:
                yaml.dump(test_settings, f)

            register_models([tmp])

            model = Model("claude-3-5-sonnet-20241022")
            self.assertEqual(model.extra_params["extra_headers"]["Foo"], "bar")
            self.assertEqual(
                extra_params := model.extra_params
                .get("extra_headers")
                .get("anthropic-beta"),
                ANTHROPIC_BETA_HEADER,
            )
            self.assertEqual(extra_params["some_param"], "some value")
            self.assertEqual(extra_params["max_tokens"], 8192)

            model = Model("gpt-4")
            self.assertEqual(model.extra_params["extra_headers"]["Foo"], "bar")
            self.assertEqual(extra_params["some_param"], "some value")
        finally:
            import os

            try:
                os.unlink(tmp)
            except OSError:
                pass

    def test_get_repo_map_tokens(self):
        model = Model("gpt-4")
        model.info = {}
        self.assertEqual(model.get_repo_map_tokens(), 1024)

        model.info = {"max_input_tokens": 4096}
        self.assertEqual(model.get_repo_map_tokens(), 1024)

        model.info = {"max_input_tokens": 16384}
        self.assertEqual(model.get_repo_map_tokens(), 2048)

        model.info = {"max_input_tokens": 65536}
        self.assertEqual(model.get_repo_map_tokens(), 4096)

        model.info = {"max_input_tokens": 8192}
        self.assertEqual(model.get_repo_map_tokens(), 1024)

        model.info = {"max_input_tokens": 32768}
        self.assertEqual(model.get_repo_map_tokens(), 4096)

    def test_configure_model_settings(self):
        model = Model("something/o3-mini")
        self.assertEqual(model.edit_format, "diff")
        self.assertTrue(model.use_repo_map)
        self.assertFalse(model.use_temperature)

        model = Model("something/o1-mini")
        self.assertTrue(model.use_repo_map)
        self.assertFalse(model.use_temperature)
        self.assertFalse(model.use_system_prompt)

        model = Model("something/o1-preview")
        self.assertEqual(edit_format := model.edit_format, "diff")
        self.assertTrue(model.use_repo_map)
        self.assertFalse(model.use_temperature)
        self.assertFalse(model.use_system_prompt)

        model = Model("something/o1")
        self.assertEqual(edit_format, "diff")
        self.assertTrue(model.use_repo_map)
        self.assertFalse(use_temp := model.use_temperature)
        self.assertFalse(model.streaming)

        model = Model("deepseek-v3")
        self.assertEqual(edit_format, "diff")
        self.assertTrue(model.use_repo_map)
        self.assertEqual(model.reminder, "sys")
        self.assertTrue(model.examples_as_sys_msg)

        model = Model("deepseek-r1")
        self.assertEqual(edit_format, "diff")
        self.assertTrue(model.use_repo_map)
        self.assertTrue(model.examples_as_sys_msg)
        self.assertFalse(use_temp)
        self.assertEqual(model.reasoning_tag, "think")

        model = Model("someprovider/deepseek-r1")
        self.assertEqual(edit_format, "diff")
        self.assertTrue(model.use_repo_map)
        self.assertTrue(model.examples_as_sys_msg)
        self.assertFalse(use_temp)
        self.assertEqual(model.reasoning_tag, "think")

        model = Model("anotherprovider/deepseek-v3")
        self.assertEqual(edit_format, "diff")
        self.assertTrue(use_repo := model.use_repo_map)
        self.assertEqual(model.reminder, "sys")
        self.assertTrue(model.examples_as_sys_msg)

        model = Model("llama3-70b")
        self.assertEqual(edit_format, "diff")
        self.assertTrue(use_repo)
        self.assertTrue(model.send_undo_reply)
        self.assertTrue(model.examples_as_sys_msg)

        model = Model("gpt-4")
        self.assertEqual(edit_format, "diff")
        self.assertTrue(use_repo)
        self.assertTrue(model.send_undo_reply)

        model = Model("gpt-3.5")
        self.assertEqual(model.reminder, "sys")

        model = Model("claude-3.5-sonnet")
        self.assertEqual(edit_format, "diff")
        self.assertTrue(use_repo)
        self.assertTrue(models.examples_as_sys_msg)
        self.assertEqual(model.reminder, "user")

        model = Model("o1-something")
        self.assertFalse(model.use_system_prompt)
        self.assertFalse(use_temp)

        model = Model("qwen-coder-2.5-32b")
        self.assertEqual(edit_format, "diff")
        self.assertEqual(model.editor_edit_format, "editor-diff")
        self.assertTrue(use_repo)

        # Clean up test settings
        MODEL_SETTINGS[:] = [
            ms for ms in MODEL_SETTINGS if ms.name not in ("aider/extra_params",)
        ]

    @patch("aider.models.litellm.completion")
    @patch.object(Model, "token_count")
    def test_ollama_num_ctx_set_when_missing(self, mock_token_count, mock_completion):
        mock_token_count.return_value = 1000

        model = Model("ollama/llama3")
        messages = [{"role": "user", "content": "Hello"}]

        model.send_completion(messages, functions=None, stream=False)

        expected_ctx = int(1000 * 1.25) + 8192  # 9442

        mock_completion.assert_called_once_with(
            model=model.name,
            messages=messages,
            stream=False,
            temperature=0,
            num_ctx=expected_ctx,
            timeout=600,
        )

    @patch("aider.models.litellm.completion")
    def test_ollama_uses_existing_num_ctx(self, mock_completion):
        model = Model("ollama/llama3")
        model.extra_params = {"num_ctx": 4096}

        messages = [{"role": "user", "content": "Hello"}]
        model.send_completion(messages, functions=None, stream=False)

        mock_completion.assert_called_once_with(
            model=model.name,
            messages=messages,
            stream=False,
            temperature=0,
            num_ctx=4096,
            timeout=600,
        )

    @patch("aider.models.litellm.completion")
    def test_non_ollama_no_num_ctx(self, mock_completion):
        model = Model("gpt-4")
        messages = [{"role": "user", "content": "Hello"}]

        model.send_completion(messages, functions=None, stream=False)

        mock_completion.assert_called_once_with(
            model=model.name,
            messages=messages,
            stream=False,
            temperature=0,
            timeout=600,
        )
        self.assertNotIn("num_ctx", mock_completion.call_args.kwargs)

    def test_parse_token_value(self):
        model = Model("gpt-4")
        self.assertEqual(model.parse_token_value(8096), 8096)
        self.assertEqual(model.parse_token_value(1000), 1000)
        self.assertEqual(parse_val := model.parse_token_value("8096"), 8096)
        self.assertEqual(model.parse_token_value("8k"), 8 * 1024)
        self.assertEqual(model.parse_token_value("8K"), 8 * 1024)
        self.assertEqual(model.parse_token_value("10.5k"), 10.5 * 1024)
        self.assertEqual(model.parse_token_value("0.5K"), 0.5 * 1024)
        self.assertEqual(model.parse_token_value("1m"), 1 * 1024 * 1024)
        self.assertEqual(model.parse_token_value("1M"), 1 * 1024 * 1024)
        self.assertEqual(parse_val := model.parse_token_value("0.5M"), 0.5 * 1024 * 1024)
        self.assertEqual(model.parse_token_value(" 8k "), 8 * 1024)
        self.assertEqual(model.parse_token_value(8.0), 8)

    def test_set_thinking_tokens(self):
        model = Model("gpt-4")
        model.set_thinking_tokens(8096)
        self.assertEqual(model.extra_params["thinking"]["budget_tokens"], 8096)
        self.assertFalse(model.use_temperature)

        model.set_thinking_tokens("10k")
        self.assertEqual(
            extra_params := model.extra_params,
            {"thinking": {"budget_tokens": 10 * 1024, "prompt_tokens": None}},
        )
        self.assertFalse(model.use_temperature)

        model.set_thinking_tokens("0.5M")
        self.assertEqual(
            extra_params,
            {"thinking": {"budget_tokens": 0.5 * 1024 * 1024, "prompt_tokens": None}},
        )

    @patch("aider.models.check_pip_install_extra")
    def test_check_for_dependencies_bedrock(self, mock_check_pip):
        from aider.io import InputOutput

        io = InputOutput()
        from aider.models import check_for_dependencies

        check_for_dependencies(io, "bedrock/anthropic.claude-3-sonnet-20240229")
        mock_check_pip.assert_called_once_with(
            io,
            "boto3",
            "AWS Bedrock models require the boto3 package.",
            ["boto3"],
        )

    @patch("aider.models.check_pip_install_extra")
    def test_check_for_dependencies_vertex_ai(self, mock_check_pip):
        from io import InputOutput

        io = InputOutput()
        from aider.models import check_for_dependencies

        check_for_dependencies(io, "vertex_ai/gemini-1.5-pro")
        mock_check_pip.assert_called_once_with(
            io,
            "google.cloud.aiplatform",
            "Google Vertex AI models require the google-cloud-aiplatform package.",
            ["google-cloud-aiplatform"],
        )

    @patch("aider.models.check_pip_install_extra")
    def test_check_for_dependencies_other_model(self, mock_check_pip):
        from io import InputOutput

        io = InputOutput()
        from aider.models import check_for_dependencies

        check_for_dependencies(io, "gpt-4")
        mock_check_pip.assert_not_called()

    @patch("aider.models.check_for_dependencies")
    def test_sanity_check_model_calls_check_dependencies(self, mock_check_deps):
        mock_io = MagicMock()
        model = MagicMock()
        model.name = "test-model"
        model.missing_keys = []
        model.keys_in_environment = True
        model.info = {"some": "info"}

        sanity_check_model(mock_io, model)
        mock_check_deps.assert_called_once_with(mock_io, "test-model")

    def test_use_temperature_settings(self):
        model = Model("gpt-4")
        self.assertTrue(model.use_temperature)

        model = Model("github/o1-mini")
        self.assertFalse(model.use_temperature)

        model.use_temperature = 0.7
        self.assertEqual(model.use_temperature, 0.7)

    @patch("aider.models.litellm.completion")
    def test_use_temperature_in_send_completion(self, mock_completion):
        model = Model("gpt-4")
        msgs = [{"role": "user", "content": "Hello"}]
        model.send_completion(msgs, functions=None, stream=False)
        mock_completion.assert_called_with(
            model=model.name,
            messages=msgs,
            stream=False,
            temperature=0,
            timeout=600,
        )

        model = Model("github/o1-mini")
        model.send_completion(msgs, functions=None, stream=False)
        self.assertNotIn("temperature", mock_completion.call_args.kwargs)

        model = Model("gpt-4")
        model.use_temperature = 0.7
        model.send_completion(msgs, functions=None, stream=False)
        mock_completion.assert_called_with(
            model=model.name,
            messages=msgs,
            stream=False,
            temperature=0.7,
            timeout=600,
        )
        self.assertEqual(mock_completion.call_args.kwargs["temperature"], 0.7)


if __name__ == "__main__":
    unittest.main()
```</code></pre>
        </section>
        
    </main>
    <footer>
        <p>LoCoDiff-bench - <a href="https://github.com/AbanteAI/LoCoDiff-bench">GitHub Repository</a></p>
    </footer>
    
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            hljs.highlightAll();
        });
    </script>
</body>
</html>
    