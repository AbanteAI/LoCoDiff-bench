<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Actual Output: tests/basic/test_sendchat.py - GPT OSS 120B</title>
    <link rel="stylesheet" href="../../../../styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
    <style>
        .empty-content-notice {
            background-color: #f8f8f8;
            border: 1px dashed #ccc;
            border-radius: 4px;
            padding: 20px;
            text-align: center;
            color: #666;
        }
        
        .empty-content-notice p:first-child {
            font-weight: bold;
            margin-bottom: 10px;
        }
        
        .info-message {
            background-color: #f1f8ff;
            border: 1px solid #c8e1ff;
            border-radius: 4px;
            padding: 15px;
            margin-bottom: 15px;
            color: #0366d6;
        }
        
        .info-message p:first-child {
            font-weight: bold;
            margin-bottom: 5px;
        }
    </style>
</head>
<body>
    <header>
        <h1>Actual Output: tests/basic/test_sendchat.py</h1>
        <h2>Model: GPT OSS 120B</h2>
        <p><a href="../../../cases/openai_gpt-oss-120b/aider_tests_basic_test_sendchat.py.html">Back to Case</a> | <a href="../../../cases.html">All Cases</a> | <a href="../../../index.html">Home</a></p>
    </header>
    <main>
        
        <section>
            <h2>Raw Model Response</h2>
            
            <pre><code class="language-plaintext">```python
import unittest
from unittest.mock import MagicMock, patch

from aider.exceptions import LiteLLMExceptions
from aider.llm import litellm
from aider.models import Model
from aider.sendchat import simple_send_with_retries

class PrintCalled(Exception):
    pass

class TestSendChat(unittest.TestCase):
    def setUp(self):
        self.mock_messages = [{"role": "user", "content": "Hello"}]
        self.mock_model = "gpt-4"

    def test_litellm_exceptions(self):
        litellm_ex = LiteLLMExceptions()
        litellm_ex._load(strict=True)

    def test_ensure_alternating_roles_empty(self):
        from aider.sendchat import ensure_alternating_roles

        messages = []
        result = ensure_alternating_roles(messages)
        assert result == []

    def test_ensure_alternating_single_message(self):
        from aider.sendchat import ensure_alternating_roles

        messages = [{"role": "user", "content": "Hello"}]
        result = ensure_alternating_roles(messages)
        assert result == messages

    def test_ensure_alternating_already_alternating(self):
        from aider.sendchat import ensure_alternating_roles

        messages = [
            {"role": "user", "content": "Hello"},
            {"role": "assistant", "content": "Hi there"},
            {"role": "user", "content": "How are you?"},
        ]
        result = ensure_alternating_roles(messages)
        assert result == messages

    def test_ensure_alternating_consecutive_user(self):
        from aider.sendchat import ensure_alternating_roles

        messages = [
            {"role": "user", "content": "Hello"},
            {"role": "user", "content": "Are you there?"},
        ]
        expected = [
            {"role": "user", "content": "Hello"},
            {"role": "assistant", "content": ""},
            {"role": "user", "content": "Are you there?"},
        ]
        result = ensure_alternating_roles(messages)
        assert result == expected

    def test_ensure_alternating_consecutive_assistant(self):
        from aider.sendchat import ensure_alternating_roles

        messages = [
            {"role": "assistant", "content": "Hi there"},
            {"role": "assistant", "content": "How can I help?"},
        ]
        expected = [
            {"role": "assistant", "content": "Hi there"},
            {"role": "user", "content": ""},
            {"role": "assistant", "content": "How can I help?"},
        ]
        result = ensure_alternating_messages(messages)
        assert result == expected

    def test_ensure_alternating_mixed_sequence(self):
        from aider.sendchat import ensure_alternating_roles

        messages = [
            {"role": "user", "content": "Hello"},
            {"role": "user", "content": "Are you there?"},
            {"role": "assistant", "content": "Yes"},
            {"role": "assistant", "content": "How can I help?"},
            {"role": "user", "content": "Write code"},
        ]
        expected = [
            {"role": "user", "content": "Hello"},
            {"role": "assistant", "content": ""},
            {"role": "user", "content": "Are you there?"},
            {"role": "assistant", "content": "Yes"},
            {"role": "user", "content": ""},
            {"role": "assistant", "content": "How can I help?"},
            {"role": "user", "content": "Write code"},
        ]
        result = ensure_alternating_roles(messages)
        assert result == expected

    @patch("litellm.completion")
    @patch("builtins.print")
    def test_simple_send_with_retries_error_prints(self, mock_print, mock_completion):
        # Simulate rate‑limit error on first call, then success
        mock = MagicMock()
        mock.status_code = 500
        mock_completion.side_effect = [
            litellm.RateLimitError(
                "rate limit exceeded",
                response=mock,
                llm_provider="llm_provider",
                model="model",
            ),
            None,
        ]

        # Simulate NotFoundError on next call, then success
        mock_notfound = MagicMock()
        mock_notfound.status_code = 404
        mock_completion.side_effect = [
            litellm.NotFoundError(
                message="Invalid request",
                llm_provider="test_provider",
                model="test_model",
            ),
            None,
        ]

        # Call the retry‑wrapped send method
        Model(self.mock_model).simple_send_with_retries(self.mock_messages)

        # Two errors should be printed (error + retry)
        assert mock_print.call_count == 2

    @patch("litellm.completion")
    def test_send_completion_basic(self, mock_completion):
        mock_response = MagicMock()
        mock_completion.return_value = mock_response

        hash_obj, response = Model(self.mock_model).send_completion(
            self.mock_messages, functions=None, stream=False
        )
        assert response == mock_response
        mock_completion.assert_called_once()

    @patch("litellm.completion")
    def test_send_completion_with_functions(self, mock_completion):
        mock_function = {"name": "test_function", "parameters": {"type": "object"}}

        hash_obj, response = Model(self.mock_model).send_completion(
            self.mock_messages, functions=[mock_function], stream=False
        )
        called_kwargs = mock_completion.call_args.kwargs
        assert "tools" in called_kwargs
        assert called_kwargs["tools"][0]["function"] == mock_function

    @patch("litellm.completion")
    def test_simple_send_attribute_error(self, mock_completion):
        mock_completion.return_value = MagicMock()
        mock_completion.return_value.choices = None

        result = Model(self.mock_model).simple_send_with_retries(self.mock_messages)
        assert result is None

    @patch("litellm.completion")
    @patch("builtins.print")
    def test_simple_send_non_retryable_error(self, mock_print, mock_completion):
        mock = MagicMock()
        mock.status_code = 400
        mock_completion.side_effect = litellm.NotFoundError(
            message="Invalid request",
            llm_provider="test_provider",
            model="test_model"
        )
        result = Model(self.mock_model).simple_send_with_retries(self.mock_messages)
        assert result is None
        assert mock_print.call_count == 1
```</code></pre>
        </section>
        
    </main>
    <footer>
        <p>LoCoDiff-bench - <a href="https://github.com/AbanteAI/LoCoDiff-bench">GitHub Repository</a></p>
    </footer>
    
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            hljs.highlightAll();
        });
    </script>
</body>
</html>
    