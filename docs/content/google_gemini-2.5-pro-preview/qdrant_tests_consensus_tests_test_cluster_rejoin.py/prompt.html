<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Prompt: tests/consensus_tests/test_cluster_rejoin.py - Gemini 2.5 Pro 05-06</title>
    <link rel="stylesheet" href="../../../../styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
</head>
<body>
    <header>
        <h1>Prompt: tests/consensus_tests/test_cluster_rejoin.py</h1>
        <h2>Model: Gemini 2.5 Pro 05-06</h2>
        <p><a href="../../../cases/google_gemini-2.5-pro-preview/qdrant_tests_consensus_tests_test_cluster_rejoin.py.html">Back to Case</a> | <a href="../../../cases.html">All Cases</a> | <a href="../../../index.html">Home</a></p>
    </header>
    <main>
        <section>
            <h2>Prompt Content</h2>
            <pre><code class="language-plaintext"># Instructions

You are being benchmarked. You will see the output of a git log command, and from that must infer the current state of a file. Think carefully, as you must output the exact state of the file to earn full marks.

**Important:** Your goal is to reproduce the file's content *exactly* as it exists at the final commit, even if the code appears broken, buggy, or contains obvious errors. Do **not** try to "fix" the code. Attempting to correct issues will result in a poor score, as this benchmark evaluates your ability to reproduce the precise state of the file based on its history.

# Required Response Format

Wrap the content of the file in triple backticks (```). Any text outside the final closing backticks will be ignored. End your response after outputting the closing backticks.

# Example Response

```python
#!/usr/bin/env python
print('Hello, world!')
```

# File History

> git log -p --cc --topo-order --reverse -- tests/consensus_tests/test_cluster_rejoin.py

commit 16d3bba2339874768bf1e537f2b6d70892feb7e1
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Thu Nov 24 15:36:03 2022 +0100

    Fix consensus freezes (#1241)
    
    * do not await for replica deactivation if there is a logical error
    
    * use async mutex in raft service + integration test for deadlock
    
    * fmt

diff --git a/tests/consensus_tests/test_cluster_rejoin.py b/tests/consensus_tests/test_cluster_rejoin.py
new file mode 100644
index 000000000..f89792e35
--- /dev/null
+++ b/tests/consensus_tests/test_cluster_rejoin.py
@@ -0,0 +1,60 @@
+import pathlib
+
+from consensus_tests.fixtures import create_collection, upsert_random_points, drop_collection
+import requests
+from .utils import *
+
+N_PEERS = 3
+N_REPLICA = 2
+N_SHARDS = 3
+
+
+def test_rejoin_cluster(tmp_path: pathlib.Path):
+    assert_project_root()
+
+    peer_api_uris, peer_dirs, bootstrap_uri = start_cluster(tmp_path, N_PEERS)
+
+    create_collection(peer_api_uris[0], shard_number=N_SHARDS, replication_factor=N_REPLICA)
+    wait_collection_exists_and_active_on_all_peers(collection_name="test_collection", peer_api_uris=peer_api_uris)
+    upsert_random_points(peer_api_uris[0], 100)
+
+    p = processes.pop()
+    p.kill()
+
+    # Validate upsert works with the dead node
+    upsert_random_points(peer_api_uris[0], 100)
+
+    # Assert that there are dead replicas
+    wait_for_some_replicas_not_active(peer_api_uris[0], "test_collection")
+
+    for i in range(0, 2):
+        print(f"creating collection {i}")
+        drop_collection(peer_api_uris[0], timeout=1)
+        create_collection(peer_api_uris[0], shard_number=N_SHARDS, replication_factor=N_REPLICA, timeout=1)
+        # Collection might not be ready yet, we don't care
+        upsert_random_points(peer_api_uris[0], 100)
+        print(f"after recovery end {i}")
+        res = requests.get(f"{peer_api_uris[1]}/collections")
+        print(res.json())
+
+    create_collection(
+        peer_api_uris[0],
+        "test_collection2",
+        shard_number=N_SHARDS,
+        replication_factor=N_REPLICA,
+        timeout=1
+    )
+
+    new_url = start_peer(peer_dirs[-1], f"peer_0_restarted.log", bootstrap_uri)
+
+    for i in range(0, 5):
+        print(f"after recovery start {i}")
+        drop_collection(peer_api_uris[0], timeout=1)
+        create_collection(peer_api_uris[0], shard_number=N_SHARDS, replication_factor=N_REPLICA, timeout=1)
+        upsert_random_points(peer_api_uris[0], 500, fail_on_error=False)
+        print(f"after recovery end {i}")
+        res = requests.get(f"{new_url}/collections")
+        print(res.json())
+
+    wait_for_all_replicas_active(peer_api_uris[0], "test_collection2")
+    wait_for_all_replicas_active(new_url, "test_collection2")

commit ac57478ba4f0d45ff6bd71859024fe09cd9bdd2f
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Fri Dec 23 00:45:04 2022 +0100

    Fixing missed leader inconsistency (#1286)
    
    * WIP: introduce local state
    
    * WIP: sync local state with consensus in idle
    
    * fmt
    
    * rm extensive debug
    
    * update triple replication test
    
    * update triple replication test
    
    * rm debug logs
    
    * fix check for established leader + only sync local if no proposals
    
    * remove unused file
    
    * test compatible with python 3.8
    
    * test compatible with python 3.8
    
    * longer wait for consensus
    
    * longer wait for consensus
    
    * extra sleep in test
    
    * remove extra sleep
    
    * Fixing missed leader inconsistency - transfers (#1298)
    
    * explicit request timeout
    
    * explicit request timeout
    
    * explicit request timeout
    
    * explicit request timeout
    
    * explicit request timeout
    
    * prevent double handelling of the transfer termination
    
    * kill the process
    
    * log on inconsistency
    
    * log on inconsistency
    
    * log on inconsistency
    
    * debug
    
    * revert debug in test
    
    * forward updates to partial shards, abort transactions on dead node report
    
    * fmt
    
    * disable retry of transfer, if the transfer was cancelled + allow predictable ports in test
    
    * fix import

diff --git a/tests/consensus_tests/test_cluster_rejoin.py b/tests/consensus_tests/test_cluster_rejoin.py
index f89792e35..3d7ba5ca9 100644
--- a/tests/consensus_tests/test_cluster_rejoin.py
+++ b/tests/consensus_tests/test_cluster_rejoin.py
@@ -12,7 +12,7 @@ N_SHARDS = 3
 def test_rejoin_cluster(tmp_path: pathlib.Path):
     assert_project_root()
 
-    peer_api_uris, peer_dirs, bootstrap_uri = start_cluster(tmp_path, N_PEERS)
+    peer_api_uris, peer_dirs, bootstrap_uri = start_cluster(tmp_path, N_PEERS, port_seed=10000)
 
     create_collection(peer_api_uris[0], shard_number=N_SHARDS, replication_factor=N_REPLICA)
     wait_collection_exists_and_active_on_all_peers(collection_name="test_collection", peer_api_uris=peer_api_uris)
@@ -45,7 +45,7 @@ def test_rejoin_cluster(tmp_path: pathlib.Path):
         timeout=1
     )
 
-    new_url = start_peer(peer_dirs[-1], f"peer_0_restarted.log", bootstrap_uri)
+    new_url = start_peer(peer_dirs[-1], f"peer_0_restarted.log", bootstrap_uri, port=20000)
 
     for i in range(0, 5):
         print(f"after recovery start {i}")

commit bcb52f9aee210d02a10eb250ab3e602d29e17313
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Sun Dec 25 22:36:31 2022 +0100

    Id mapper inconsistency (#1302)
    
    * always flush wal
    
    * always flush wal fix
    
    * always flush wal fmt
    
    * flush wal during background flush
    
    * async wal flush
    
    * use id-tracker internal id for next-id instead of vector storage
    
    * add flush order and recovery comment
    
    fix merge bug
    
    * longer timeout in test

diff --git a/tests/consensus_tests/test_cluster_rejoin.py b/tests/consensus_tests/test_cluster_rejoin.py
index 3d7ba5ca9..ce68a5669 100644
--- a/tests/consensus_tests/test_cluster_rejoin.py
+++ b/tests/consensus_tests/test_cluster_rejoin.py
@@ -29,8 +29,8 @@ def test_rejoin_cluster(tmp_path: pathlib.Path):
 
     for i in range(0, 2):
         print(f"creating collection {i}")
-        drop_collection(peer_api_uris[0], timeout=1)
-        create_collection(peer_api_uris[0], shard_number=N_SHARDS, replication_factor=N_REPLICA, timeout=1)
+        drop_collection(peer_api_uris[0], timeout=3)
+        create_collection(peer_api_uris[0], shard_number=N_SHARDS, replication_factor=N_REPLICA, timeout=3)
         # Collection might not be ready yet, we don't care
         upsert_random_points(peer_api_uris[0], 100)
         print(f"after recovery end {i}")
@@ -42,15 +42,15 @@ def test_rejoin_cluster(tmp_path: pathlib.Path):
         "test_collection2",
         shard_number=N_SHARDS,
         replication_factor=N_REPLICA,
-        timeout=1
+        timeout=3
     )
 
     new_url = start_peer(peer_dirs[-1], f"peer_0_restarted.log", bootstrap_uri, port=20000)
 
     for i in range(0, 5):
         print(f"after recovery start {i}")
-        drop_collection(peer_api_uris[0], timeout=1)
-        create_collection(peer_api_uris[0], shard_number=N_SHARDS, replication_factor=N_REPLICA, timeout=1)
+        drop_collection(peer_api_uris[0], timeout=3)
+        create_collection(peer_api_uris[0], shard_number=N_SHARDS, replication_factor=N_REPLICA, timeout=3)
         upsert_random_points(peer_api_uris[0], 500, fail_on_error=False)
         print(f"after recovery end {i}")
         res = requests.get(f"{new_url}/collections")

commit 2f0b8ec20e6b4d2fdb1ae50789386e866ca467ab
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Thu Jan 26 00:49:18 2023 +0100

    Telemetry collection (#1401)
    
    * sending telemetry requests
    
    * report panics
    
    * fmt
    
    * remove extra runtime
    
    * fix telemetry lock release

diff --git a/tests/consensus_tests/test_cluster_rejoin.py b/tests/consensus_tests/test_cluster_rejoin.py
index ce68a5669..9234223f1 100644
--- a/tests/consensus_tests/test_cluster_rejoin.py
+++ b/tests/consensus_tests/test_cluster_rejoin.py
@@ -14,6 +14,8 @@ def test_rejoin_cluster(tmp_path: pathlib.Path):
 
     peer_api_uris, peer_dirs, bootstrap_uri = start_cluster(tmp_path, N_PEERS, port_seed=10000)
 
+    print("cluster started")
+
     create_collection(peer_api_uris[0], shard_number=N_SHARDS, replication_factor=N_REPLICA)
     wait_collection_exists_and_active_on_all_peers(collection_name="test_collection", peer_api_uris=peer_api_uris)
     upsert_random_points(peer_api_uris[0], 100)
@@ -47,6 +49,10 @@ def test_rejoin_cluster(tmp_path: pathlib.Path):
 
     new_url = start_peer(peer_dirs[-1], f"peer_0_restarted.log", bootstrap_uri, port=20000)
 
+    peer_api_uris[-1] = new_url
+
+    wait_all_peers_up([new_url])
+
     for i in range(0, 5):
         print(f"after recovery start {i}")
         drop_collection(peer_api_uris[0], timeout=3)

commit b8bbedd7aab8e12bb057c5464de22d703bbf8e56
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Mon Mar 27 10:28:56 2023 +0200

    Increase timeout for rejoin cluster test (#1600)

diff --git a/tests/consensus_tests/test_cluster_rejoin.py b/tests/consensus_tests/test_cluster_rejoin.py
index 9234223f1..65a8453c5 100644
--- a/tests/consensus_tests/test_cluster_rejoin.py
+++ b/tests/consensus_tests/test_cluster_rejoin.py
@@ -11,15 +11,14 @@ N_SHARDS = 3
 
 def test_rejoin_cluster(tmp_path: pathlib.Path):
     assert_project_root()
-
+    # Start cluster
     peer_api_uris, peer_dirs, bootstrap_uri = start_cluster(tmp_path, N_PEERS, port_seed=10000)
 
-    print("cluster started")
-
     create_collection(peer_api_uris[0], shard_number=N_SHARDS, replication_factor=N_REPLICA)
     wait_collection_exists_and_active_on_all_peers(collection_name="test_collection", peer_api_uris=peer_api_uris)
     upsert_random_points(peer_api_uris[0], 100)
 
+    # Stop last node
     p = processes.pop()
     p.kill()
 
@@ -29,16 +28,20 @@ def test_rejoin_cluster(tmp_path: pathlib.Path):
     # Assert that there are dead replicas
     wait_for_some_replicas_not_active(peer_api_uris[0], "test_collection")
 
+    # Repeatedly drop, re-create collection and add data to it to accumulate Raft log entries
     for i in range(0, 2):
         print(f"creating collection {i}")
-        drop_collection(peer_api_uris[0], timeout=3)
+        # Drop test_collection
+        drop_collection(peer_api_uris[0], "test_collection", timeout=5)
+        # Re-create test_collection
         create_collection(peer_api_uris[0], shard_number=N_SHARDS, replication_factor=N_REPLICA, timeout=3)
         # Collection might not be ready yet, we don't care
         upsert_random_points(peer_api_uris[0], 100)
-        print(f"after recovery end {i}")
+        print(f"before recovery end {i}")
         res = requests.get(f"{peer_api_uris[1]}/collections")
         print(res.json())
 
+    # Create new collection unknown to the dead node
     create_collection(
         peer_api_uris[0],
         "test_collection2",
@@ -47,15 +50,20 @@ def test_rejoin_cluster(tmp_path: pathlib.Path):
         timeout=3
     )
 
+    # Restart last node
     new_url = start_peer(peer_dirs[-1], f"peer_0_restarted.log", bootstrap_uri, port=20000)
 
     peer_api_uris[-1] = new_url
 
+    # Wait for restarted node to be up and ready
     wait_all_peers_up([new_url])
 
+    # Repeatedly drop, re-create collection and add data to it to accumulate Raft log entries
     for i in range(0, 5):
         print(f"after recovery start {i}")
-        drop_collection(peer_api_uris[0], timeout=3)
+        # Drop test_collection
+        drop_collection(peer_api_uris[0], "test_collection", timeout=5)
+        # Re-create test_collection
         create_collection(peer_api_uris[0], shard_number=N_SHARDS, replication_factor=N_REPLICA, timeout=3)
         upsert_random_points(peer_api_uris[0], 500, fail_on_error=False)
         print(f"after recovery end {i}")
@@ -63,4 +71,5 @@ def test_rejoin_cluster(tmp_path: pathlib.Path):
         print(res.json())
 
     wait_for_all_replicas_active(peer_api_uris[0], "test_collection2")
+    # Assert that the restarted node has recovered the new collection
     wait_for_all_replicas_active(new_url, "test_collection2")

commit 041e990bd1504d1c4468a979484490956c4795d4
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Thu Feb 1 09:37:29 2024 +0100

    Lint Python tests (#3504)

diff --git a/tests/consensus_tests/test_cluster_rejoin.py b/tests/consensus_tests/test_cluster_rejoin.py
index 65a8453c5..ca5ca79e1 100644
--- a/tests/consensus_tests/test_cluster_rejoin.py
+++ b/tests/consensus_tests/test_cluster_rejoin.py
@@ -51,7 +51,7 @@ def test_rejoin_cluster(tmp_path: pathlib.Path):
     )
 
     # Restart last node
-    new_url = start_peer(peer_dirs[-1], f"peer_0_restarted.log", bootstrap_uri, port=20000)
+    new_url = start_peer(peer_dirs[-1], "peer_0_restarted.log", bootstrap_uri, port=20000)
 
     peer_api_uris[-1] = new_url
 

commit 83c1dc987e7958718e4a335e7e3c5ea6c8f02d75
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Thu Oct 10 19:30:21 2024 +0200

    When initializing new cluster, propose origin peer to consensus (#5141)
    
    Co-authored-by: Tim Visée <tim+github@visee.me>

diff --git a/tests/consensus_tests/test_cluster_rejoin.py b/tests/consensus_tests/test_cluster_rejoin.py
index ca5ca79e1..6cdf88653 100644
--- a/tests/consensus_tests/test_cluster_rejoin.py
+++ b/tests/consensus_tests/test_cluster_rejoin.py
@@ -1,4 +1,6 @@
+import io
 import pathlib
+from typing import Any
 
 from consensus_tests.fixtures import create_collection, upsert_random_points, drop_collection
 import requests
@@ -73,3 +75,138 @@ def test_rejoin_cluster(tmp_path: pathlib.Path):
     wait_for_all_replicas_active(peer_api_uris[0], "test_collection2")
     # Assert that the restarted node has recovered the new collection
     wait_for_all_replicas_active(new_url, "test_collection2")
+
+
+def test_rejoin_origin_from_wal(tmp_path: pathlib.Path):
+    """
+    This test checks that origin peer (first peer of the cluster) commits its own peer ID to consensus.
+
+    - remove origin peer from cluster
+    - modify second peer's `raft_state.json`, so that it does *not* provide origin peer ID and URL
+      when bootstrapping new peer
+    - add new peer to the cluster (bootstrapping from second peer), and check that it has valid
+      state after it syncs with consensus
+    - if new peer has valid state at the end of the test, it means it received correct origin peer
+      ID and URL from consensus
+    """
+
+    # Overwrite `first_voter` peer
+    def overwrite_first_voter(state: dict[str, Any], _: Any):
+        state["first_voter"] = state["this_peer_id"]
+        return state
+
+    rejoin_cluster_test(tmp_path, start_cluster, overwrite_first_voter)
+
+
+def rejoin_cluster_test(
+    tmp_path: pathlib.Path,
+    start_cluster: Callable[[pathlib.Path, int], tuple[list[str], list[pathlib.Path], str]],
+    raft_state: Callable[[dict[str, Any], int], Any | None],
+    collection: str = "test_collection",
+    peers: int = 3,
+    shards: int = 3,
+    expected_shards: int = 3,
+):
+    """
+    Parameterized test body, that tests adding new peer after origin peer was removed from the cluster.
+    See: <https://github.com/qdrant/qdrant/issues/5138>
+    """
+
+    # Start cluster
+    peer_uris, peer_dirs, bootstrap_uri = start_cluster(tmp_path, peers)
+
+    # Get origin peer ID
+    origin_peer_id = get_cluster_info(peer_uris[0])["peer_id"]
+
+    # Create collection, move all shards from first peer, remove first peer from cluster
+    create_collection(peer_uris[0], collection, shards, 1)
+    move_all_shards_from_peer(peer_uris[0], collection)
+    remove_peer(peer_uris[0])
+    processes.pop(0).kill()
+
+    # Generally, we could use *any* (second/third/random/last/etc) peer to bootstrap new peer from,
+    # but using second peer allows to (trivially) catch a single additional corner case in how we
+    # initialize consensus state when bootstrapping new peer.
+
+    # Kill second peer
+    second_peer = processes.pop(0)
+    second_peer.kill()
+
+    # Check/modify last peer `raft_state.json`
+    with open(f"{peer_dirs[1]}/storage/raft_state.json", "r+") as file:
+        state = json.load(file)
+
+        if new_state := raft_state(state, origin_peer_id):
+            file.seek(0, io.SEEK_SET)
+            file.truncate()
+            json.dump(new_state, file)
+
+    # Restart second peer with the same URI and ports
+    second_peer_uri, bootstrap_uri = start_first_peer(peer_dirs[1], "peer_0_1_restarted.log", second_peer.p2p_port)
+    wait_for_peer_online(second_peer_uri)
+
+    # Add new peer to cluster
+    new_peer_uri, new_peer_dir = add_new_peer(tmp_path, peers, bootstrap_uri, collection)
+
+    # Assert that new peer observe expected number of remote shards
+    info = get_collection_cluster_info(new_peer_uri, collection)
+    assert len(info["remote_shards"]) == expected_shards
+
+
+def move_all_shards_from_peer(peer_uri: str, collection: str = "test_collection") -> tuple[int, int]:
+    """
+    Moves all shards from peer at `peer_uri` to another (random) peer in the cluster.
+    """
+
+    # Find peer to move shards to
+    info = get_cluster_info(peer_uri)
+
+    current_peer_id = info["peer_id"]
+    other_peer_id = None
+
+    for peer_id, info in info["peers"].items():
+        peer_id = int(peer_id)
+
+        if peer_id != current_peer_id:
+            other_peer_id = peer_id
+            break
+
+    assert other_peer_id
+
+    # Move all shards from first peer to second peer
+    info = get_collection_cluster_info(peer_uri, collection)
+
+    for shard in info["local_shards"]:
+        resp = requests.post(f"{peer_uri}/collections/{collection}/cluster", json={
+            "move_shard": {
+                "from_peer_id": current_peer_id,
+                "to_peer_id": other_peer_id,
+                "shard_id": shard["shard_id"],
+            }
+        })
+
+        assert_http_ok(resp)
+
+    # Wait until all transfers finished
+    wait_for_collection_shard_transfers_count(peer_uri, collection, 0)
+
+    return current_peer_id, other_peer_id
+
+def remove_peer(peer_uri: str, peer_id: int | None = None):
+    if peer_id is None:
+        info = get_cluster_info(peer_uri)
+        peer_id = info["peer_id"]
+
+    resp = requests.delete(f"{peer_uri}/cluster/peer/{peer_id}")
+    assert_http_ok(resp)
+
+def add_new_peer(tmp_path: pathlib.Path, peer_idx: int, bootstrap_uri: str, collection: str | None = None):
+    peer_dir = make_peer_folder(tmp_path, peer_idx)
+    peer_uri = start_peer(peer_dir, f"peer_0_{peer_idx}.log", bootstrap_uri)
+
+    wait_for_peer_online(peer_uri)
+
+    if collection is not None:
+        wait_collection_on_all_peers(collection, [peer_uri])
+
+    return peer_uri, peer_dir

commit e6ba988baa7d2a306133e83cc3f4a2997f1b42d4
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Fri Oct 11 17:21:04 2024 +0200

    Persist and propagate origin peer when bootstrapping new nodes (#5147)

diff --git a/tests/consensus_tests/test_cluster_rejoin.py b/tests/consensus_tests/test_cluster_rejoin.py
index 6cdf88653..7abbba0ea 100644
--- a/tests/consensus_tests/test_cluster_rejoin.py
+++ b/tests/consensus_tests/test_cluster_rejoin.py
@@ -1,5 +1,7 @@
 import io
 import pathlib
+import shutil
+from time import sleep
 from typing import Any
 
 from consensus_tests.fixtures import create_collection, upsert_random_points, drop_collection
@@ -97,6 +99,104 @@ def test_rejoin_origin_from_wal(tmp_path: pathlib.Path):
 
     rejoin_cluster_test(tmp_path, start_cluster, overwrite_first_voter)
 
+def test_rejoin_origin_from_state(tmp_path: pathlib.Path):
+    """
+    This test checks that Qdrant persists origin peer ID (`first_voter` field in `raft_state.json`)
+    and propagates fake origin peer URL when bootstrapping new peer.
+
+    - start cluster using *preconfigured* origin peer that does *not* have origin peer ID and URL
+      committed to consensus
+    - remove origin peer from cluster
+    - assert that second peer's `raft_state.json` contains valid origin peer ID
+    - add new peer to the cluster (bootstrapping from second peer), and check that it has valid
+      state after it syncs with consensus
+    - if new peer has valid state at the end of the test, it means it received correct origin peer
+      ID and (fake) URL from second peer during bootstrap
+    """
+
+    # Assert origin peer ID is persisted as `first_voter`
+    def assert_first_voter(state: dict[str, Any], origin_peer_id: int):
+        assert state["first_voter"] == origin_peer_id
+
+    rejoin_cluster_test(tmp_path, start_preconfigured_cluster, assert_first_voter)
+
+@pytest.mark.skip("this test simulates and asserts past, incorrect behavior")
+def test_rejoin_no_origin(tmp_path: pathlib.Path):
+    """
+    This test checks that `rejoin_cluster_test` is sufficient to reproduce "missing origin peer" bug.
+
+    It simulates *earlier* behavior of Qdrant (bypassing all fixes to commit/persist/recover origin
+    peer ID/URL), and then checks that new peer added to such cluster has *invalid* state.
+
+    This test is disabled by default, but it's useful to "test the tests" and reproduce original bug.
+    """
+
+    # Overwrite `first_voter` peer
+    def overwrite_first_voter(state: dict[str, Any], _: Any):
+        state["first_voter"] = 1337
+        return state
+
+    rejoin_cluster_test(tmp_path, start_preconfigured_cluster, overwrite_first_voter, expected_shards=2)
+
+
+def test_rejoin_recover_origin(tmp_path: pathlib.Path):
+    """
+    This test checks that Qdrant recovers origin peer ID from WAL, if origin peer was not yet
+    removed from the cluster.
+    """
+
+    collection = "test_collection"
+    peers = 3
+    shards = 3
+
+    # Start cluster
+    peer_uris, peer_dirs, bootstrap_uri = start_preconfigured_cluster(tmp_path, peers)
+
+    # Get origin peer ID
+    origin_peer_id = get_cluster_info(peer_uris[0])["peer_id"]
+
+    # Wait a few seconds for consensus to catch up
+    sleep(5)
+
+    # Kill second peer
+    second_peer = processes.pop(1)
+    second_peer.kill()
+
+    # Remove `first_voter` from `raft_state.json`
+    with open(f"{peer_dirs[1]}/storage/raft_state.json", "r+") as file:
+        state = json.load(file)
+
+        del state["first_voter"]
+
+        file.seek(0, io.SEEK_SET)
+        file.truncate()
+        json.dump(state, file)
+
+    # Restart second peer with the same URI and ports
+    second_peer_uri, bootstrap_uri = start_first_peer(peer_dirs[1], "peer_0_1_restarted.log", second_peer.p2p_port)
+    wait_for_peer_online(second_peer_uri)
+
+    # Assert second peer recovered `first_voter` from WAL
+    with open(f"{peer_dirs[1]}/storage/raft_state.json", "r") as file:
+        state = json.load(file)
+        assert state["first_voter"] == origin_peer_id
+
+    # Create collection, move all shards from first peer, remove first peer from cluster
+    create_collection(peer_uris[0], collection, shards, 1)
+    move_all_shards_from_peer(peer_uris[0], collection)
+    remove_peer(peer_uris[0])
+    processes.pop(0).kill()
+
+    # Wait a few seconds for new leader
+    sleep(5)
+
+    # Add new peer to cluster
+    new_peer_uri, new_peer_dir = add_new_peer(tmp_path, peers, bootstrap_uri, collection)
+
+    # Assert that new peer observe expected number of remote shards
+    info = get_collection_cluster_info(new_peer_uri, collection)
+    assert len(info["remote_shards"]) == shards
+
 
 def rejoin_cluster_test(
     tmp_path: pathlib.Path,
@@ -152,6 +252,50 @@ def rejoin_cluster_test(
     info = get_collection_cluster_info(new_peer_uri, collection)
     assert len(info["remote_shards"]) == expected_shards
 
+def start_preconfigured_cluster(tmp_path: pathlib.Path, peers: int = 3):
+    assert_project_root()
+
+    # Collect peer URIs
+    peer_uris = []
+
+    # Create peer directories
+    peer_dirs = make_peer_folders(tmp_path, peers)
+
+    # Copy first peer Raft state and WAL from `test_cluster_rejoin_data`.
+    #
+    # It's just an "empty" peer, but its peer ID is *not* committed into WAL. We can use this peer to
+    # test that first peer ID is correctly recovered/propagated, even when it's not committed into WAL.
+    shutil.copytree("tests/consensus_tests/test_cluster_rejoin_data", f"{peer_dirs[0]}/storage")
+
+    # Modify peer URI in Raft state to prevent URI change on startup 🙄
+    p2p_port = get_port()
+    grpc_port = get_port()
+    http_port = get_port()
+
+    with open(f"{peer_dirs[0]}/storage/raft_state.json", "r+") as file:
+        state = json.load(file)
+
+        state["peer_address_by_id"][str(state["this_peer_id"])] = f"http://127.0.0.1:{p2p_port}"
+
+        file.seek(0, io.SEEK_SET)
+        file.truncate()
+        json.dump(state, file)
+
+    # Start first peer
+    first_peer_uri, bootstrap_uri = start_first_peer(peer_dirs[0], "peer_0_0.log", p2p_port)
+    peer_uris.append(first_peer_uri)
+
+    wait_for_peer_online(first_peer_uri)
+
+    # Bootstrap other peers
+    for peer_idx in range(1, peers):
+        peer_uri = start_peer(peer_dirs[peer_idx], f"peer_0_{peer_idx}.log", bootstrap_uri)
+        peer_uris.append(peer_uri)
+
+    wait_all_peers_up(peer_uris)
+
+    return peer_uris, peer_dirs, bootstrap_uri
+
 
 def move_all_shards_from_peer(peer_uri: str, collection: str = "test_collection") -> tuple[int, int]:
     """

commit dc6e9993455970892770ebd9224ac58f74e9812f
Author: Tim Visée <tim+github@visee.me>
Date:   Fri Jan 31 10:17:44 2025 +0100

    Add test for URI and boostrap URI environment variables (#5916)
    
    * Restructure process spawning in consensus tests, build list of args
    
    * Extend consensus test, also test if providing URIs through env vars work
    
    * Set URI and bootstrap variables unconditionally

diff --git a/tests/consensus_tests/test_cluster_rejoin.py b/tests/consensus_tests/test_cluster_rejoin.py
index 7abbba0ea..bf58d261d 100644
--- a/tests/consensus_tests/test_cluster_rejoin.py
+++ b/tests/consensus_tests/test_cluster_rejoin.py
@@ -13,10 +13,11 @@ N_REPLICA = 2
 N_SHARDS = 3
 
 
-def test_rejoin_cluster(tmp_path: pathlib.Path):
+@pytest.mark.parametrize("uris_in_env", [False, True])
+def test_rejoin_cluster(tmp_path: pathlib.Path, uris_in_env):
     assert_project_root()
     # Start cluster
-    peer_api_uris, peer_dirs, bootstrap_uri = start_cluster(tmp_path, N_PEERS, port_seed=10000)
+    peer_api_uris, peer_dirs, bootstrap_uri = start_cluster(tmp_path, N_PEERS, port_seed=10000, uris_in_env=uris_in_env)
 
     create_collection(peer_api_uris[0], shard_number=N_SHARDS, replication_factor=N_REPLICA)
     wait_collection_exists_and_active_on_all_peers(collection_name="test_collection", peer_api_uris=peer_api_uris)
@@ -55,7 +56,7 @@ def test_rejoin_cluster(tmp_path: pathlib.Path):
     )
 
     # Restart last node
-    new_url = start_peer(peer_dirs[-1], "peer_0_restarted.log", bootstrap_uri, port=20000)
+    new_url = start_peer(peer_dirs[-1], "peer_0_restarted.log", bootstrap_uri, port=20000, uris_in_env=uris_in_env)
 
     peer_api_uris[-1] = new_url
 

</code></pre>
        </section>
    </main>
    <footer>
        <p>LoCoDiff-bench - <a href="https://github.com/AbanteAI/LoCoDiff-bench">GitHub Repository</a></p>
    </footer>
    
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            hljs.highlightAll();
        });
    </script>
</body>
</html>
    