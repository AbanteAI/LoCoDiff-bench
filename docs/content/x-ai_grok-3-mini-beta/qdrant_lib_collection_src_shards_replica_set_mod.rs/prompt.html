<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Prompt: lib/collection/src/shards/replica_set/mod.rs - Grok 3 Mini</title>
    <link rel="stylesheet" href="../../../../styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
</head>
<body>
    <header>
        <h1>Prompt: lib/collection/src/shards/replica_set/mod.rs</h1>
        <h2>Model: Grok 3 Mini</h2>
        <p><a href="../../../cases/x-ai_grok-3-mini-beta/qdrant_lib_collection_src_shards_replica_set_mod.rs.html">Back to Case</a> | <a href="../../../cases.html">All Cases</a> | <a href="../../../index.html">Home</a></p>
    </header>
    <main>
        <section>
            <h2>Prompt Content</h2>
            <pre><code class="language-plaintext"># Instructions

You are being benchmarked. You will see the output of a git log command, and from that must infer the current state of a file. Think carefully, as you must output the exact state of the file to earn full marks.

**Important:** Your goal is to reproduce the file's content *exactly* as it exists at the final commit, even if the code appears broken, buggy, or contains obvious errors. Do **not** try to "fix" the code. Attempting to correct issues will result in a poor score, as this benchmark evaluates your ability to reproduce the precise state of the file based on its history.

# Required Response Format

Wrap the content of the file in triple backticks (```). Any text outside the final closing backticks will be ignored. End your response after outputting the closing backticks.

# Example Response

```python
#!/usr/bin/env python
print('Hello, world!')
```

# File History

> git log -p --cc --topo-order --reverse -- lib/collection/src/shards/replica_set/mod.rs

commit adb840737c85ca7ba0e90e1ee2aea8a685742e2e
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Fri Oct 13 22:26:12 2023 +0200

    Split `replica_set.rs` into sub-modules (#2818)
    
    * Rename `replica_set.rs` to `replica_set/mod.rs`
    
    * Fix code formatting in `ShardReplicaSet::load`
    
    * Remove unused items in `replica_set/mod.rs`
    
    * Extract `replica_set/execute_read_operation.rs`
    
    * Extract `replica_set/read_ops.rs`
    
    * Extract `replica_set/shard_transfer.rs`
    
    * Extract `replica_set/snapshots.rs`
    
    * Extract `replica_set/update.rs`
    
    * Reorder items in `replica_set/mod.rs` (1/19)
    
    * Reorder items in `replica_set/mod.rs` (2/19)
    
    * Reorder items in `replica_set/mod.rs` (3/19)
    
    * Reorder items in `replica_set/mod.rs` (4/19)
    
    * Reorder items in `replica_set/mod.rs` (5/19)
    
    * Reorder items in `replica_set/mod.rs` (6/19)
    
    * Reorder items in `replica_set/mod.rs` (7/19)
    
    * Reorder items in `replica_set/mod.rs` (8/19)
    
    * Reorder items in `replica_set/mod.rs` (9/19)
    
    * Reorder items in `replica_set/mod.rs` (10/19)
    
    * Reorder items in `replica_set/mod.rs` (11/19)
    
    * Reorder items in `replica_set/mod.rs` (12/19)
    
    * Reorder items in `replica_set/mod.rs` (13/19)
    
    * Reorder items in `replica_set/mod.rs` (14/19)
    
    * Reorder items in `replica_set/mod.rs` (15/19)
    
    * Reorder items in `replica_set/mod.rs` (16/19)
    
    * Reorder items in `replica_set/mod.rs` (17/19)
    
    * Reorder items in `replica_set/mod.rs` (18/19)
    
    * Reorder items in `replica_set/mod.rs` (19/19)
    
    * Add explicit imports to `replica_set/execute_read_operation.rs`
    
    * Reorder items in `replica_set/read_ops.rs` (1/2)
    
    * Reorder items in `replica_set/read_ops.rs` (2/2)
    
    * Add explicit imports to `replica_set/read_ops.rs`
    
    * Add explicit imports to `replica_set/shard_transfer.rs`
    
    * Reorder items in `replica_set/snapshots.rs` (1/2)
    
    * Reorder items in `replica_set/snapshots.rs` (2/2)
    
    * Add explicit imports to `replica_set/snapshots.rs`
    
    * Reorder items in `replica_set/update.rs` (1/?)
    
    * Reorder items in `replica_set/update.rs` (2/?)
    
    * Reorder items in `replica_set/update.rs` (3/?)
    
    * Reorder items in `replica_set/update.rs` (4/?)
    
    * Reorder items in `replica_set/update.rs` (5/?)
    
    * Reorder items in `replica_set/update.rs` (6/?)
    
    * Reorder items in `replica_set/update.rs` (7/?)
    
    * Add explicit imports to `replica_set/update.rs`
    
    * Optimize imports in `replica_set/mod.rs`
    
    * fixup! Add explicit imports to `replica_set/update.rs`
    
    Add missing imports to the `tests` sub-module

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
new file mode 100644
index 000000000..cbe0eb810
--- /dev/null
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -0,0 +1,717 @@
+mod execute_read_operation;
+mod read_ops;
+mod shard_transfer;
+mod snapshots;
+mod update;
+
+use std::collections::{HashMap, HashSet};
+use std::ops::Deref as _;
+use std::path::{Path, PathBuf};
+use std::sync::Arc;
+
+use schemars::JsonSchema;
+use serde::{Deserialize, Serialize};
+use tokio::runtime::Handle;
+use tokio::sync::{Mutex, RwLock};
+
+use super::local_shard::LocalShard;
+use super::remote_shard::RemoteShard;
+use super::CollectionId;
+use crate::config::CollectionConfig;
+use crate::operations::shared_storage_config::SharedStorageConfig;
+use crate::operations::types::CollectionResult;
+use crate::save_on_disk::SaveOnDisk;
+use crate::shards::channel_service::ChannelService;
+use crate::shards::dummy_shard::DummyShard;
+use crate::shards::shard::{PeerId, Shard, ShardId};
+use crate::shards::shard_config::ShardConfig;
+use crate::shards::telemetry::ReplicaSetTelemetry;
+
+//    │    Collection Created
+//    │
+//    ▼
+//  ┌──────────────┐
+//  │              │
+//  │ Initializing │
+//  │              │
+//  └──────┬───────┘
+//         │  Report created    ┌───────────┐
+//         └────────────────────►           │
+//             Activate         │ Consensus │
+//        ┌─────────────────────┤           │
+//        │                     └───────────┘
+//  ┌─────▼───────┐   User Promote           ┌──────────┐
+//  │             ◄──────────────────────────►          │
+//  │ Active      │                          │ Listener │
+//  │             ◄───────────┐              │          │
+//  └──┬──────────┘           │Transfer      └──┬───────┘
+//     │                      │Finished         │
+//     │               ┌──────┴────────┐        │Update
+//     │Update         │               │        │Failure
+//     │Failure        │ Partial       ├───┐    │
+//     │               │               │   │    │
+//     │               └───────▲───────┘   │    │
+//     │                       │           │    │
+//  ┌──▼──────────┐ Transfer   │           │    │
+//  │             │ Started    │           │    │
+//  │ Dead        ├────────────┘           │    │
+//  │             │                        │    │
+//  └─▲───────▲───┘        Transfer        │    │
+//    │       │            Failed/Cancelled│    │
+//    │       └────────────────────────────┘    │
+//    │                                         │
+//    └─────────────────────────────────────────┘
+//
+
+/// A set of shard replicas.
+/// Handles operations so that the state is consistent across all the replicas of the shard.
+/// Prefers local shard for read-only operations.
+/// Perform updates on all replicas and report error if there is at least one failure.
+///
+pub struct ShardReplicaSet {
+    local: RwLock<Option<Shard>>, // Abstract Shard to be able to use a Proxy during replication
+    remotes: RwLock<Vec<RemoteShard>>,
+    replica_state: Arc<SaveOnDisk<ReplicaSetState>>,
+    /// List of peers that are marked as dead locally, but are not yet submitted to the consensus.
+    /// List is checked on each consensus round and submitted to the consensus.
+    /// If the state of the peer is changed in the consensus, it is removed from the list.
+    /// Update and read operations are not performed on the peers marked as dead.
+    locally_disabled_peers: parking_lot::RwLock<HashSet<PeerId>>,
+    pub(crate) shard_path: PathBuf,
+    pub(crate) shard_id: ShardId,
+    notify_peer_failure_cb: ChangePeerState,
+    channel_service: ChannelService,
+    collection_id: CollectionId,
+    collection_config: Arc<RwLock<CollectionConfig>>,
+    shared_storage_config: Arc<SharedStorageConfig>,
+    update_runtime: Handle,
+    search_runtime: Handle,
+    /// Lock to serialized write operations on the replicaset when a write ordering is used.
+    write_ordering_lock: Mutex<()>,
+}
+
+pub type ChangePeerState = Arc<dyn Fn(PeerId, ShardId) + Send + Sync>;
+
+const REPLICA_STATE_FILE: &str = "replica_state.json";
+
+impl ShardReplicaSet {
+    /// Create a new fresh replica set, no previous state is expected.
+    #[allow(clippy::too_many_arguments)]
+    pub async fn build(
+        shard_id: ShardId,
+        collection_id: CollectionId,
+        this_peer_id: PeerId,
+        local: bool,
+        remotes: HashSet<PeerId>,
+        on_peer_failure: ChangePeerState,
+        collection_path: &Path,
+        collection_config: Arc<RwLock<CollectionConfig>>,
+        shared_storage_config: Arc<SharedStorageConfig>,
+        channel_service: ChannelService,
+        update_runtime: Handle,
+        search_runtime: Handle,
+    ) -> CollectionResult<Self> {
+        let shard_path = super::create_shard_dir(collection_path, shard_id).await?;
+        let local = if local {
+            let shard = LocalShard::build(
+                shard_id,
+                collection_id.clone(),
+                &shard_path,
+                collection_config.clone(),
+                shared_storage_config.clone(),
+                update_runtime.clone(),
+            )
+            .await?;
+            Some(Shard::Local(shard))
+        } else {
+            None
+        };
+        let replica_state: SaveOnDisk<ReplicaSetState> =
+            SaveOnDisk::load_or_init(shard_path.join(REPLICA_STATE_FILE))?;
+        replica_state.write(|rs| {
+            rs.this_peer_id = this_peer_id;
+            if local.is_some() {
+                rs.is_local = true;
+                rs.set_peer_state(this_peer_id, ReplicaState::Initializing);
+            }
+            for peer in remotes {
+                rs.set_peer_state(peer, ReplicaState::Initializing);
+            }
+        })?;
+
+        let remote_shards = Self::init_remote_shards(
+            shard_id,
+            collection_id.clone(),
+            &replica_state.read(),
+            &channel_service,
+        );
+
+        // Save shard config as the last step, to ensure that the file state is consistent
+        // Presence of shard config indicates that the shard is ready to be used
+        let replica_set_shard_config = ShardConfig::new_replica_set();
+        replica_set_shard_config.save(&shard_path)?;
+
+        Ok(Self {
+            shard_id,
+            local: RwLock::new(local),
+            remotes: RwLock::new(remote_shards),
+            replica_state: replica_state.into(),
+            locally_disabled_peers: Default::default(),
+            shard_path,
+            notify_peer_failure_cb: on_peer_failure,
+            channel_service,
+            collection_id,
+            collection_config,
+            shared_storage_config,
+            update_runtime,
+            search_runtime,
+            write_ordering_lock: Mutex::new(()),
+        })
+    }
+
+    /// Recovers shard from disk.
+    ///
+    /// WARN: This method intended to be used only on the initial start of the node.
+    /// It does not implement any logic to recover from a failure.
+    /// Will panic or load partial state if there is a failure.
+    #[allow(clippy::too_many_arguments)]
+    pub async fn load(
+        shard_id: ShardId,
+        collection_id: CollectionId,
+        shard_path: &Path,
+        collection_config: Arc<RwLock<CollectionConfig>>,
+        shared_storage_config: Arc<SharedStorageConfig>,
+        channel_service: ChannelService,
+        on_peer_failure: ChangePeerState,
+        this_peer_id: PeerId,
+        update_runtime: Handle,
+        search_runtime: Handle,
+    ) -> Self {
+        let replica_state: SaveOnDisk<ReplicaSetState> =
+            SaveOnDisk::load_or_init(shard_path.join(REPLICA_STATE_FILE)).unwrap();
+
+        if replica_state.read().this_peer_id != this_peer_id {
+            replica_state
+                .write(|rs| {
+                    let this_peer_id = rs.this_peer_id;
+                    let local_state = rs.remove_peer_state(&this_peer_id);
+                    if let Some(state) = local_state {
+                        rs.set_peer_state(this_peer_id, state);
+                    }
+                    rs.this_peer_id = this_peer_id;
+                })
+                .map_err(|e| {
+                    panic!("Failed to update replica state in {shard_path:?}: {e}");
+                })
+                .unwrap();
+        }
+
+        let remote_shards: Vec<_> = Self::init_remote_shards(
+            shard_id,
+            collection_id.clone(),
+            &replica_state.read(),
+            &channel_service,
+        );
+
+        let mut local_load_failure = false;
+        let local = if replica_state.read().is_local {
+            let shard = if let Some(recovery_reason) = &shared_storage_config.recovery_mode {
+                Shard::Dummy(DummyShard::new(recovery_reason))
+            } else {
+                let res = LocalShard::load(
+                    shard_id,
+                    collection_id.clone(),
+                    shard_path,
+                    collection_config.clone(),
+                    shared_storage_config.clone(),
+                    update_runtime.clone(),
+                )
+                .await;
+
+                match res {
+                    Ok(shard) => Shard::Local(shard),
+                    Err(err) => {
+                        if !shared_storage_config.handle_collection_load_errors {
+                            panic!("Failed to load local shard {shard_path:?}: {err}")
+                        }
+
+                        local_load_failure = true;
+
+                        log::error!(
+                            "Failed to load local shard {shard_path:?}, \
+                             initializing \"dummy\" shard instead: \
+                             {err}"
+                        );
+
+                        Shard::Dummy(DummyShard::new(format!(
+                            "Failed to load local shard {shard_path:?}: {err}"
+                        )))
+                    }
+                }
+            };
+
+            Some(shard)
+        } else {
+            None
+        };
+
+        let replica_set = Self {
+            shard_id,
+            local: RwLock::new(local),
+            remotes: RwLock::new(remote_shards),
+            replica_state: replica_state.into(),
+            // TODO: move to collection config
+            locally_disabled_peers: Default::default(),
+            shard_path: shard_path.to_path_buf(),
+            notify_peer_failure_cb: on_peer_failure,
+            channel_service,
+            collection_id,
+            collection_config,
+            shared_storage_config,
+            update_runtime,
+            search_runtime,
+            write_ordering_lock: Mutex::new(()),
+        };
+
+        if local_load_failure && replica_set.active_remote_shards().await.is_empty() {
+            replica_set
+                .locally_disabled_peers
+                .write()
+                .insert(this_peer_id);
+        }
+
+        replica_set
+    }
+
+    pub fn this_peer_id(&self) -> PeerId {
+        self.replica_state.read().this_peer_id
+    }
+
+    pub async fn has_local_shard(&self) -> bool {
+        self.local.read().await.is_some()
+    }
+
+    pub async fn is_local(&self) -> bool {
+        let local_read = self.local.read().await;
+        matches!(*local_read, Some(Shard::Local(_) | Shard::Dummy(_)))
+    }
+
+    pub async fn is_dummy(&self) -> bool {
+        let local_read = self.local.read().await;
+        matches!(*local_read, Some(Shard::Dummy(_)))
+    }
+
+    pub fn peers(&self) -> HashMap<PeerId, ReplicaState> {
+        self.replica_state.read().peers()
+    }
+
+    pub fn peer_state(&self, peer_id: &PeerId) -> Option<ReplicaState> {
+        self.replica_state.read().get_peer_state(peer_id).copied()
+    }
+
+    pub async fn active_remote_shards(&self) -> Vec<PeerId> {
+        let replica_state = self.replica_state.read();
+        let this_peer_id = replica_state.this_peer_id;
+        replica_state
+            .active_peers()
+            .into_iter()
+            .filter(|peer_id| !self.is_locally_disabled(peer_id) && *peer_id != this_peer_id)
+            .collect()
+    }
+
+    pub async fn init_empty_local_shard(&self) -> CollectionResult<()> {
+        let mut local = self.local.write().await;
+
+        let current_shard = local.take();
+
+        // ToDo: Remove shard files here?
+        let local_shard_res = LocalShard::build(
+            self.shard_id,
+            self.collection_id.clone(),
+            &self.shard_path,
+            self.collection_config.clone(),
+            self.shared_storage_config.clone(),
+            self.update_runtime.clone(),
+        )
+        .await;
+
+        match local_shard_res {
+            Ok(local_shard) => {
+                *local = Some(Shard::Local(local_shard));
+                Ok(())
+            }
+            Err(err) => {
+                log::error!(
+                    "Failed to initialize local shard {:?}: {err}",
+                    self.shard_path
+                );
+                *local = current_shard;
+                Err(err)
+            }
+        }
+    }
+
+    pub async fn set_local(
+        &self,
+        local: LocalShard,
+        state: Option<ReplicaState>,
+    ) -> CollectionResult<Option<Shard>> {
+        let old_shard = self.local.write().await.replace(Shard::Local(local));
+
+        if !self.replica_state.read().is_local || state.is_some() {
+            self.replica_state.write(|rs| {
+                rs.is_local = true;
+                if let Some(active) = state {
+                    rs.set_peer_state(self.this_peer_id(), active);
+                }
+            })?;
+        }
+        self.update_locally_disabled(self.this_peer_id());
+        Ok(old_shard)
+    }
+
+    pub async fn remove_local(&self) -> CollectionResult<()> {
+        self.replica_state.write(|rs| {
+            rs.is_local = false;
+            let this_peer_id = rs.this_peer_id;
+            rs.remove_peer_state(&this_peer_id);
+        })?;
+
+        self.update_locally_disabled(self.this_peer_id());
+
+        let removing_local = {
+            let mut local = self.local.write().await;
+            local.take()
+        };
+
+        if let Some(removing_local) = removing_local {
+            // stop ongoing tasks and delete data
+            drop(removing_local);
+            LocalShard::clear(&self.shard_path).await?;
+        }
+        Ok(())
+    }
+
+    pub async fn add_remote(&self, peer_id: PeerId, state: ReplicaState) -> CollectionResult<()> {
+        self.replica_state.write(|rs| {
+            rs.set_peer_state(peer_id, state);
+        })?;
+
+        self.update_locally_disabled(peer_id);
+
+        let mut remotes = self.remotes.write().await;
+
+        // check remote already exists
+        if remotes.iter().any(|remote| remote.peer_id == peer_id) {
+            return Ok(());
+        }
+
+        remotes.push(RemoteShard::new(
+            self.shard_id,
+            self.collection_id.clone(),
+            peer_id,
+            self.channel_service.clone(),
+        ));
+
+        Ok(())
+    }
+
+    pub async fn remove_remote(&self, peer_id: PeerId) -> CollectionResult<()> {
+        self.replica_state.write(|rs| {
+            rs.remove_peer_state(&peer_id);
+        })?;
+
+        self.update_locally_disabled(peer_id);
+
+        let mut remotes = self.remotes.write().await;
+        remotes.retain(|remote| remote.peer_id != peer_id);
+        Ok(())
+    }
+
+    /// Change state of the replica to the given.
+    /// Ensure that remote shard is initialized.
+    pub async fn ensure_replica_with_state(
+        &self,
+        peer_id: &PeerId,
+        state: ReplicaState,
+    ) -> CollectionResult<()> {
+        if *peer_id == self.replica_state.read().this_peer_id {
+            self.set_replica_state(peer_id, state)?;
+        } else {
+            // Create remote shard if necessary
+            self.add_remote(*peer_id, state).await?;
+        }
+        Ok(())
+    }
+
+    pub fn set_replica_state(&self, peer_id: &PeerId, state: ReplicaState) -> CollectionResult<()> {
+        log::debug!(
+            "Changing local shard {}:{} state from {:?} to {state:?}",
+            self.collection_id,
+            self.shard_id,
+            self.replica_state.read().get_peer_state(peer_id),
+        );
+
+        self.replica_state.write(|rs| {
+            if rs.this_peer_id == *peer_id {
+                rs.is_local = true;
+            }
+            rs.set_peer_state(*peer_id, state);
+        })?;
+        self.update_locally_disabled(*peer_id);
+        Ok(())
+    }
+
+    pub async fn remove_peer(&self, peer_id: PeerId) -> CollectionResult<()> {
+        if self.this_peer_id() == peer_id {
+            self.remove_local().await?;
+        } else {
+            self.remove_remote(peer_id).await?;
+        }
+        Ok(())
+    }
+
+    pub async fn apply_state(
+        &self,
+        replicas: HashMap<PeerId, ReplicaState>,
+    ) -> CollectionResult<()> {
+        let old_peers = self.replica_state.read().peers();
+
+        self.replica_state.write(|state| {
+            state.set_peers(replicas.clone());
+        })?;
+
+        self.locally_disabled_peers.write().clear();
+
+        let removed_peers = old_peers
+            .keys()
+            .filter(|peer_id| !replicas.contains_key(peer_id))
+            .copied()
+            .collect::<Vec<_>>();
+        for peer_id in removed_peers {
+            self.remove_peer(peer_id).await?;
+        }
+
+        for (peer_id, state) in replicas {
+            let peer_already_exists = old_peers.get(&peer_id).is_some();
+
+            if peer_already_exists {
+                // do nothing
+                // We only need to change state and it is already saved
+                continue;
+            }
+
+            if peer_id == self.this_peer_id() {
+                // Consensus wants a local replica on this peer
+                let local_shard = LocalShard::build(
+                    self.shard_id,
+                    self.collection_id.clone(),
+                    &self.shard_path,
+                    self.collection_config.clone(),
+                    self.shared_storage_config.clone(),
+                    self.update_runtime.clone(),
+                )
+                .await?;
+                match state {
+                    ReplicaState::Active => {
+                        // No way we can provide up-to-date replica right away at this point,
+                        // so we report a failure to consensus
+                        self.set_local(local_shard, Some(ReplicaState::Active))
+                            .await?;
+                        self.notify_peer_failure(peer_id);
+                    }
+                    ReplicaState::Dead => {
+                        self.set_local(local_shard, Some(ReplicaState::Dead))
+                            .await?;
+                    }
+                    ReplicaState::Partial => {
+                        self.set_local(local_shard, Some(ReplicaState::Partial))
+                            .await?;
+                    }
+                    ReplicaState::Initializing => {
+                        self.set_local(local_shard, Some(ReplicaState::Initializing))
+                            .await?;
+                    }
+                    ReplicaState::Listener => {
+                        // Same as `Active`, we report a failure to consensus
+                        self.set_local(local_shard, Some(ReplicaState::Listener))
+                            .await?;
+                        self.notify_peer_failure(peer_id);
+                    }
+                }
+                continue;
+            }
+
+            // Otherwise it is a missing remote replica, we simply create it
+
+            let new_remote = RemoteShard::new(
+                self.shard_id,
+                self.collection_id.clone(),
+                peer_id,
+                self.channel_service.clone(),
+            );
+            self.remotes.write().await.push(new_remote);
+        }
+        Ok(())
+    }
+
+    pub(crate) async fn on_optimizer_config_update(&self) -> CollectionResult<()> {
+        let read_local = self.local.read().await;
+        if let Some(shard) = &*read_local {
+            shard.on_optimizer_config_update().await
+        } else {
+            Ok(())
+        }
+    }
+
+    /// Check if the are any locally disabled peers
+    /// And if so, report them to the consensus
+    pub async fn sync_local_state(&self) -> CollectionResult<()> {
+        for failed_peer in self.locally_disabled_peers.read().iter() {
+            self.notify_peer_failure(*failed_peer);
+        }
+        Ok(())
+    }
+
+    pub(crate) async fn get_telemetry_data(&self) -> ReplicaSetTelemetry {
+        let local_shard = self.local.read().await;
+        let local = local_shard
+            .as_ref()
+            .map(|local_shard| local_shard.get_telemetry_data());
+        ReplicaSetTelemetry {
+            id: self.shard_id,
+            local,
+            remote: self
+                .remotes
+                .read()
+                .await
+                .iter()
+                .map(|remote| remote.get_telemetry_data())
+                .collect(),
+            replicate_states: self.replica_state.read().peers(),
+        }
+    }
+
+    fn init_remote_shards(
+        shard_id: ShardId,
+        collection_id: CollectionId,
+        state: &ReplicaSetState,
+        channel_service: &ChannelService,
+    ) -> Vec<RemoteShard> {
+        state
+            .peers()
+            .iter()
+            .filter(|(peer, _)| **peer != state.this_peer_id)
+            .map(|(peer_id, _is_active)| {
+                RemoteShard::new(
+                    shard_id,
+                    collection_id.clone(),
+                    *peer_id,
+                    channel_service.clone(),
+                )
+            })
+            .collect()
+    }
+
+    /// Check whether a peer is registered as `active`.
+    /// Unknown peers are not active.
+    fn peer_is_active(&self, peer_id: &PeerId) -> bool {
+        self.peer_state(peer_id) == Some(ReplicaState::Active) && !self.is_locally_disabled(peer_id)
+    }
+
+    fn is_locally_disabled(&self, peer_id: &PeerId) -> bool {
+        self.locally_disabled_peers.read().contains(peer_id)
+    }
+
+    // Make sure that locally disabled peers do not contradict the consensus
+    fn update_locally_disabled(&self, peer_id_to_remove: PeerId) {
+        // Check that we are not trying to disable the last active peer
+        let peers = self.peers();
+
+        let active_peers: HashSet<_> = peers
+            .iter()
+            .filter(|(_, state)| **state == ReplicaState::Active)
+            .map(|(peer, _)| *peer)
+            .collect();
+
+        let mut locally_disabled = self.locally_disabled_peers.write();
+
+        locally_disabled.remove(&peer_id_to_remove);
+
+        if active_peers.is_subset(&locally_disabled) {
+            log::warn!("Resolving consensus/local state inconsistency");
+            locally_disabled.clear();
+        }
+    }
+
+    fn notify_peer_failure(&self, peer_id: PeerId) {
+        log::debug!("Notify peer failure: {}", peer_id);
+        self.notify_peer_failure_cb.deref()(peer_id, self.shard_id)
+    }
+}
+
+/// Represents a replica set state
+#[derive(Debug, Deserialize, Serialize, Default, PartialEq, Eq, Clone)]
+pub struct ReplicaSetState {
+    pub is_local: bool,
+    pub this_peer_id: PeerId,
+    peers: HashMap<PeerId, ReplicaState>,
+}
+
+impl ReplicaSetState {
+    pub fn get_peer_state(&self, peer_id: &PeerId) -> Option<&ReplicaState> {
+        self.peers.get(peer_id)
+    }
+
+    pub fn set_peer_state(&mut self, peer_id: PeerId, state: ReplicaState) {
+        self.peers.insert(peer_id, state);
+    }
+
+    pub fn remove_peer_state(&mut self, peer_id: &PeerId) -> Option<ReplicaState> {
+        self.peers.remove(peer_id)
+    }
+
+    pub fn peers(&self) -> HashMap<PeerId, ReplicaState> {
+        self.peers.clone()
+    }
+
+    pub fn active_peers(&self) -> Vec<PeerId> {
+        self.peers
+            .iter()
+            .filter_map(|(peer_id, state)| {
+                if *state == ReplicaState::Active {
+                    Some(*peer_id)
+                } else {
+                    None
+                }
+            })
+            .collect()
+    }
+
+    pub fn set_peers(&mut self, peers: HashMap<PeerId, ReplicaState>) {
+        self.peers = peers;
+    }
+}
+
+/// State of the single shard within a replica set.
+#[derive(Debug, Deserialize, Serialize, JsonSchema, Default, PartialEq, Eq, Hash, Clone, Copy)]
+pub enum ReplicaState {
+    // Active and sound
+    #[default]
+    Active,
+    // Failed for some reason
+    Dead,
+    // The shard is partially loaded and is currently receiving data from other shards
+    Partial,
+    // Collection is being created
+    Initializing,
+    // A shard which receives data, but is not used for search
+    // Useful for backup shards
+    Listener,
+}
+
+/// Represents a change in replica set, due to scaling of `replication_factor`
+#[derive(Debug, Deserialize, Serialize, PartialEq, Eq, Hash, Clone)]
+pub enum Change {
+    Remove(ShardId, PeerId),
+}

commit 7d00b5ed226719f16ef32b7a77a09410cf5da3d6
Author: Tim Visée <tim+github@visee.me>
Date:   Mon Oct 16 18:37:00 2023 +0200

    Fix first try of shard transfer always failing (#2813)
    
    * Add function to wait on replica set state
    
    * On shard transfer init on receiver, wait for shard to be initialized
    
    * Promote consensus meta wait to common constant, use for local timeout
    
    * Improve constant name and use everywhere with `defaults::` prefix
    
    * Do not block async runtime with blocking replica set wait

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index cbe0eb810..020b56d5e 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -8,6 +8,7 @@ use std::collections::{HashMap, HashSet};
 use std::ops::Deref as _;
 use std::path::{Path, PathBuf};
 use std::sync::Arc;
+use std::time::Duration;
 
 use schemars::JsonSchema;
 use serde::{Deserialize, Serialize};
@@ -19,7 +20,7 @@ use super::remote_shard::RemoteShard;
 use super::CollectionId;
 use crate::config::CollectionConfig;
 use crate::operations::shared_storage_config::SharedStorageConfig;
-use crate::operations::types::CollectionResult;
+use crate::operations::types::{CollectionError, CollectionResult};
 use crate::save_on_disk::SaveOnDisk;
 use crate::shards::channel_service::ChannelService;
 use crate::shards::dummy_shard::DummyShard;
@@ -319,6 +320,35 @@ impl ShardReplicaSet {
             .collect()
     }
 
+    /// Wait for a local shard to be initialized.
+    ///
+    /// Uses a blocking thread internally.
+    ///
+    /// Returns `true` if initialized, `false` if timed out.
+    pub async fn wait_for_local(&self, timeout: Duration) -> CollectionResult<bool> {
+        self.wait_for(|replica_set_state| replica_set_state.is_local, timeout)
+            .await
+    }
+
+    /// Wait for a replica set state condition to be true.
+    ///
+    /// Uses a blocking thread internally.
+    ///
+    /// Returns `true` if condition is true, `false` if timed out.
+    async fn wait_for<F>(&self, check: F, timeout: Duration) -> CollectionResult<bool>
+    where
+        F: Fn(&ReplicaSetState) -> bool + Send + 'static,
+    {
+        let replica_state = self.replica_state.clone();
+        tokio::task::spawn_blocking(move || replica_state.wait_for(check, timeout))
+            .await
+            .map_err(|err| {
+                CollectionError::service_error(format!(
+                    "Failed to wait for replica set state: {err}"
+                ))
+            })
+    }
+
     pub async fn init_empty_local_shard(&self) -> CollectionResult<()> {
         let mut local = self.local.write().await;
 

commit 3fc7e0908ed15829e336f0e48f5f3cdd0ac0cca6
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Wed Oct 25 16:12:19 2023 +0200

    Add `ReplicaState::PartialSnapshot` (#2858)
    
    * WIP: Add `ReplicaState::PartialSnapshot`
    
    * Add documentation
    
    * fixup! Add documentation
    
    * Update OpenAPI spec
    
    * Remove `notify_peer_failure` call for `PartialSnapshot` branch in `apply_state`

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 020b56d5e..51109fe07 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -568,6 +568,10 @@ impl ShardReplicaSet {
                             .await?;
                         self.notify_peer_failure(peer_id);
                     }
+                    ReplicaState::PartialSnapshot => {
+                        self.set_local(local_shard, Some(ReplicaState::PartialSnapshot))
+                            .await?;
+                    }
                 }
                 continue;
             }
@@ -738,6 +742,8 @@ pub enum ReplicaState {
     // A shard which receives data, but is not used for search
     // Useful for backup shards
     Listener,
+    // Snapshot shard transfer is in progress, updates aren't sent to the shard
+    PartialSnapshot,
 }
 
 /// Represents a change in replica set, due to scaling of `replication_factor`

commit 5bdf5e4bfe5ca2de32aff603a755ad263d5eca56
Author: Tim Visée <tim+github@visee.me>
Date:   Mon Oct 30 12:44:39 2023 +0100

    Shard snapshot transfer integration (#2467)
    
    * Clone inside blocks
    
    * Add shard transfer method to distinguish between batching and snapshots
    
    * Add stub method to drive snapshot transfer
    
    * Store remote shard in forward proxy, merge unproxy methods
    
    * On snapshot shard transfer, create a shard snapshot
    
    * Unify logic for unproxifying forward and queue proxy
    
    * Error snapshot transfer if shard is not a queue proxy
    
    * Add remote shard function to request remote HTTP port
    
    * Handle all specific shard types when proxifying
    
    * Allow queue proxy for some shard holder snapshot methods
    
    * Bring local and remote shard snapshot transfer URLs into transfer logic
    
    * Expose optional shard transfer method parameter in REST and gRPC API
    
    * Expose shard transfer method in list of active transfers
    
    * Fix off-by-one error in queue proxy shard batch transfer logic
    
    * Do not set max ack version for WAL twice, already set when finalizing
    
    * Merge comment for two similar calls
    
    * Use reqwest client to transfer and recover shard snapshot on remote
    
    Using the reqwest client should be temporary. We better switch to a gRPC
    call here eventually to use our existing channels. That way we don't
    require an extra HTTP client (and dependency) just for this.
    
    * Send queue proxy updates to remote when shard is transferred
    
    * On shard queue transfer, set max WAL ack to last transferred
    
    * Add safe queue proxy destructor, skip destructing in error
    
    This adds a finalize method to safely destruct a queue proxy shard. It
    ensures that all remaining updates are transferred to the remote, and
    that the max acknowledged version for our WAL is released. Only then is
    the queue proxy shard destructed unwrapping the inner local shard.
    
    Our unproxify logic now ensures that the queue proxy shard remains if
    transferring the updates fails.
    
    * Clean up method driving shard snapshot transfer a bit
    
    * Change default shard transfer method to stream records
    
    This changes the default transfer method to stream records rather than
    using a snaphsot transfer. We can switch this once snapshot transfer is
    fully integrated.
    
    * Improve error handling, don't panic but return proper error
    
    * Do not unwrap in type conversions
    
    * Update OpenAPI and gRPC specification
    
    * Resolve and remove some TODOs
    
    * During shard snapshot transfer, use REST port from config
    
    * Always release max acknowledged WAL version on queue proxy finalize
    
    * Rework queue unproxying, transform into forward proxy to handle errors
    
    When a queue or forward proxy shard needs to be unproxified into a local
    shard again we typically don't have room to handle errors. A queue proxy
    shard may error if it fails to send updates to the remote shard, while a
    forward proxy does not fail at all when transforming.
    
    We now transfer queued updates before a shard is unproxified. This
    allows for proper error handling. After everything is transferred the
    shard is transformed into a forward proxy which can eventually be safely
    unproxified later.
    
    * Add trace logging for transferring queue proxy updates in batch
    
    * Simplify snapshot method conversion from gRPC
    
    * Remove remote shard parameter
    
    * Add safe guard to queue proxy handler, panic in debug if not finalized
    
    * Improve safety and architecture of queue proxy shard
    
    Switch from an explicit finalized flag to an outer-inner architecture.
    This improves the interface and robustness of the type.
    
    * Do not panic on drop if already unwinding
    
    * Make REST port interface in channel service for local node explicitly
    
    * Recover shard on remote over gRPC, remove reqwest client
    
    * Use shard transfer priority for shard snapshot recovery
    
    * Remove obsolete comment
    
    * Simplify qualified path with use
    
    * Don't construct URLs ourselves as a string, use `parse` and `set_port`
    
    * Use `set_path` when building shard download URL
    
    * Fix error handling in queue to forward proxy transformation
    
    Before, we didn't handle finalization errors properly. If this failed,
    tie shard would be lost.  With this change the queue proxy shard is put
    back.
    
    * Set default shard transfer method to stream records, eliminate panics
    
    * Fix shard snapshot transfer not correctly aborting due to queue proxy
    
    When a shard transfer fails (for any reason), the transfer is aborted.
    If we still have a queue proxy shard it should also be reverted, and
    collected updates should be forgotten. Before this change it would try
    to send all collected updates to the remote, even if the transfer
    failed.
    
    * Review fixes
    
    Co-authored-by: Roman Titov <ffuugoo@users.noreply.github.com>
    
    * Review fixes
    
    Co-authored-by: Roman Titov <ffuugoo@users.noreply.github.com>
    
    * Initiate forward and queue proxy shard in specialized transfer methods
    
    Co-authored-by: Roman Titov <ffuugoo@users.noreply.github.com>
    
    * Add consensus interface to shard transfer, repurpose dispatcher (#2873)
    
    * Add shard transfer consensus interface
    
    * Integrate shard transfer consensus interface into toc and transfer logic
    
    * Repurpose dispatcher for getting consensus into shard transfer
    
    * Derive clone
    
    * Mark consensus as unused for now
    
    * Use custom dispatcher with weak ref to prevent Arc cycle for ToC
    
    * Add comment on why a weak reference is used
    
    * Do exhaustive match in shard unproxy logic
    
    * Restructure match statement, use match if
    
    * When queue proxifying shard, allow forward proxy state if same remote
    
    * Before retrying a shard transfer after error, destruct queue proxy
    
    * Synchronize consensus across all nodes for shard snapshot transfer (#2874)
    
    * Move await consensus commit functions into channel service
    
    * Add shard consensus method to synchronize consensus across all nodes
    
    * Move transfer config, channels and local address into snapshot transfer
    
    * Await other nodes to reach consensus before finalizing shard transfer
    
    * Do not fail right away awaiting consensus if still on older term
    
    Instead, give the node time to reach the same term.
    
    * Fix `await_commit_on_all_peers` not catching peer errors properly
    
    * Change return type of `wait_for_consensus_commit` to `Result`
    
    This is of course more conventional, and automatically sets `must_use`.
    
    * Explicitly note number of peers when awaiting consensus
    
    * Before consensus sync, wait for local shard to reach partial state
    
    * Fix timeout error handling when waiting for replica set state
    
    * Wait for replica set to have remote in partial state instead
    
    * Set `(Partial)Snapshot` states for shard snapshot transfer through consensus (#2881)
    
    * When doing a shard snapshot transfer, set shard to `PartialSnapshot`
    
    * Add shard transfer method to set shard state to partial
    
    It currently uses a naive implementation. Using a custom consensus
    operation to also confirm a transfer is still active will be implemented
    later.
    
    * Add consensus snapshot transfer operation to change shard to partial
    
    The operation `ShardTransferOperations::SnapshotRecovered` is called
    after the shard snapshot is recovered on the remote and it progresses
    the transfer further.
    
    The operation sets the shard state from `PartialSnapshot` to `Partial`
    and ensures the transfer is still active.
    
    * Confirm consensus put shard into partial state, retry 3 times
    
    * Get replica set once
    
    * Add extensive shard snapshot transfer process docs, clean up function
    
    * Fix typo
    
    * Review suggestion
    
    Co-authored-by: Luis Cossío <luis.cossio@qdrant.com>
    
    * Add delay between consensus confirmation retries
    
    * Rename retry timeout to retry delay
    
    ---------
    
    Co-authored-by: Luis Cossío <luis.cossio@qdrant.com>
    
    * On replicate shard, remember specified method
    
    ---------
    
    Co-authored-by: Roman Titov <ffuugoo@users.noreply.github.com>
    Co-authored-by: Luis Cossío <luis.cossio@qdrant.com>

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 51109fe07..6d296c14e 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -297,6 +297,11 @@ impl ShardReplicaSet {
         matches!(*local_read, Some(Shard::Local(_) | Shard::Dummy(_)))
     }
 
+    pub async fn is_queue_proxy(&self) -> bool {
+        let local_read = self.local.read().await;
+        matches!(*local_read, Some(Shard::QueueProxy(_)))
+    }
+
     pub async fn is_dummy(&self) -> bool {
         let local_read = self.local.read().await;
         matches!(*local_read, Some(Shard::Dummy(_)))
@@ -323,30 +328,55 @@ impl ShardReplicaSet {
     /// Wait for a local shard to be initialized.
     ///
     /// Uses a blocking thread internally.
-    ///
-    /// Returns `true` if initialized, `false` if timed out.
-    pub async fn wait_for_local(&self, timeout: Duration) -> CollectionResult<bool> {
+    pub async fn wait_for_local(&self, timeout: Duration) -> CollectionResult<()> {
         self.wait_for(|replica_set_state| replica_set_state.is_local, timeout)
             .await
     }
 
-    /// Wait for a replica set state condition to be true.
+    /// Wait for the replica set to reach the `Partial` state for a peer
     ///
     /// Uses a blocking thread internally.
+    pub async fn wait_for_partial(
+        &self,
+        peer_id: PeerId,
+        timeout: Duration,
+    ) -> CollectionResult<()> {
+        self.wait_for(
+            move |replica_set_state| {
+                matches!(
+                    replica_set_state.get_peer_state(&peer_id),
+                    Some(ReplicaState::Partial)
+                )
+            },
+            timeout,
+        )
+        .await
+    }
+
+    /// Wait for a replica set state condition to be true.
     ///
-    /// Returns `true` if condition is true, `false` if timed out.
-    async fn wait_for<F>(&self, check: F, timeout: Duration) -> CollectionResult<bool>
+    /// Uses a blocking thread internally.
+    async fn wait_for<F>(&self, check: F, timeout: Duration) -> CollectionResult<()>
     where
         F: Fn(&ReplicaSetState) -> bool + Send + 'static,
     {
         let replica_state = self.replica_state.clone();
-        tokio::task::spawn_blocking(move || replica_state.wait_for(check, timeout))
-            .await
-            .map_err(|err| {
-                CollectionError::service_error(format!(
-                    "Failed to wait for replica set state: {err}"
-                ))
-            })
+        let timed_out =
+            !tokio::task::spawn_blocking(move || replica_state.wait_for(check, timeout))
+                .await
+                .map_err(|err| {
+                    CollectionError::service_error(format!(
+                        "Failed to wait for replica set state: {err}"
+                    ))
+                })?;
+
+        if timed_out {
+            return Err(CollectionError::service_error(
+                "Failed to wait for replica set state, timed out",
+            ));
+        }
+
+        Ok(())
     }
 
     pub async fn init_empty_local_shard(&self) -> CollectionResult<()> {

commit b1eb5a2a687ab81236fc34202fcd119d142dcc1e
Author: Tim Visée <tim+github@visee.me>
Date:   Mon Oct 30 13:18:16 2023 +0100

    Improve shard snapshot transfer replica set state synchronization (#2884)
    
    * Add `WaitForShardState` gRPC call definition
    
    * Implement logic for `WaitForShardState` call
    
    * In next shard snapshot transfer stage, wait for remote to reach `Partial`
    
    * In shard snapshot transfer, synchronize consensus as last step
    
    We don't have to synchronize consensus right away. Instead we just
    confirm that the remote shard has reached `Partial` state. Then we
    transform the queue proxy shard into the forward proxy.
    
    Right before we finalize the transfer we do want to synchronize
    consensus. First make sure the shard has reached `Partial` state in our
    local replica set. Then synchronize all other nodes to make sure they
    reach at least the same consensus state.
    
    * Reformat internal collection service definition

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 6d296c14e..4dd244838 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -333,26 +333,39 @@ impl ShardReplicaSet {
             .await
     }
 
-    /// Wait for the replica set to reach the `Partial` state for a peer
+    /// Wait for a local shard to get into `state`
     ///
     /// Uses a blocking thread internally.
-    pub async fn wait_for_partial(
+    pub async fn wait_for_local_state(
         &self,
-        peer_id: PeerId,
+        state: ReplicaState,
         timeout: Duration,
     ) -> CollectionResult<()> {
         self.wait_for(
             move |replica_set_state| {
-                matches!(
-                    replica_set_state.get_peer_state(&peer_id),
-                    Some(ReplicaState::Partial)
-                )
+                replica_set_state.get_peer_state(&replica_set_state.this_peer_id) == Some(&state)
             },
             timeout,
         )
         .await
     }
 
+    /// Wait for a peer shard to get into `state`
+    ///
+    /// Uses a blocking thread internally.
+    pub async fn wait_for_state(
+        &self,
+        peer_id: PeerId,
+        state: ReplicaState,
+        timeout: Duration,
+    ) -> CollectionResult<()> {
+        self.wait_for(
+            move |replica_set_state| replica_set_state.get_peer_state(&peer_id) == Some(&state),
+            timeout,
+        )
+        .await
+    }
+
     /// Wait for a replica set state condition to be true.
     ///
     /// Uses a blocking thread internally.

commit 4c14eae798911fd25c896c6575652c363111352b
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Fri Nov 10 12:18:42 2023 +0100

    Improve abort/cancellation support for shard transfer (#2926)
    
    * Ensure shard snapshot methods and API are cancel safe
    
    * Remove resolved TODOs
    
    * Extend comment on why we don't unproxify queue in a normal way
    
    * fixup! Remove resolved TODOs
    
    * Restructure `spawn_transfer_task`
    
    * Update lib/collection/src/shards/transfer/shard_transfer.rs
    
    Co-authored-by: Roman Titov <ffuugoo@users.noreply.github.com>
    
    * fixup! Restructure `spawn_transfer_task`
    
    Make `spawn_transfer_task` more readable
    
    ---------
    
    Co-authored-by: timvisee <tim@visee.me>
    Co-authored-by: Tim Visée <tim+github@visee.me>

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 4dd244838..4754c241c 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -353,6 +353,10 @@ impl ShardReplicaSet {
     /// Wait for a peer shard to get into `state`
     ///
     /// Uses a blocking thread internally.
+    ///
+    /// # Cancel safety
+    ///
+    /// This method is cancel safe.
     pub async fn wait_for_state(
         &self,
         peer_id: PeerId,
@@ -369,10 +373,16 @@ impl ShardReplicaSet {
     /// Wait for a replica set state condition to be true.
     ///
     /// Uses a blocking thread internally.
+    ///
+    /// # Cancel safety
+    ///
+    /// This method is cancel safe.
     async fn wait_for<F>(&self, check: F, timeout: Duration) -> CollectionResult<()>
     where
         F: Fn(&ReplicaSetState) -> bool + Send + 'static,
     {
+        // TODO: Propagate cancellation into `spawn_blocking` task!?
+
         let replica_state = self.replica_state.clone();
         let timed_out =
             !tokio::task::spawn_blocking(move || replica_state.wait_for(check, timeout))
@@ -444,6 +454,8 @@ impl ShardReplicaSet {
     }
 
     pub async fn remove_local(&self) -> CollectionResult<()> {
+        // TODO: Ensure cancel safety!
+
         self.replica_state.write(|rs| {
             rs.is_local = false;
             let this_peer_id = rs.this_peer_id;

commit f14aa4d6213816737a2e5a6aa36574e28e7da7be
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Tue Nov 21 16:23:04 2023 +0100

    Abort shard transfer during `sync_local_state`, if transfer-sender node failed (#2975, #3012)

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 4754c241c..c03b8a374 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -17,6 +17,7 @@ use tokio::sync::{Mutex, RwLock};
 
 use super::local_shard::LocalShard;
 use super::remote_shard::RemoteShard;
+use super::transfer::ShardTransfer;
 use super::CollectionId;
 use crate::config::CollectionConfig;
 use crate::operations::shared_storage_config::SharedStorageConfig;
@@ -81,6 +82,7 @@ pub struct ShardReplicaSet {
     pub(crate) shard_path: PathBuf,
     pub(crate) shard_id: ShardId,
     notify_peer_failure_cb: ChangePeerState,
+    abort_shard_transfer_cb: AbortShardTransfer,
     channel_service: ChannelService,
     collection_id: CollectionId,
     collection_config: Arc<RwLock<CollectionConfig>>,
@@ -91,6 +93,7 @@ pub struct ShardReplicaSet {
     write_ordering_lock: Mutex<()>,
 }
 
+pub type AbortShardTransfer = Arc<dyn Fn(ShardTransfer, &str) + Send + Sync>;
 pub type ChangePeerState = Arc<dyn Fn(PeerId, ShardId) + Send + Sync>;
 
 const REPLICA_STATE_FILE: &str = "replica_state.json";
@@ -105,6 +108,7 @@ impl ShardReplicaSet {
         local: bool,
         remotes: HashSet<PeerId>,
         on_peer_failure: ChangePeerState,
+        abort_shard_transfer: AbortShardTransfer,
         collection_path: &Path,
         collection_config: Arc<RwLock<CollectionConfig>>,
         shared_storage_config: Arc<SharedStorageConfig>,
@@ -159,6 +163,7 @@ impl ShardReplicaSet {
             replica_state: replica_state.into(),
             locally_disabled_peers: Default::default(),
             shard_path,
+            abort_shard_transfer_cb: abort_shard_transfer,
             notify_peer_failure_cb: on_peer_failure,
             channel_service,
             collection_id,
@@ -184,6 +189,7 @@ impl ShardReplicaSet {
         shared_storage_config: Arc<SharedStorageConfig>,
         channel_service: ChannelService,
         on_peer_failure: ChangePeerState,
+        abort_shard_transfer: AbortShardTransfer,
         this_peer_id: PeerId,
         update_runtime: Handle,
         search_runtime: Handle,
@@ -265,6 +271,7 @@ impl ShardReplicaSet {
             locally_disabled_peers: Default::default(),
             shard_path: shard_path.to_path_buf(),
             notify_peer_failure_cb: on_peer_failure,
+            abort_shard_transfer_cb: abort_shard_transfer,
             channel_service,
             collection_id,
             collection_config,
@@ -655,9 +662,22 @@ impl ShardReplicaSet {
 
     /// Check if the are any locally disabled peers
     /// And if so, report them to the consensus
-    pub async fn sync_local_state(&self) -> CollectionResult<()> {
-        for failed_peer in self.locally_disabled_peers.read().iter() {
-            self.notify_peer_failure(*failed_peer);
+    pub async fn sync_local_state<F>(&self, get_shard_transfers: F) -> CollectionResult<()>
+    where
+        F: Fn(ShardId, PeerId) -> Vec<ShardTransfer>,
+    {
+        for &failed_peer in self.locally_disabled_peers.read().iter() {
+            self.notify_peer_failure(failed_peer);
+
+            for transfer in get_shard_transfers(self.shard_id, failed_peer) {
+                self.abort_shard_transfer(
+                    transfer,
+                    &format!(
+                        "{failed_peer}/{}:{} replica failed",
+                        self.collection_id, self.shard_id
+                    ),
+                );
+            }
         }
         Ok(())
     }
@@ -737,6 +757,18 @@ impl ShardReplicaSet {
         log::debug!("Notify peer failure: {}", peer_id);
         self.notify_peer_failure_cb.deref()(peer_id, self.shard_id)
     }
+
+    fn abort_shard_transfer(&self, transfer: ShardTransfer, reason: &str) {
+        log::debug!(
+            "Abort {}:{} / {} -> {} shard transfer",
+            self.collection_id,
+            transfer.shard_id,
+            transfer.from,
+            transfer.to,
+        );
+
+        self.abort_shard_transfer_cb.deref()(transfer, reason)
+    }
 }
 
 /// Represents a replica set state

commit e085c4dbc12f32715d13afe14f163d4d40f7751c
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Tue Nov 21 17:19:24 2023 +0100

    Add a health check before starting shard transfer during `sync_local_state` (#2977, #3026)

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index c03b8a374..4dfc3db74 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -701,6 +701,20 @@ impl ShardReplicaSet {
         }
     }
 
+    pub(crate) async fn health_check(&self, peer_id: PeerId) -> CollectionResult<()> {
+        let remotes = self.remotes.read().await;
+
+        let Some(remote) = remotes.iter().find(|remote| remote.peer_id == peer_id) else {
+            return Err(CollectionError::NotFound {
+                what: format!("{}/{}:{} shard", peer_id, self.collection_id, self.shard_id),
+            });
+        };
+
+        remote.health_check().await?;
+
+        Ok(())
+    }
+
     fn init_remote_shards(
         shard_id: ShardId,
         collection_id: CollectionId,

commit 0866f1c07dccab2f684c012a1e54084a4e2727f3
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Tue Nov 21 19:45:58 2023 +0100

    Add backoff when notifying peer failure during `sync_local_state` (#2942)

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 4dfc3db74..529db7132 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -1,4 +1,5 @@
 mod execute_read_operation;
+mod locally_disabled_peers;
 mod read_ops;
 mod shard_transfer;
 mod snapshots;
@@ -78,7 +79,7 @@ pub struct ShardReplicaSet {
     /// List is checked on each consensus round and submitted to the consensus.
     /// If the state of the peer is changed in the consensus, it is removed from the list.
     /// Update and read operations are not performed on the peers marked as dead.
-    locally_disabled_peers: parking_lot::RwLock<HashSet<PeerId>>,
+    locally_disabled_peers: parking_lot::RwLock<locally_disabled_peers::Registry>,
     pub(crate) shard_path: PathBuf,
     pub(crate) shard_id: ShardId,
     notify_peer_failure_cb: ChangePeerState,
@@ -285,7 +286,7 @@ impl ShardReplicaSet {
             replica_set
                 .locally_disabled_peers
                 .write()
-                .insert(this_peer_id);
+                .disable_peer(this_peer_id);
         }
 
         replica_set
@@ -662,11 +663,18 @@ impl ShardReplicaSet {
 
     /// Check if the are any locally disabled peers
     /// And if so, report them to the consensus
-    pub async fn sync_local_state<F>(&self, get_shard_transfers: F) -> CollectionResult<()>
+    pub fn sync_local_state<F>(&self, get_shard_transfers: F) -> CollectionResult<()>
     where
         F: Fn(ShardId, PeerId) -> Vec<ShardTransfer>,
     {
-        for &failed_peer in self.locally_disabled_peers.read().iter() {
+        let peers_to_notify: Vec<_> = self
+            .locally_disabled_peers
+            .write()
+            .notify_elapsed()
+            .collect();
+
+        for failed_peer in peers_to_notify {
+            // TODO: Only `notify_peer_failure` if `failed_peer` is *not* the last `Active` peer? 🤔
             self.notify_peer_failure(failed_peer);
 
             for transfer in get_shard_transfers(self.shard_id, failed_peer) {
@@ -679,6 +687,7 @@ impl ShardReplicaSet {
                 );
             }
         }
+
         Ok(())
     }
 
@@ -743,7 +752,17 @@ impl ShardReplicaSet {
     }
 
     fn is_locally_disabled(&self, peer_id: &PeerId) -> bool {
-        self.locally_disabled_peers.read().contains(peer_id)
+        self.locally_disabled_peers.read().is_disabled(*peer_id)
+    }
+
+    fn add_locally_disabled(&self, peer_id: PeerId) {
+        if self
+            .locally_disabled_peers
+            .write()
+            .disable_peer_and_notify_if_elapsed(peer_id)
+        {
+            self.notify_peer_failure(peer_id);
+        }
     }
 
     // Make sure that locally disabled peers do not contradict the consensus
@@ -751,19 +770,21 @@ impl ShardReplicaSet {
         // Check that we are not trying to disable the last active peer
         let peers = self.peers();
 
-        let active_peers: HashSet<_> = peers
-            .iter()
-            .filter(|(_, state)| **state == ReplicaState::Active)
-            .map(|(peer, _)| *peer)
-            .collect();
-
-        let mut locally_disabled = self.locally_disabled_peers.write();
+        let active_peers = peers.iter().filter_map(|(&peer_id, &state)| {
+            if state == ReplicaState::Active {
+                Some(peer_id)
+            } else {
+                None
+            }
+        });
 
-        locally_disabled.remove(&peer_id_to_remove);
+        let mut locally_disabled_peers = self.locally_disabled_peers.write();
 
-        if active_peers.is_subset(&locally_disabled) {
+        if locally_disabled_peers.is_all_disabled(active_peers) {
             log::warn!("Resolving consensus/local state inconsistency");
-            locally_disabled.clear();
+            locally_disabled_peers.clear();
+        } else {
+            locally_disabled_peers.enable_peer(peer_id_to_remove);
         }
     }
 

commit 680165bda7dea4e5df00a5151a03f8ee0b700f47
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Tue Nov 28 15:22:29 2023 +0100

    fix awaiting of the consensus thread to actually await entry to be ap… (#3103)
    
    * fix awaiting of the consensus thread to actually await entry to be applied
    
    * use last_applied_entry

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 529db7132..237bbf446 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -116,6 +116,7 @@ impl ShardReplicaSet {
         channel_service: ChannelService,
         update_runtime: Handle,
         search_runtime: Handle,
+        init_state: Option<ReplicaState>,
     ) -> CollectionResult<Self> {
         let shard_path = super::create_shard_dir(collection_path, shard_id).await?;
         let local = if local {
@@ -134,14 +135,16 @@ impl ShardReplicaSet {
         };
         let replica_state: SaveOnDisk<ReplicaSetState> =
             SaveOnDisk::load_or_init(shard_path.join(REPLICA_STATE_FILE))?;
+
+        let init_replica_state = init_state.unwrap_or(ReplicaState::Initializing);
         replica_state.write(|rs| {
             rs.this_peer_id = this_peer_id;
             if local.is_some() {
                 rs.is_local = true;
-                rs.set_peer_state(this_peer_id, ReplicaState::Initializing);
+                rs.set_peer_state(this_peer_id, init_replica_state);
             }
             for peer in remotes {
-                rs.set_peer_state(peer, ReplicaState::Initializing);
+                rs.set_peer_state(peer, init_replica_state);
             }
         })?;
 

commit ad89655acde1a367bbb0dafe29602b38dfbe2b92
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Tue Dec 5 10:39:26 2023 +0100

    Better shard transfer init (#3156)
    
    * check for shard state before accepting transfer
    
    * fmt
    
    * Add method to replica state to check whether it is partial-like
    
    ---------
    
    Co-authored-by: timvisee <tim@visee.me>

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 237bbf446..5460c1672 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -344,6 +344,14 @@ impl ShardReplicaSet {
             .await
     }
 
+    pub fn wait_for_state_condition_sync<F>(&self, check: F, timeout: Duration) -> bool
+    where
+        F: Fn(&ReplicaSetState) -> bool,
+    {
+        let replica_state = self.replica_state.clone();
+        replica_state.wait_for(check, timeout)
+    }
+
     /// Wait for a local shard to get into `state`
     ///
     /// Uses a blocking thread internally.
@@ -871,6 +879,19 @@ pub enum ReplicaState {
     PartialSnapshot,
 }
 
+impl ReplicaState {
+    /// Check whether the replica state is partial or partial-like.
+    pub fn is_partial_like(self) -> bool {
+        match self {
+            ReplicaState::Partial | ReplicaState::PartialSnapshot => true,
+            ReplicaState::Active
+            | ReplicaState::Dead
+            | ReplicaState::Initializing
+            | ReplicaState::Listener => false,
+        }
+    }
+}
+
 /// Represents a change in replica set, due to scaling of `replication_factor`
 #[derive(Debug, Deserialize, Serialize, PartialEq, Eq, Hash, Clone)]
 pub enum Change {

commit cb46a73609aaa79d2d8bb1b6389778740d3f1935
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Tue Dec 5 17:59:46 2023 +0100

    Extend `/readyz` with shards readiness check (#3053, #3084)
    
    Co-authored-by: generall <andrey@vasnetsov.com>
    Co-authored-by: timvisee <tim@visee.me>

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 5460c1672..65b36d0a9 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -880,10 +880,25 @@ pub enum ReplicaState {
 }
 
 impl ReplicaState {
+    /// Check whether the replica state is active or listener.
+    pub fn is_active_or_listener(self) -> bool {
+        // Use explicit match, to catch future changes to `ReplicaState`
+        match self {
+            ReplicaState::Active | ReplicaState::Listener => true,
+
+            ReplicaState::Dead
+            | ReplicaState::Initializing
+            | ReplicaState::Partial
+            | ReplicaState::PartialSnapshot => false,
+        }
+    }
+
     /// Check whether the replica state is partial or partial-like.
     pub fn is_partial_like(self) -> bool {
+        // Use explicit match, to catch future changes to `ReplicaState`
         match self {
             ReplicaState::Partial | ReplicaState::PartialSnapshot => true,
+
             ReplicaState::Active
             | ReplicaState::Dead
             | ReplicaState::Initializing

commit 19514265330ac9a1049b9439517deb104a5a19ed
Author: Tim Visée <tim+github@visee.me>
Date:   Wed Jan 31 11:56:34 2024 +0100

    Dynamic CPU saturation internals (#3364)
    
    * Move CPU count function to common, fix wrong CPU count in visited list
    
    * Change default number of rayon threads to 8
    
    * Use CPU budget and CPU permits for optimizer tasks to limit utilization
    
    * Respect configured thread limits, use new sane defaults in config
    
    * Fix spelling issues
    
    * Fix test compilation error
    
    * Improve breaking if there is no CPU budget
    
    * Block optimizations until CPU budget, fix potentially getting stuck
    
    Our optimization worker now blocks until CPU budget is available to
    perform the task.
    
    Fix potential issue where optimization worker could get stuck. This
    would happen if no optimization task is started because there's no
    available CPU budget. This ensures the worker is woken up again to
    retry.
    
    * Utilize n-1 CPUs with optimization tasks
    
    * Better handle situations where CPU budget is drained
    
    * Dynamically scale rayon CPU count based on CPU size
    
    * Fix incorrect default for max_indexing_threads conversion
    
    * Respect max_indexing_threads for collection
    
    * Make max_indexing_threads optional, use none to set no limit
    
    * Update property documentation and comments
    
    * Property max_optimization_threads is per shard, not per collection
    
    * If we reached shard optimization limit, skip further checks
    
    * Add remaining TODOs
    
    * Fix spelling mistake
    
    * Align gRPC comment blocks
    
    * Fix compilation errors since last rebase
    
    * Make tests aware of CPU budget
    
    * Use new CPU budget calculation function everywhere
    
    * Make CPU budget configurable in settings, move static budget to common
    
    * Do not use static CPU budget, instance it and pass it through
    
    * Update CPU budget description
    
    * Move heuristic into defaults
    
    * Fix spelling issues
    
    * Move cpu_budget property to a better place
    
    * Move some things around
    
    * Minor review improvements
    
    * Use range match statement for CPU count heuristics
    
    * Systems with 1 or 2 CPUs do not keep cores unallocated by default
    
    * Fix compilation errors since last rebase
    
    * Update lib/segment/src/types.rs
    
    Co-authored-by: Luis Cossío <luis.cossio@qdrant.com>
    
    * Update lib/storage/src/content_manager/toc/transfer.rs
    
    Co-authored-by: Luis Cossío <luis.cossio@qdrant.com>
    
    * Rename cpu_budget to optimizer_cpu_budget
    
    * Update OpenAPI specification
    
    * Require at least half of the desired CPUs for optimizers
    
    This prevents running optimizations with just one CPU, which could be
    very slow.
    
    * Don't use wildcard in CPU heuristic match statements
    
    * Rename cpu_budget setting to optimizer_cpu_budget
    
    * Update CPU budget comments
    
    * Spell acquire correctly
    
    * Change if-else into match
    
    Co-authored-by: Luis Cossío <luis.cossio@qdrant.com>
    
    * Rename max_rayon_threads to num_rayon_threads, add explanation
    
    * Explain limit in update handler
    
    * Remove numbers for automatic selection of indexing threads
    
    * Inline max_workers variable
    
    * Remove CPU budget from ShardTransferConsensus trait, it is in collection
    
    * small allow(dead_code) => cfg(test)
    
    * Remove now obsolete lazy_static
    
    * Fix incorrect CPU calculation in CPU saturation test
    
    * Make waiting for CPU budget async, don't block current thread
    
    * Prevent deadlock on optimizer signal channel
    
    Do not block the optimization worker task anymore to wait for CPU budget
    to be available. That prevents our optimizer signal channel from being
    drained, blocking incoming updates because the cannot send another
    optimizer signal. Now, prevent blocking this task all together and
    retrigger the optimizers separately when CPU budget is available again.
    
    * Fix incorrect CPU calculation in optimization cancel test
    
    * Rename CPU budget wait function to notify
    
    * Detach API changes from CPU saturation internals
    
    This allows us to merge into a patch version of Qdrant. We can
    reintroduce the API changes in the upcoming minor release to make all of
    it fully functional.
    
    ---------
    
    Co-authored-by: Luis Cossío <luis.cossio@qdrant.com>
    Co-authored-by: Luis Cossío <luis.cossio@outlook.com>

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 65b36d0a9..e3cd8369d 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -11,6 +11,7 @@ use std::path::{Path, PathBuf};
 use std::sync::Arc;
 use std::time::Duration;
 
+use common::cpu::CpuBudget;
 use schemars::JsonSchema;
 use serde::{Deserialize, Serialize};
 use tokio::runtime::Handle;
@@ -90,6 +91,7 @@ pub struct ShardReplicaSet {
     shared_storage_config: Arc<SharedStorageConfig>,
     update_runtime: Handle,
     search_runtime: Handle,
+    optimizer_cpu_budget: CpuBudget,
     /// Lock to serialized write operations on the replicaset when a write ordering is used.
     write_ordering_lock: Mutex<()>,
 }
@@ -116,6 +118,7 @@ impl ShardReplicaSet {
         channel_service: ChannelService,
         update_runtime: Handle,
         search_runtime: Handle,
+        optimizer_cpu_budget: CpuBudget,
         init_state: Option<ReplicaState>,
     ) -> CollectionResult<Self> {
         let shard_path = super::create_shard_dir(collection_path, shard_id).await?;
@@ -127,6 +130,7 @@ impl ShardReplicaSet {
                 collection_config.clone(),
                 shared_storage_config.clone(),
                 update_runtime.clone(),
+                optimizer_cpu_budget.clone(),
             )
             .await?;
             Some(Shard::Local(shard))
@@ -175,6 +179,7 @@ impl ShardReplicaSet {
             shared_storage_config,
             update_runtime,
             search_runtime,
+            optimizer_cpu_budget,
             write_ordering_lock: Mutex::new(()),
         })
     }
@@ -197,6 +202,7 @@ impl ShardReplicaSet {
         this_peer_id: PeerId,
         update_runtime: Handle,
         search_runtime: Handle,
+        optimizer_cpu_budget: CpuBudget,
     ) -> Self {
         let replica_state: SaveOnDisk<ReplicaSetState> =
             SaveOnDisk::load_or_init(shard_path.join(REPLICA_STATE_FILE)).unwrap();
@@ -236,6 +242,7 @@ impl ShardReplicaSet {
                     collection_config.clone(),
                     shared_storage_config.clone(),
                     update_runtime.clone(),
+                    optimizer_cpu_budget.clone(),
                 )
                 .await;
 
@@ -282,6 +289,7 @@ impl ShardReplicaSet {
             shared_storage_config,
             update_runtime,
             search_runtime,
+            optimizer_cpu_budget,
             write_ordering_lock: Mutex::new(()),
         };
 
@@ -434,6 +442,7 @@ impl ShardReplicaSet {
             self.collection_config.clone(),
             self.shared_storage_config.clone(),
             self.update_runtime.clone(),
+            self.optimizer_cpu_budget.clone(),
         )
         .await;
 
@@ -614,6 +623,7 @@ impl ShardReplicaSet {
                     self.collection_config.clone(),
                     self.shared_storage_config.clone(),
                     self.update_runtime.clone(),
+                    self.optimizer_cpu_budget.clone(),
                 )
                 .await?;
                 match state {

commit d5e5e36ea33628f0f0a5e394f334e549a673375a
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Wed Feb 7 13:44:53 2024 +0100

    Assign clock tags to internal update operations (and update operation responses) (#3426)
    
    * Add `ClockSet` type to generate clock tags for update operations
    
    * Generate and assign clock tag to update operations
    
    * Advance clock if replica returns *newer* tick for operation
    
    * Cleanup `clock_set` module (and fix a minor bug in `Clock::advance_to`)
    
    * Apply suggestions from code review
    
    * Minor `ClockSet` fixes
    
    * Make `Clock` start from `0` instead of `1`
    
    * Rename clock value to next_tick to prevent ambiguity
    
    The field always holds the next tick value we will be using, and our
    tick functions return first and increment after. This helps making this
    behavior more clear.
    
    * add drops
    
    * Minor `ClockSet` fixes
    
    * Safe-guard `ClockSet` from advancing past clock `0` "prematurely"
    
    * Use `into` instead of explicit `Atomic*::new`
    
    * Remove unnecessary conditional in `ShardReplicaSet::update`
    
    * Update `clock_set::Clock` documentation, rename `Clock::lock` to `Clock::try_lock`
    
    * fixup! Update `clock_set::Clock` documentation, rename `Clock::lock` to `Clock::try_lock`
    
    Fix a typo
    
    ---------
    
    Co-authored-by: Roman Titov <ffuugoo@users.noreply.github.com>
    Co-authored-by: Tim Visée <tim+github@visee.me>
    Co-authored-by: timvisee <tim@visee.me>

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index e3cd8369d..19526b5a9 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -1,3 +1,4 @@
+mod clock_set;
 mod execute_read_operation;
 mod locally_disabled_peers;
 mod read_ops;
@@ -27,6 +28,7 @@ use crate::operations::types::{CollectionError, CollectionResult};
 use crate::save_on_disk::SaveOnDisk;
 use crate::shards::channel_service::ChannelService;
 use crate::shards::dummy_shard::DummyShard;
+use crate::shards::replica_set::clock_set::ClockSet;
 use crate::shards::shard::{PeerId, Shard, ShardId};
 use crate::shards::shard_config::ShardConfig;
 use crate::shards::telemetry::ReplicaSetTelemetry;
@@ -94,6 +96,8 @@ pub struct ShardReplicaSet {
     optimizer_cpu_budget: CpuBudget,
     /// Lock to serialized write operations on the replicaset when a write ordering is used.
     write_ordering_lock: Mutex<()>,
+    /// Local clock set, used to tag new operations on this shard.
+    clock_set: Mutex<ClockSet>,
 }
 
 pub type AbortShardTransfer = Arc<dyn Fn(ShardTransfer, &str) + Send + Sync>;
@@ -181,6 +185,7 @@ impl ShardReplicaSet {
             search_runtime,
             optimizer_cpu_budget,
             write_ordering_lock: Mutex::new(()),
+            clock_set: Default::default(),
         })
     }
 
@@ -291,6 +296,7 @@ impl ShardReplicaSet {
             search_runtime,
             optimizer_cpu_budget,
             write_ordering_lock: Mutex::new(()),
+            clock_set: Default::default(),
         };
 
         if local_load_failure && replica_set.active_remote_shards().await.is_empty() {

commit f08df0b22e28c22c0f1eddeba0343c516c4939a7
Author: Tim Visée <tim+github@visee.me>
Date:   Fri Feb 9 16:57:12 2024 +0100

    Add endpoint to request recovery point for remote shard (#3510)
    
    * Add initial gRPC call for requesting WAL recovery point for remote shard
    
    * Add remote shard method to request WAL recovery point
    
    * Add recovery point type in gRPC, use it in recovery point functions
    
    * Add function to extend recovery point with missing clocks from clock map
    
    * Add new gRPC type for recovery point clocks
    
    * Remove atomic loading, because we use regular integers now

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 19526b5a9..51fd9c407 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -18,6 +18,7 @@ use serde::{Deserialize, Serialize};
 use tokio::runtime::Handle;
 use tokio::sync::{Mutex, RwLock};
 
+use super::local_shard::clock_map::RecoveryPoint;
 use super::local_shard::LocalShard;
 use super::remote_shard::RemoteShard;
 use super::transfer::ShardTransfer;
@@ -831,6 +832,18 @@ impl ShardReplicaSet {
 
         self.abort_shard_transfer_cb.deref()(transfer, reason)
     }
+
+    /// Get shard recovery point for WAL.
+    pub(crate) async fn shard_recovery_point(&self) -> CollectionResult<RecoveryPoint> {
+        let local_shard = self.local.read().await;
+        let Some(local_shard) = local_shard.as_ref() else {
+            return Err(CollectionError::NotFound {
+                what: "Peer does not have local shard".into(),
+            });
+        };
+
+        local_shard.shard_recovery_point().await
+    }
 }
 
 /// Represents a replica set state

commit df18dbbe755a14e717d6656504fef27307eb589d
Author: Tim Visée <tim+github@visee.me>
Date:   Fri Feb 16 16:33:33 2024 +0100

    WAL delta resolution and recovery tests (#3610)
    
    * Fix error when reading WAL backwards with zero items
    
    * WAL delta resolve function should take arc value, not reference
    
    * Reject WAL delta resolution if recovery point defines unknown clocks
    
    * Check if recovery point is empty after adding missed clocks
    
    * Test resolving one operation from WAL
    
    * Test empty recovery point error
    
    * Test recovery point with unknown clocks error
    
    * Test recovery point with higher clocks than current WAL error
    
    * Test recovery point is not in WAL error
    
    * Test recovery point is cut off
    
    * In test, recover WAL with delta
    
    * Move empty WAL construction in test into fixture
    
    * In tests, resolve WAL on both node A and B to check consistency
    
    * WAL resolve function can take local recovery point by reference
    
    * Test WAL delta with many operations, not just one
    
    * Reformat
    
    * Test WAL delta with operations from multiple entrypoints intermixed
    
    * Test WAL delta with operations in a different order on different nodes
    
    * Source clock IDs from clock guard
    
    * In tests, when recovering WAL, update clock map of the recovering node
    
    * Rename WAL delta test functions
    
    * suggestion for tests
    
    * Utilize RecoverableWal in WAL delta tests, advance for clock responses
    
    * an advanced test scenario
    
    * remove mut
    
    * Mock all test operations, test error explicitly
    
    * Fix WAL cutoff test, allow clocks not in cutoff point
    
    * Allow WAL delta resolution for equal states to return None
    
    * Advance highest seen clocks along with updating cutoff point
    
    * Update full transfer delta test with cutoff point, still failing
    
    * Strict WAL delta error checks
    
    * Fix typo
    
    * update tests
    
    * fmt
    
    * Use correct clock set for node D operation in failing test
    
    * Apply suggestions from code review
    
    Co-authored-by: Roman Titov <ffuugoo@users.noreply.github.com>
    
    * Mark recovery point insert function as testing only
    
    ---------
    
    Co-authored-by: generall <andrey@vasnetsov.com>
    Co-authored-by: Roman Titov <ffuugoo@users.noreply.github.com>

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 51fd9c407..26ab05e14 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -1,4 +1,4 @@
-mod clock_set;
+pub(crate) mod clock_set;
 mod execute_read_operation;
 mod locally_disabled_peers;
 mod read_ops;

commit 5feccc5ed16356df34a2dad9dbe826f528984122
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Fri Feb 16 17:56:28 2024 +0100

    Add unit tests for `ClockMap` and `ClockSet`/`ClockMap` workflow (#3572)

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 26ab05e14..dafb33029 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -1,4 +1,4 @@
-pub(crate) mod clock_set;
+pub mod clock_set;
 mod execute_read_operation;
 mod locally_disabled_peers;
 mod read_ops;

commit d39a483017d14971051e30be5023dd4e969163b6
Author: xzfc <5121426+xzfc@users.noreply.github.com>
Date:   Tue Feb 20 14:55:57 2024 +0000

    Refactor: introduce details level enum (#3612)

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index dafb33029..df49499bd 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -13,6 +13,7 @@ use std::sync::Arc;
 use std::time::Duration;
 
 use common::cpu::CpuBudget;
+use common::types::TelemetryDetail;
 use schemars::JsonSchema;
 use serde::{Deserialize, Serialize};
 use tokio::runtime::Handle;
@@ -719,11 +720,11 @@ impl ShardReplicaSet {
         Ok(())
     }
 
-    pub(crate) async fn get_telemetry_data(&self) -> ReplicaSetTelemetry {
+    pub(crate) async fn get_telemetry_data(&self, detail: TelemetryDetail) -> ReplicaSetTelemetry {
         let local_shard = self.local.read().await;
         let local = local_shard
             .as_ref()
-            .map(|local_shard| local_shard.get_telemetry_data());
+            .map(|local_shard| local_shard.get_telemetry_data(detail));
         ReplicaSetTelemetry {
             id: self.shard_id,
             local,
@@ -732,7 +733,7 @@ impl ShardReplicaSet {
                 .read()
                 .await
                 .iter()
-                .map(|remote| remote.get_telemetry_data())
+                .map(|remote| remote.get_telemetry_data(detail))
                 .collect(),
             replicate_states: self.replica_state.read().peers(),
         }

commit 80681aa2209eda80c9df7273f715410d53b0e407
Author: Tim Visée <tim+github@visee.me>
Date:   Thu Feb 22 13:09:39 2024 +0100

    Add replica state: Recovery (#3659)
    
    * Add recovery shard replica set state
    
    * Accept forced operations in partial snapshot state
    
    * Fix switching into wrong state
    
    * Add some helpful comments
    
    * Deduplication in match statement

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index df49499bd..2f0f462af 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -635,34 +635,18 @@ impl ShardReplicaSet {
                 )
                 .await?;
                 match state {
-                    ReplicaState::Active => {
+                    ReplicaState::Active | ReplicaState::Listener => {
                         // No way we can provide up-to-date replica right away at this point,
                         // so we report a failure to consensus
-                        self.set_local(local_shard, Some(ReplicaState::Active))
-                            .await?;
+                        self.set_local(local_shard, Some(state)).await?;
                         self.notify_peer_failure(peer_id);
                     }
-                    ReplicaState::Dead => {
-                        self.set_local(local_shard, Some(ReplicaState::Dead))
-                            .await?;
-                    }
-                    ReplicaState::Partial => {
-                        self.set_local(local_shard, Some(ReplicaState::Partial))
-                            .await?;
-                    }
-                    ReplicaState::Initializing => {
-                        self.set_local(local_shard, Some(ReplicaState::Initializing))
-                            .await?;
-                    }
-                    ReplicaState::Listener => {
-                        // Same as `Active`, we report a failure to consensus
-                        self.set_local(local_shard, Some(ReplicaState::Listener))
-                            .await?;
-                        self.notify_peer_failure(peer_id);
-                    }
-                    ReplicaState::PartialSnapshot => {
-                        self.set_local(local_shard, Some(ReplicaState::PartialSnapshot))
-                            .await?;
+                    ReplicaState::Dead
+                    | ReplicaState::Partial
+                    | ReplicaState::Initializing
+                    | ReplicaState::PartialSnapshot
+                    | ReplicaState::Recovery => {
+                        self.set_local(local_shard, Some(state)).await?;
                     }
                 }
                 continue;
@@ -906,7 +890,12 @@ pub enum ReplicaState {
     // Useful for backup shards
     Listener,
     // Snapshot shard transfer is in progress, updates aren't sent to the shard
+    // Normally rejects updates. Since 1.8 it allows updates if force is true.
+    // TODO(1.9): deprecate this state
     PartialSnapshot,
+    // Shard is undergoing recovery by an external node
+    // Normally rejects updates, accepts updates if force is true
+    Recovery,
 }
 
 impl ReplicaState {
@@ -919,15 +908,16 @@ impl ReplicaState {
             ReplicaState::Dead
             | ReplicaState::Initializing
             | ReplicaState::Partial
-            | ReplicaState::PartialSnapshot => false,
+            | ReplicaState::PartialSnapshot
+            | ReplicaState::Recovery => false,
         }
     }
 
     /// Check whether the replica state is partial or partial-like.
-    pub fn is_partial_like(self) -> bool {
+    pub fn is_partial_or_recovery(self) -> bool {
         // Use explicit match, to catch future changes to `ReplicaState`
         match self {
-            ReplicaState::Partial | ReplicaState::PartialSnapshot => true,
+            ReplicaState::Partial | ReplicaState::PartialSnapshot | ReplicaState::Recovery => true,
 
             ReplicaState::Active
             | ReplicaState::Dead

commit bb8dcb15c8ef694d86a68df718b850f8a65ec8aa
Author: Tim Visée <tim+github@visee.me>
Date:   Thu Feb 22 13:29:09 2024 +0100

    Add gRPC API to set shard cutoff point (#3661)
    
    * Add functions to propagate updating cutoff point from collection level
    
    * Add gRPC endpoint to set cutoff point
    
    * Lock highest and cutoff clock maps separately

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 2f0f462af..2544ff5c0 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -829,6 +829,21 @@ impl ShardReplicaSet {
 
         local_shard.shard_recovery_point().await
     }
+
+    /// Update the cutoff point for the local shard.
+    pub(crate) async fn update_shard_cutoff_point(
+        &self,
+        cutoff: &RecoveryPoint,
+    ) -> CollectionResult<()> {
+        let local_shard = self.local.read().await;
+        let Some(local_shard) = local_shard.as_ref() else {
+            return Err(CollectionError::NotFound {
+                what: "Peer does not have local shard".into(),
+            });
+        };
+
+        local_shard.update_cutoff(cutoff).await
+    }
 }
 
 /// Represents a replica set state

commit 37fdc56729362393074d8c431a4d36c1839f7593
Author: Alexandru Cihodaru <40807189+AlexandruCihodaru@users.noreply.github.com>
Date:   Wed Feb 28 21:04:12 2024 +0200

    Snapshot store (#3643)
    
    * [snapshots]: Define trait for snapshots managemet
    
    * Define a trait with methods for managing snapshots after their
      creation. The goal of this trait is to allow implementation of
      snapshot management logic for different storage backends without major
      changes in the code.
    
    Signed-off-by: Alexandru Cihodaru <alexandru.cihodaru@gmail.com>
    
    * [snapshots]: Move snapshot deletion logic
    
    * Move the snapshot deletion logic into the implementation of
      SnapshotStorage for LocalFileSystemConfig.
    * Replace snapshot deletion logic with calls to the SnapshotStorage
      implementation.
    
    Signed-off-by: Alexandru Cihodaru <alexandru.cihodaru@gmail.com>
    
    * [snapshots]: Move snapshot list logic
    
    * Move the snapshot listing logic into the implementation of
      SnapshotStorage for LocalFileSystemConfig.
    
    Signed-off-by: Alexandru Cihodaru <alexandru.cihodaru@gmail.com>
    
    * [snapshots]: Move snapshot creation logic
    
    * Move the snapshot creation logic into the implementation of
      SnapshotStorage for LocalFileSystemConfig.
    
    Signed-off-by: Alexandru Cihodaru <alexandru.cihodaru@gmail.com>
    
    * review fixes
    
    * fmt
    
    * refactor full storage snapshot
    
    * create directory on get_stored_file + make sure temp archive file is removed
    
    ---------
    
    Signed-off-by: Alexandru Cihodaru <alexandru.cihodaru@gmail.com>
    Co-authored-by: generall <andrey@vasnetsov.com>

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 2544ff5c0..2eb02b134 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -24,6 +24,7 @@ use super::local_shard::LocalShard;
 use super::remote_shard::RemoteShard;
 use super::transfer::ShardTransfer;
 use super::CollectionId;
+use crate::common::snapshots_manager::SnapshotStorageManager;
 use crate::config::CollectionConfig;
 use crate::operations::shared_storage_config::SharedStorageConfig;
 use crate::operations::types::{CollectionError, CollectionResult};
@@ -92,7 +93,7 @@ pub struct ShardReplicaSet {
     channel_service: ChannelService,
     collection_id: CollectionId,
     collection_config: Arc<RwLock<CollectionConfig>>,
-    shared_storage_config: Arc<SharedStorageConfig>,
+    pub(crate) shared_storage_config: Arc<SharedStorageConfig>,
     update_runtime: Handle,
     search_runtime: Handle,
     optimizer_cpu_budget: CpuBudget,
@@ -844,6 +845,10 @@ impl ShardReplicaSet {
 
         local_shard.update_cutoff(cutoff).await
     }
+
+    pub(crate) fn get_snapshots_storage_manager(&self) -> SnapshotStorageManager {
+        SnapshotStorageManager::new(self.shared_storage_config.s3_config.clone())
+    }
 }
 
 /// Represents a replica set state

commit caa03125ed5b361ba47a8d2350f2053ca5b49c5d
Author: Tim Visée <tim+github@visee.me>
Date:   Tue Mar 19 14:50:13 2024 +0100

    Switch to Recovery state, deprecated PartialSnapshot, for Qdrant 1.9 (#3847)
    
    * Deprecate PartialSnapshot replica set state
    
    * Improve logging
    
    * Update gRPC docs
    
    * Remove now obsolete migration TODO

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 2eb02b134..307d688f0 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -909,9 +909,11 @@ pub enum ReplicaState {
     // A shard which receives data, but is not used for search
     // Useful for backup shards
     Listener,
+    // Deprecated since Qdrant 1.9.0, used in Qdrant 1.7.0 and 1.8.0
+    //
     // Snapshot shard transfer is in progress, updates aren't sent to the shard
     // Normally rejects updates. Since 1.8 it allows updates if force is true.
-    // TODO(1.9): deprecate this state
+    // TODO(1.10): remove PartialSnapshot state entirely?
     PartialSnapshot,
     // Shard is undergoing recovery by an external node
     // Normally rejects updates, accepts updates if force is true

commit 9eebf045e78c4fa766f4b473f6e7394ab94f7fb2
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Fri Apr 12 10:05:06 2024 +0200

    test of stream shard transfer consistency (#3977)
    
    * test of stream shard transfer consistency
    
    * ensure that remote shard object is created when shard transfer created
    
    * fmt
    
    * special error type for handelling creation of partial shard
    
    * ensure we dont try to create remote shard to itself

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 307d688f0..eefd1a7ed 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -515,6 +515,8 @@ impl ShardReplicaSet {
     }
 
     pub async fn add_remote(&self, peer_id: PeerId, state: ReplicaState) -> CollectionResult<()> {
+        debug_assert!(peer_id != self.this_peer_id());
+
         self.replica_state.write(|rs| {
             rs.set_peer_state(peer_id, state);
         })?;
@@ -557,7 +559,7 @@ impl ShardReplicaSet {
         peer_id: &PeerId,
         state: ReplicaState,
     ) -> CollectionResult<()> {
-        if *peer_id == self.replica_state.read().this_peer_id {
+        if *peer_id == self.this_peer_id() {
             self.set_replica_state(peer_id, state)?;
         } else {
             // Create remote shard if necessary

commit 854d54ec62fef35a5cc8420c8e89b17f58a34588
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Tue Apr 30 09:22:29 2024 +0200

    Proactively fix lints for Clippy 1.78 (#4140)

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index eefd1a7ed..9de63c5aa 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -617,7 +617,7 @@ impl ShardReplicaSet {
         }
 
         for (peer_id, state) in replicas {
-            let peer_already_exists = old_peers.get(&peer_id).is_some();
+            let peer_already_exists = old_peers.contains_key(&peer_id);
 
             if peer_already_exists {
                 // do nothing

commit 22c3ee286e9390cff9d060f9158c9751f0b64bd4
Author: Kenshin Tanaka <70839560+kemkemG0@users.noreply.github.com>
Date:   Fri May 10 23:54:23 2024 +0900

    Implement S3 snapshot manager (#4150)
    
    * Add SnapshotsStorageConfig enum(Local or S3) and deserialize implementation
    
    * [refactor]  use snapshots_config instead of s3_config
    
    * update config
    
    * add AWS official`aws-sdk-s3`
    
    * implement store_file() WITHOUT error handling
    
    * implement list_snapshots
    
    * implement delete_snapshot
    
    * run `cargo +nightly fmt`
    
    * delete println
    
    * implement get_stored_file
    
    * Add error handlings
    
    * Refactor AWS S3 configuration and error handling
    
    * fix bugs
    
    * create an empty test file
    
    * fix `alias_test.rs` for StorageConfig type
    
    * tempolary delete some test and try s3 test
    
    * Update integration-tests.yml to use snap instead of apt-get for installing yq
    
    * Update integration-tests.yml to use sudo when installing yq
    
    * add sudo
    
    * make (full/non-full) snapshots downloadable
    
    * debug
    
    * small fix
    
    * Add S3 endpoint URL configuration option
    
    * fix
    
    * fix
    
    * debug
    
    * fix endpoint
    
    * update to http://127.0.0.1:9000/
    
    * update
    
    * fix
    
    * fix `#[get("/collections/{collection}/shards/{shard}/snapshots/{snapshot}")]` for s3
    
    * put original tests back
    
    * refactor
    
    * small fix (delete println & echo)
    
    * use object_store and refactor
    
    * create snapshot_storage_ops and implement
    
    * Refactor get_appropriate_chunk_size function to adjust chunk size based on service limits and file size
    
    * cargo +nightly fmt --all
    
    * make it more abstract
    
    * Refactor SnapshotsStorageConfig deserialization in SnapShotsConfig
    
    * small update
    
    * small fix
    
    * Update dependencies in Cargo.lock
    
    * Update minio image to satantime/minio-server
    
    * Refactor snapshot storage paths in snapshots_manager.rs and snapshot_storage_ops.rs
    
    * Fix issue with downloaded file size not matching expected size in download_snapshot function
    
    * add flush
    
    * Use Streaming instead of donloading once
    
    * apply `cargo +nightly fmt --all`
    
    * Fix issue with opening file in SnapshotStream::LocalFS variant
    
    * Fix error handling in SnapshotStream::LocalFS variant
    
    * Add integration test for Shard Snapshot API with S3 storage (#7)

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 9de63c5aa..a35f58c13 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -848,8 +848,8 @@ impl ShardReplicaSet {
         local_shard.update_cutoff(cutoff).await
     }
 
-    pub(crate) fn get_snapshots_storage_manager(&self) -> SnapshotStorageManager {
-        SnapshotStorageManager::new(self.shared_storage_config.s3_config.clone())
+    pub(crate) fn get_snapshots_storage_manager(&self) -> CollectionResult<SnapshotStorageManager> {
+        SnapshotStorageManager::new(self.shared_storage_config.snapshots_config.clone())
     }
 }
 

commit 89b4be0241e6240890156c0cec149408f88f7f17
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Mon May 13 13:31:03 2024 +0200

    Add basic resharding types (#4216)

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index a35f58c13..4b7abb495 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -644,11 +644,13 @@ impl ShardReplicaSet {
                         self.set_local(local_shard, Some(state)).await?;
                         self.notify_peer_failure(peer_id);
                     }
+
                     ReplicaState::Dead
                     | ReplicaState::Partial
                     | ReplicaState::Initializing
                     | ReplicaState::PartialSnapshot
-                    | ReplicaState::Recovery => {
+                    | ReplicaState::Recovery
+                    | ReplicaState::Resharding => {
                         self.set_local(local_shard, Some(state)).await?;
                     }
                 }
@@ -920,6 +922,9 @@ pub enum ReplicaState {
     // Shard is undergoing recovery by an external node
     // Normally rejects updates, accepts updates if force is true
     Recovery,
+    // Points are being migrated to this shard as part of resharding
+    #[schemars(skip)]
+    Resharding,
 }
 
 impl ReplicaState {
@@ -933,7 +938,8 @@ impl ReplicaState {
             | ReplicaState::Initializing
             | ReplicaState::Partial
             | ReplicaState::PartialSnapshot
-            | ReplicaState::Recovery => false,
+            | ReplicaState::Recovery
+            | ReplicaState::Resharding => false,
         }
     }
 
@@ -941,7 +947,10 @@ impl ReplicaState {
     pub fn is_partial_or_recovery(self) -> bool {
         // Use explicit match, to catch future changes to `ReplicaState`
         match self {
-            ReplicaState::Partial | ReplicaState::PartialSnapshot | ReplicaState::Recovery => true,
+            ReplicaState::Partial
+            | ReplicaState::PartialSnapshot
+            | ReplicaState::Recovery
+            | ReplicaState::Resharding => true,
 
             ReplicaState::Active
             | ReplicaState::Dead

commit 504ee0f630e9e768c1e605e04c893082bd4c18c7
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Thu May 30 16:30:39 2024 +0200

    Send updates to the new shard during resharding (#4256)

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 4b7abb495..a0948c70b 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -769,6 +769,17 @@ impl ShardReplicaSet {
         self.peer_state(peer_id) == Some(ReplicaState::Active) && !self.is_locally_disabled(peer_id)
     }
 
+    fn peer_is_active_or_resharding(&self, peer_id: &PeerId) -> bool {
+        let is_active_or_resharding = matches!(
+            self.peer_state(peer_id),
+            Some(ReplicaState::Active | ReplicaState::Resharding)
+        );
+
+        let is_locally_disabled = self.is_locally_disabled(peer_id);
+
+        is_active_or_resharding && !is_locally_disabled
+    }
+
     fn is_locally_disabled(&self, peer_id: &PeerId) -> bool {
         self.locally_disabled_peers.read().is_disabled(*peer_id)
     }

commit f7a7e6ff128b7134e0cdf9804494981a7880ccee
Author: Jojii <15957865+JojiiOfficial@users.noreply.github.com>
Date:   Fri May 31 09:02:44 2024 +0200

    Add optimizer_overwrite config option (#4317)
    
    * add config option
    
    * add optimizers_config to optimizer calls
    
    * also add for tests
    
    * add to build_optimizers
    
    * rename function parameter

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index a0948c70b..de98326f4 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -28,6 +28,7 @@ use crate::common::snapshots_manager::SnapshotStorageManager;
 use crate::config::CollectionConfig;
 use crate::operations::shared_storage_config::SharedStorageConfig;
 use crate::operations::types::{CollectionError, CollectionResult};
+use crate::optimizers_builder::OptimizersConfig;
 use crate::save_on_disk::SaveOnDisk;
 use crate::shards::channel_service::ChannelService;
 use crate::shards::dummy_shard::DummyShard;
@@ -93,6 +94,7 @@ pub struct ShardReplicaSet {
     channel_service: ChannelService,
     collection_id: CollectionId,
     collection_config: Arc<RwLock<CollectionConfig>>,
+    optimizers_config: OptimizersConfig,
     pub(crate) shared_storage_config: Arc<SharedStorageConfig>,
     update_runtime: Handle,
     search_runtime: Handle,
@@ -121,6 +123,7 @@ impl ShardReplicaSet {
         abort_shard_transfer: AbortShardTransfer,
         collection_path: &Path,
         collection_config: Arc<RwLock<CollectionConfig>>,
+        effective_optimizers_config: OptimizersConfig,
         shared_storage_config: Arc<SharedStorageConfig>,
         channel_service: ChannelService,
         update_runtime: Handle,
@@ -138,6 +141,7 @@ impl ShardReplicaSet {
                 shared_storage_config.clone(),
                 update_runtime.clone(),
                 optimizer_cpu_budget.clone(),
+                effective_optimizers_config.clone(),
             )
             .await?;
             Some(Shard::Local(shard))
@@ -183,6 +187,7 @@ impl ShardReplicaSet {
             channel_service,
             collection_id,
             collection_config,
+            optimizers_config: effective_optimizers_config,
             shared_storage_config,
             update_runtime,
             search_runtime,
@@ -203,6 +208,7 @@ impl ShardReplicaSet {
         collection_id: CollectionId,
         shard_path: &Path,
         collection_config: Arc<RwLock<CollectionConfig>>,
+        effective_optimizers_config: OptimizersConfig,
         shared_storage_config: Arc<SharedStorageConfig>,
         channel_service: ChannelService,
         on_peer_failure: ChangePeerState,
@@ -248,6 +254,7 @@ impl ShardReplicaSet {
                     collection_id.clone(),
                     shard_path,
                     collection_config.clone(),
+                    effective_optimizers_config.clone(),
                     shared_storage_config.clone(),
                     update_runtime.clone(),
                     optimizer_cpu_budget.clone(),
@@ -294,6 +301,7 @@ impl ShardReplicaSet {
             channel_service,
             collection_id,
             collection_config,
+            optimizers_config: effective_optimizers_config,
             shared_storage_config,
             update_runtime,
             search_runtime,
@@ -452,6 +460,7 @@ impl ShardReplicaSet {
             self.shared_storage_config.clone(),
             self.update_runtime.clone(),
             self.optimizer_cpu_budget.clone(),
+            self.optimizers_config.clone(),
         )
         .await;
 
@@ -635,6 +644,7 @@ impl ShardReplicaSet {
                     self.shared_storage_config.clone(),
                     self.update_runtime.clone(),
                     self.optimizer_cpu_budget.clone(),
+                    self.optimizers_config.clone(),
                 )
                 .await?;
                 match state {

commit 9f55338104b138ea642d8ef21037273ff2d2d239
Author: Tim Visée <tim+github@visee.me>
Date:   Tue Jun 4 11:35:28 2024 +0200

    Persist state in resharding driver (#4379)
    
    * Allow SaveOnDisk to initialize with non-default state
    
    * Persist resharding driver state

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index de98326f4..19fa3ee50 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -149,7 +149,7 @@ impl ShardReplicaSet {
             None
         };
         let replica_state: SaveOnDisk<ReplicaSetState> =
-            SaveOnDisk::load_or_init(shard_path.join(REPLICA_STATE_FILE))?;
+            SaveOnDisk::load_or_init_default(shard_path.join(REPLICA_STATE_FILE))?;
 
         let init_replica_state = init_state.unwrap_or(ReplicaState::Initializing);
         replica_state.write(|rs| {
@@ -219,7 +219,7 @@ impl ShardReplicaSet {
         optimizer_cpu_budget: CpuBudget,
     ) -> Self {
         let replica_state: SaveOnDisk<ReplicaSetState> =
-            SaveOnDisk::load_or_init(shard_path.join(REPLICA_STATE_FILE)).unwrap();
+            SaveOnDisk::load_or_init_default(shard_path.join(REPLICA_STATE_FILE)).unwrap();
 
         if replica_state.read().this_peer_id != this_peer_id {
             replica_state

commit 7a3f489655fecd58c78c1f61b21990f39a4c7683
Author: Tim Visée <tim+github@visee.me>
Date:   Tue Jun 18 17:51:13 2024 +0200

    Properly integrate replica set state switching in resharding transfers (#4480)
    
    * Add serde/validate attributes to resharding operations, matching others
    
    * Fix broken comment
    
    * Add debug message for resharding driver entering stages
    
    * Fix shard transfer start setting state of wrong replica for resharding
    
    * Remove obsolete clones
    
    * Add target shard ID to shard key, add relevant gRPC types
    
    * Move target shard ID below source shard ID field
    
    * Rename collection_name to collection_id
    
    * Reformat
    
    * Transferring point batches must merge points in case of resharding
    
    * In resharding state, sync list of peers on start
    
    * Add logic for setting replica set state through consensus dispatcher
    
    * Properly start resharding transfer
    
    * Properly finish resharding transfers, set shard state correctly
    
    * Fix shard transfer initialisation with different target shard
    
    * Fix shard state handling with resharding on all nodes on transfer start
    
    * Don't reset locally disabled state if only existing shard is resharding
    
    * Add important TODOs
    
    * Update OpenAPI and gRPC specification
    
    * Elaborate on some logic in code with comments
    
    * Use user configured shard transfer method for replication
    
    * Add debug assert, on transfer start we should not replace existing shard
    
    * On transfer start, be aware of different sender/receiver local states
    
    This fixes transfers where we might not have a replica on all nodes
    
    * Fix shard transfer not setting cutoff point on target shard
    
    * While resharding, migrate shards in numerical order
    
    * On shard transfer initialisation, ensure transfer targets given shard

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 19fa3ee50..390d93b86 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -809,17 +809,17 @@ impl ShardReplicaSet {
         // Check that we are not trying to disable the last active peer
         let peers = self.peers();
 
-        let active_peers = peers.iter().filter_map(|(&peer_id, &state)| {
-            if state == ReplicaState::Active {
-                Some(peer_id)
-            } else {
-                None
-            }
+        // List the peers in active or resharding state
+        // If resharding, there will be only one shard in resharding state and we should not
+        // consider all to be dead
+        // TODO(resharding): accept resharding state as active like below?
+        let active_or_resharding_peers = peers.iter().filter_map(|(&peer_id, &state)| {
+            matches!(state, ReplicaState::Active | ReplicaState::Resharding).then_some(peer_id)
         });
 
         let mut locally_disabled_peers = self.locally_disabled_peers.write();
 
-        if locally_disabled_peers.is_all_disabled(active_peers) {
+        if locally_disabled_peers.is_all_disabled(active_or_resharding_peers) {
             log::warn!("Resolving consensus/local state inconsistency");
             locally_disabled_peers.clear();
         } else {

commit 32c6c88b9d1bfbf1974081b6d6013bace7c33e6c
Author: Tim Visée <tim+github@visee.me>
Date:   Wed Jun 19 14:11:39 2024 +0200

    While resharding, support migrating points on a node locally (#4491)
    
    * Support resharding point migration transfers within the same node
    
    * Clean up branching in point migration logic
    
    * Extend resharding stage debug messages, report collection and shard
    
    * Initialize resharding driver state with peers right away
    
    * Remove some TODOs

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 390d93b86..9e37cb582 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -351,6 +351,17 @@ impl ShardReplicaSet {
         self.replica_state.read().get_peer_state(peer_id).copied()
     }
 
+    /// List the peer IDs on which this shard is active, both the local and remote peers.
+    pub async fn active_shards(&self) -> Vec<PeerId> {
+        let replica_state = self.replica_state.read();
+        replica_state
+            .active_peers()
+            .into_iter()
+            .filter(|peer_id| !self.is_locally_disabled(peer_id))
+            .collect()
+    }
+
+    /// List the remote peer IDs on which this shard is active, excludes the local peer ID.
     pub async fn active_remote_shards(&self) -> Vec<PeerId> {
         let replica_state = self.replica_state.read();
         let this_peer_id = replica_state.this_peer_id;

commit 63b2801e4fe25fea190e5a4069d6a1d3702a4661
Author: Tim Visée <tim+github@visee.me>
Date:   Fri Jun 21 20:01:05 2024 +0200

    Fix new appendable segments not having payload indices (#4523)
    
    * Propagate payload index schema down to shard replica set + update handler
    
    * Configure payload indices when creating new appendable segment
    
    * When loading segments, make sure applied payload indices match config
    
    * Add test to assert creating new segments with payload index
    
    * Fix unit test because the collection payload schema wasn't updated
    
    * Add test for updating payload index configuration on segment load
    
    * Update test documentation
    
    * Also create payload indices in temporary snapshot segment
    
    * do not delete extra payload index from segments
    
    * do not delete extra payload index from segments
    
    * fix test
    
    ---------
    
    Co-authored-by: generall <andrey@vasnetsov.com>

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 9e37cb582..025376b5e 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -24,6 +24,7 @@ use super::local_shard::LocalShard;
 use super::remote_shard::RemoteShard;
 use super::transfer::ShardTransfer;
 use super::CollectionId;
+use crate::collection::payload_index_schema::PayloadIndexSchema;
 use crate::common::snapshots_manager::SnapshotStorageManager;
 use crate::config::CollectionConfig;
 use crate::operations::shared_storage_config::SharedStorageConfig;
@@ -96,6 +97,7 @@ pub struct ShardReplicaSet {
     collection_config: Arc<RwLock<CollectionConfig>>,
     optimizers_config: OptimizersConfig,
     pub(crate) shared_storage_config: Arc<SharedStorageConfig>,
+    payload_index_schema: Arc<SaveOnDisk<PayloadIndexSchema>>,
     update_runtime: Handle,
     search_runtime: Handle,
     optimizer_cpu_budget: CpuBudget,
@@ -125,6 +127,7 @@ impl ShardReplicaSet {
         collection_config: Arc<RwLock<CollectionConfig>>,
         effective_optimizers_config: OptimizersConfig,
         shared_storage_config: Arc<SharedStorageConfig>,
+        payload_index_schema: Arc<SaveOnDisk<PayloadIndexSchema>>,
         channel_service: ChannelService,
         update_runtime: Handle,
         search_runtime: Handle,
@@ -139,6 +142,7 @@ impl ShardReplicaSet {
                 &shard_path,
                 collection_config.clone(),
                 shared_storage_config.clone(),
+                payload_index_schema.clone(),
                 update_runtime.clone(),
                 optimizer_cpu_budget.clone(),
                 effective_optimizers_config.clone(),
@@ -189,6 +193,7 @@ impl ShardReplicaSet {
             collection_config,
             optimizers_config: effective_optimizers_config,
             shared_storage_config,
+            payload_index_schema,
             update_runtime,
             search_runtime,
             optimizer_cpu_budget,
@@ -210,6 +215,7 @@ impl ShardReplicaSet {
         collection_config: Arc<RwLock<CollectionConfig>>,
         effective_optimizers_config: OptimizersConfig,
         shared_storage_config: Arc<SharedStorageConfig>,
+        payload_index_schema: Arc<SaveOnDisk<PayloadIndexSchema>>,
         channel_service: ChannelService,
         on_peer_failure: ChangePeerState,
         abort_shard_transfer: AbortShardTransfer,
@@ -256,6 +262,7 @@ impl ShardReplicaSet {
                     collection_config.clone(),
                     effective_optimizers_config.clone(),
                     shared_storage_config.clone(),
+                    payload_index_schema.clone(),
                     update_runtime.clone(),
                     optimizer_cpu_budget.clone(),
                 )
@@ -303,6 +310,7 @@ impl ShardReplicaSet {
             collection_config,
             optimizers_config: effective_optimizers_config,
             shared_storage_config,
+            payload_index_schema,
             update_runtime,
             search_runtime,
             optimizer_cpu_budget,
@@ -469,6 +477,7 @@ impl ShardReplicaSet {
             &self.shard_path,
             self.collection_config.clone(),
             self.shared_storage_config.clone(),
+            self.payload_index_schema.clone(),
             self.update_runtime.clone(),
             self.optimizer_cpu_budget.clone(),
             self.optimizers_config.clone(),
@@ -653,6 +662,7 @@ impl ShardReplicaSet {
                     &self.shard_path,
                     self.collection_config.clone(),
                     self.shared_storage_config.clone(),
+                    self.payload_index_schema.clone(),
                     self.update_runtime.clone(),
                     self.optimizer_cpu_budget.clone(),
                     self.optimizers_config.clone(),

commit 7a3eff701a5e337f056946993ef725b16587ee44
Author: Tim Visée <tim+github@visee.me>
Date:   Wed Jul 10 10:23:05 2024 +0200

    Don't disable last peer locally (#4631)
    
    * Don't disable last peer locally
    
    * fix typo
    
    * Prevent double read, pass replica set state into locally disable
    
    * suggestion to use upgradable lock to prevent races between checking condition and upgrade
    
    ---------
    
    Co-authored-by: generall <andrey@vasnetsov.com>

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 025376b5e..50ceb6689 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -815,32 +815,43 @@ impl ShardReplicaSet {
         self.locally_disabled_peers.read().is_disabled(*peer_id)
     }
 
-    fn add_locally_disabled(&self, peer_id: PeerId) {
-        if self
-            .locally_disabled_peers
-            .write()
-            .disable_peer_and_notify_if_elapsed(peer_id)
+    /// Locally disable given peer
+    ///
+    /// Disables the peer and notifies consensus periodically.
+    ///
+    /// Prevents disabling the last peer (according to consensus).
+    fn add_locally_disabled(&self, state: &ReplicaSetState, peer_id: PeerId) {
+        let other_peers = state
+            .active_or_resharding_peers()
+            .filter(|id| id != &peer_id);
+
+        let mut locally_disabled_peers_guard = self.locally_disabled_peers.upgradable_read();
+
+        // Prevent disabling last peer in consensus
         {
-            self.notify_peer_failure(peer_id);
+            if !locally_disabled_peers_guard.is_disabled(peer_id)
+                && locally_disabled_peers_guard.is_all_disabled(other_peers)
+            {
+                log::warn!("Cannot locally disable last active peer {peer_id} for replica");
+                return;
+            }
         }
-    }
 
-    // Make sure that locally disabled peers do not contradict the consensus
-    fn update_locally_disabled(&self, peer_id_to_remove: PeerId) {
-        // Check that we are not trying to disable the last active peer
-        let peers = self.peers();
-
-        // List the peers in active or resharding state
-        // If resharding, there will be only one shard in resharding state and we should not
-        // consider all to be dead
-        // TODO(resharding): accept resharding state as active like below?
-        let active_or_resharding_peers = peers.iter().filter_map(|(&peer_id, &state)| {
-            matches!(state, ReplicaState::Active | ReplicaState::Resharding).then_some(peer_id)
+        locally_disabled_peers_guard.with_upgraded(|locally_disabled_peers| {
+            if locally_disabled_peers.disable_peer_and_notify_if_elapsed(peer_id) {
+                self.notify_peer_failure(peer_id);
+            }
         });
+    }
 
+    /// Make sure that locally disabled peers do not contradict the consensus
+    fn update_locally_disabled(&self, peer_id_to_remove: PeerId) {
         let mut locally_disabled_peers = self.locally_disabled_peers.write();
 
-        if locally_disabled_peers.is_all_disabled(active_or_resharding_peers) {
+        // Check that we are not trying to disable the last active peer
+        if locally_disabled_peers
+            .is_all_disabled(self.replica_state.read().active_or_resharding_peers())
+        {
             log::warn!("Resolving consensus/local state inconsistency");
             locally_disabled_peers.clear();
         } else {
@@ -926,15 +937,17 @@ impl ReplicaSetState {
         self.peers
             .iter()
             .filter_map(|(peer_id, state)| {
-                if *state == ReplicaState::Active {
-                    Some(*peer_id)
-                } else {
-                    None
-                }
+                matches!(state, ReplicaState::Active).then_some(*peer_id)
             })
             .collect()
     }
 
+    pub fn active_or_resharding_peers(&self) -> impl Iterator<Item = PeerId> + '_ {
+        self.peers.iter().filter_map(|(peer_id, state)| {
+            matches!(state, ReplicaState::Active | ReplicaState::Resharding).then_some(*peer_id)
+        })
+    }
+
     pub fn set_peers(&mut self, peers: HashMap<PeerId, ReplicaState>) {
         self.peers = peers;
     }

commit c7da6ae36c455a67859dbc2a9f1e3ce274645121
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Thu Aug 8 12:41:33 2024 +0200

    Non blocking retrieve with timeout and cancellation support (#4844)
    
    * Non blocking retrieve with timeout and cancellation support
    
    * apply timeout for extra retrieve in rescoring

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 50ceb6689..338e28344 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -144,6 +144,7 @@ impl ShardReplicaSet {
                 shared_storage_config.clone(),
                 payload_index_schema.clone(),
                 update_runtime.clone(),
+                search_runtime.clone(),
                 optimizer_cpu_budget.clone(),
                 effective_optimizers_config.clone(),
             )
@@ -264,6 +265,7 @@ impl ShardReplicaSet {
                     shared_storage_config.clone(),
                     payload_index_schema.clone(),
                     update_runtime.clone(),
+                    search_runtime.clone(),
                     optimizer_cpu_budget.clone(),
                 )
                 .await;
@@ -479,6 +481,7 @@ impl ShardReplicaSet {
             self.shared_storage_config.clone(),
             self.payload_index_schema.clone(),
             self.update_runtime.clone(),
+            self.search_runtime.clone(),
             self.optimizer_cpu_budget.clone(),
             self.optimizers_config.clone(),
         )
@@ -664,6 +667,7 @@ impl ShardReplicaSet {
                     self.shared_storage_config.clone(),
                     self.payload_index_schema.clone(),
                     self.update_runtime.clone(),
+                    self.search_runtime.clone(),
                     self.optimizer_cpu_budget.clone(),
                     self.optimizers_config.clone(),
                 )

commit 35682861325ad345058ef3e33e74cba7afba33d3
Author: Kumar Shivendu <kshivendu1@gmail.com>
Date:   Thu Sep 5 13:08:39 2024 +0530

    Introduce grey collection status and expose shard status in telemetry (#4940)
    
    * Expose shard status in telemetry API
    
    * fmt
    
    * Drop segment lock before using async fetching shard status
    
    * Use Self in From implementation for ShardStatus to CollectionStatus mapping
    
    * Improve comments
    
    * Remove redundant clone
    
    * Update openapi specs
    
    * Isolate function for shard status
    
    * Fix compiler error
    
    * Avoid adding dedicated function for shard status
    
    * review fixes
    
    * define missing var
    
    * lint err
    
    * comment
    
    * comment
    
    * refactor
    
    * improve comments
    
    * Improve comments
    
    * fix lint and update openapi specs
    
    * improve comment

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 338e28344..db203b6b9 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -746,12 +746,16 @@ impl ShardReplicaSet {
 
     pub(crate) async fn get_telemetry_data(&self, detail: TelemetryDetail) -> ReplicaSetTelemetry {
         let local_shard = self.local.read().await;
-        let local = local_shard
-            .as_ref()
-            .map(|local_shard| local_shard.get_telemetry_data(detail));
+        let local = local_shard.as_ref();
+
+        let local_telemetry = match local {
+            Some(local_shard) => Some(local_shard.get_telemetry_data(detail).await),
+            None => None,
+        };
+
         ReplicaSetTelemetry {
             id: self.shard_id,
-            local,
+            local: local_telemetry,
             remote: self
                 .remotes
                 .read()

commit f61deaf44731e37e311ad615144cae6e72d879b5
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Mon Sep 16 14:37:33 2024 +0200

    abort shard transer on delete of replica + check that we dont delete last active (#5079)

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index db203b6b9..49499a0f8 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -357,6 +357,11 @@ impl ShardReplicaSet {
         self.replica_state.read().peers()
     }
 
+    pub fn is_last_active_replica(&self, peer_id: PeerId) -> bool {
+        let active_peers = self.replica_state.read().active_peers();
+        active_peers.len() == 1 && active_peers.contains(&peer_id)
+    }
+
     pub fn peer_state(&self, peer_id: &PeerId) -> Option<ReplicaState> {
         self.replica_state.read().get_peer_state(peer_id).copied()
     }

commit d45c8bbee11b9a61b95430d8828fe25afd0e6843
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Wed Sep 18 13:03:12 2024 +0200

    Cleanup shards when aborting scale-down resharding (#5021)

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 49499a0f8..ebe664816 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -15,6 +15,7 @@ use std::time::Duration;
 use common::cpu::CpuBudget;
 use common::types::TelemetryDetail;
 use schemars::JsonSchema;
+use segment::types::{ExtendedPointId, Filter};
 use serde::{Deserialize, Serialize};
 use tokio::runtime::Handle;
 use tokio::sync::{Mutex, RwLock};
@@ -27,8 +28,10 @@ use super::CollectionId;
 use crate::collection::payload_index_schema::PayloadIndexSchema;
 use crate::common::snapshots_manager::SnapshotStorageManager;
 use crate::config::CollectionConfig;
+use crate::operations::point_ops::{self};
 use crate::operations::shared_storage_config::SharedStorageConfig;
-use crate::operations::types::{CollectionError, CollectionResult};
+use crate::operations::types::{CollectionError, CollectionResult, UpdateResult, UpdateStatus};
+use crate::operations::CollectionUpdateOperations;
 use crate::optimizers_builder::OptimizersConfig;
 use crate::save_on_disk::SaveOnDisk;
 use crate::shards::channel_service::ChannelService;
@@ -786,6 +789,70 @@ impl ShardReplicaSet {
         Ok(())
     }
 
+    pub async fn delete_local_points(&self, filter: Filter) -> CollectionResult<UpdateResult> {
+        let local_shard_guard = self.local.read().await;
+
+        let Some(local_shard) = local_shard_guard.deref() else {
+            return Err(CollectionError::NotFound {
+                what: format!("local shard {}:{}", self.collection_id, self.shard_id),
+            });
+        };
+
+        let mut next_offset = Some(ExtendedPointId::NumId(0));
+        let mut ids = Vec::new();
+
+        while let Some(current_offset) = next_offset {
+            const BATCH_SIZE: usize = 1000;
+
+            let mut points = local_shard
+                .get()
+                .scroll_by(
+                    Some(current_offset),
+                    BATCH_SIZE + 1,
+                    &false.into(),
+                    &false.into(),
+                    Some(&filter),
+                    &self.search_runtime,
+                    None,
+                    None,
+                )
+                .await?;
+
+            if points.len() > BATCH_SIZE {
+                next_offset = points.pop().map(|points| points.id);
+            } else {
+                next_offset = None;
+            }
+
+            ids.extend(points.into_iter().map(|points| points.id));
+        }
+
+        if ids.is_empty() {
+            return Ok(UpdateResult {
+                operation_id: None,
+                status: UpdateStatus::Completed,
+                clock_tag: None,
+            });
+        }
+
+        drop(local_shard_guard);
+
+        let op =
+            CollectionUpdateOperations::PointOperation(point_ops::PointOperations::DeletePoints {
+                ids,
+            });
+
+        // TODO(resharding): Assign clock tag to the operation!? 🤔
+        let result = self.update_local(op.into(), false).await?.ok_or_else(|| {
+            CollectionError::bad_request(format!(
+                "local shard {}:{} does not exist or is unavailable",
+                self.collection_id, self.shard_id,
+            ))
+        })?;
+
+        Ok(result)
+    }
+
     fn init_remote_shards(
         shard_id: ShardId,
         collection_id: CollectionId,

commit 428e09d49b8fcd943427c5d397686ed08cd08337
Author: Jojii <15957865+JojiiOfficial@users.noreply.github.com>
Date:   Tue Sep 24 17:52:03 2024 +0200

    Trigger optimizers when uploading snapshots (#5140)
    
    * Trigger optimizers when uploading snapshots
    
    * Trigger optimizers through new method when optimizer config is updated
    
    * review remarks
    
    * Add comment on why ignoring channel send errors is fine
    
    * Remove obsolete return value omitting
    
    ---------
    
    Co-authored-by: timvisee <tim@visee.me>

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index ebe664816..2f919593f 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -986,6 +986,15 @@ impl ShardReplicaSet {
     pub(crate) fn get_snapshots_storage_manager(&self) -> CollectionResult<SnapshotStorageManager> {
         SnapshotStorageManager::new(self.shared_storage_config.snapshots_config.clone())
     }
+
+    pub(crate) async fn trigger_optimizers(&self) -> bool {
+        let shard = self.local.read().await;
+        let Some(shard) = shard.as_ref() else {
+            return false;
+        };
+        shard.trigger_optimizers().await;
+        true
+    }
 }
 
 /// Represents a replica set state

commit 751b4adced7c5cbe8cea1a228e01cec5632fcc4d
Author: Kumar Shivendu <kshivendu1@gmail.com>
Date:   Wed Oct 9 14:44:24 2024 +0530

    Dont consider resharding shards as unhealthy for /readyz (#5204)
    
    * Dont consider resharding shards as unhealthy for /readyz
    
    * clippy

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 2f919593f..b98b2e11d 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -1072,18 +1072,17 @@ pub enum ReplicaState {
 }
 
 impl ReplicaState {
-    /// Check whether the replica state is active or listener.
-    pub fn is_active_or_listener(self) -> bool {
+    /// Check whether the replica state is active or listener or resharding.
+    pub fn is_active_or_listener_or_resharding(self) -> bool {
         // Use explicit match, to catch future changes to `ReplicaState`
         match self {
-            ReplicaState::Active | ReplicaState::Listener => true,
+            ReplicaState::Active | ReplicaState::Listener | ReplicaState::Resharding => true,
 
             ReplicaState::Dead
             | ReplicaState::Initializing
             | ReplicaState::Partial
             | ReplicaState::PartialSnapshot
-            | ReplicaState::Recovery
-            | ReplicaState::Resharding => false,
+            | ReplicaState::Recovery => false,
         }
     }
 

commit d4716da8e7be0111ba7ef810b3525e5bde2ae56a
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Mon Oct 14 13:13:58 2024 +0200

    fix lints for Clippy 1.82 (#5229)
    
    * fix lints for Clippy 1.82
    
    * regen openapi

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index b98b2e11d..69ae32358 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -78,6 +78,7 @@ use crate::shards::telemetry::ReplicaSetTelemetry;
 //
 
 /// A set of shard replicas.
+///
 /// Handles operations so that the state is consistent across all the replicas of the shard.
 /// Prefers local shard for read-only operations.
 /// Perform updates on all replicas and report error if there is at least one failure.

commit bdf824241332ca9ab678119f6dd3885eb70e3d5a
Author: Tim Visée <tim+github@visee.me>
Date:   Wed Oct 23 12:34:12 2024 +0200

    Minor improvements around the WAL delta fix (#5286)
    
    * Restructure WAL delta logic, similar to what we had before the patch
    
    * Tweak comments to elaborate on data race fix
    
    * Refactor recovered function, not snapshot specific anymore
    
    * Remove partial snapshot replica set state from schema

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 69ae32358..2a05c30b3 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -1062,7 +1062,7 @@ pub enum ReplicaState {
     //
     // Snapshot shard transfer is in progress, updates aren't sent to the shard
     // Normally rejects updates. Since 1.8 it allows updates if force is true.
-    // TODO(1.10): remove PartialSnapshot state entirely?
+    #[schemars(skip)]
     PartialSnapshot,
     // Shard is undergoing recovery by an external node
     // Normally rejects updates, accepts updates if force is true

commit 80310a0c7090696d0bfd7adb63e186f35e6acb1c
Author: Tim Visée <tim+github@visee.me>
Date:   Wed Oct 23 17:40:09 2024 +0200

    Fix data race in replica state switching causing inconsistencies (#5298)
    
    * Do send updates to replica in recovery state
    
    The peer holding the replica may keep it in active state for some time
    if consensus is slow to respond. It allows the peer to keep responding
    to read requests. Those reads may be stale if we don't send updates to
    it, causing data inconsistencies.
    
    * Rename function that tells us if a peer accepts updates
    
    * Link to pull request with detailed example on the data race
    
    * Update allowed replica states for precondition failure, use exhaustive match
    
    * Don't consider precondition fails for write consistency factor
    
    * The minimum number of successful update requests should never be 0
    
    * Simplify minimum allowed successes, cap write factor at replica count

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 2a05c30b3..b16e93642 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -1075,7 +1075,6 @@ pub enum ReplicaState {
 impl ReplicaState {
     /// Check whether the replica state is active or listener or resharding.
     pub fn is_active_or_listener_or_resharding(self) -> bool {
-        // Use explicit match, to catch future changes to `ReplicaState`
         match self {
             ReplicaState::Active | ReplicaState::Listener | ReplicaState::Resharding => true,
 
@@ -1088,8 +1087,9 @@ impl ReplicaState {
     }
 
     /// Check whether the replica state is partial or partial-like.
+    ///
+    /// In other words: is the state related to shard transfers?
     pub fn is_partial_or_recovery(self) -> bool {
-        // Use explicit match, to catch future changes to `ReplicaState`
         match self {
             ReplicaState::Partial
             | ReplicaState::PartialSnapshot

commit 1128ac8ff8b91ff052d85bd63974930bbbfed210
Author: Tim Visée <tim+github@visee.me>
Date:   Thu Oct 31 15:44:49 2024 +0100

    Prefer owned versus referenced usage of PeerId/ShardId, they are Copy (#5344)

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index b16e93642..bc685c913 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -236,7 +236,7 @@ impl ShardReplicaSet {
             replica_state
                 .write(|rs| {
                     let this_peer_id = rs.this_peer_id;
-                    let local_state = rs.remove_peer_state(&this_peer_id);
+                    let local_state = rs.remove_peer_state(this_peer_id);
                     if let Some(state) = local_state {
                         rs.set_peer_state(this_peer_id, state);
                     }
@@ -366,7 +366,7 @@ impl ShardReplicaSet {
         active_peers.len() == 1 && active_peers.contains(&peer_id)
     }
 
-    pub fn peer_state(&self, peer_id: &PeerId) -> Option<ReplicaState> {
+    pub fn peer_state(&self, peer_id: PeerId) -> Option<ReplicaState> {
         self.replica_state.read().get_peer_state(peer_id).copied()
     }
 
@@ -376,7 +376,7 @@ impl ShardReplicaSet {
         replica_state
             .active_peers()
             .into_iter()
-            .filter(|peer_id| !self.is_locally_disabled(peer_id))
+            .filter(|&peer_id| !self.is_locally_disabled(peer_id))
             .collect()
     }
 
@@ -387,7 +387,7 @@ impl ShardReplicaSet {
         replica_state
             .active_peers()
             .into_iter()
-            .filter(|peer_id| !self.is_locally_disabled(peer_id) && *peer_id != this_peer_id)
+            .filter(|&peer_id| !self.is_locally_disabled(peer_id) && peer_id != this_peer_id)
             .collect()
     }
 
@@ -417,7 +417,7 @@ impl ShardReplicaSet {
     ) -> CollectionResult<()> {
         self.wait_for(
             move |replica_set_state| {
-                replica_set_state.get_peer_state(&replica_set_state.this_peer_id) == Some(&state)
+                replica_set_state.get_peer_state(replica_set_state.this_peer_id) == Some(&state)
             },
             timeout,
         )
@@ -438,7 +438,7 @@ impl ShardReplicaSet {
         timeout: Duration,
     ) -> CollectionResult<()> {
         self.wait_for(
-            move |replica_set_state| replica_set_state.get_peer_state(&peer_id) == Some(&state),
+            move |replica_set_state| replica_set_state.get_peer_state(peer_id) == Some(&state),
             timeout,
         )
         .await
@@ -537,7 +537,7 @@ impl ShardReplicaSet {
         self.replica_state.write(|rs| {
             rs.is_local = false;
             let this_peer_id = rs.this_peer_id;
-            rs.remove_peer_state(&this_peer_id);
+            rs.remove_peer_state(this_peer_id);
         })?;
 
         self.update_locally_disabled(self.this_peer_id());
@@ -583,7 +583,7 @@ impl ShardReplicaSet {
 
     pub async fn remove_remote(&self, peer_id: PeerId) -> CollectionResult<()> {
         self.replica_state.write(|rs| {
-            rs.remove_peer_state(&peer_id);
+            rs.remove_peer_state(peer_id);
         })?;
 
         self.update_locally_disabled(peer_id);
@@ -597,19 +597,19 @@ impl ShardReplicaSet {
     /// Ensure that remote shard is initialized.
     pub async fn ensure_replica_with_state(
         &self,
-        peer_id: &PeerId,
+        peer_id: PeerId,
         state: ReplicaState,
     ) -> CollectionResult<()> {
-        if *peer_id == self.this_peer_id() {
+        if peer_id == self.this_peer_id() {
             self.set_replica_state(peer_id, state)?;
         } else {
             // Create remote shard if necessary
-            self.add_remote(*peer_id, state).await?;
+            self.add_remote(peer_id, state).await?;
         }
         Ok(())
     }
 
-    pub fn set_replica_state(&self, peer_id: &PeerId, state: ReplicaState) -> CollectionResult<()> {
+    pub fn set_replica_state(&self, peer_id: PeerId, state: ReplicaState) -> CollectionResult<()> {
         log::debug!(
             "Changing local shard {}:{} state from {:?} to {state:?}",
             self.collection_id,
@@ -618,12 +618,12 @@ impl ShardReplicaSet {
         );
 
         self.replica_state.write(|rs| {
-            if rs.this_peer_id == *peer_id {
+            if rs.this_peer_id == peer_id {
                 rs.is_local = true;
             }
-            rs.set_peer_state(*peer_id, state);
+            rs.set_peer_state(peer_id, state);
         })?;
-        self.update_locally_disabled(*peer_id);
+        self.update_locally_disabled(peer_id);
         Ok(())
     }
 
@@ -877,11 +877,11 @@ impl ShardReplicaSet {
 
     /// Check whether a peer is registered as `active`.
     /// Unknown peers are not active.
-    fn peer_is_active(&self, peer_id: &PeerId) -> bool {
+    fn peer_is_active(&self, peer_id: PeerId) -> bool {
         self.peer_state(peer_id) == Some(ReplicaState::Active) && !self.is_locally_disabled(peer_id)
     }
 
-    fn peer_is_active_or_resharding(&self, peer_id: &PeerId) -> bool {
+    fn peer_is_active_or_resharding(&self, peer_id: PeerId) -> bool {
         let is_active_or_resharding = matches!(
             self.peer_state(peer_id),
             Some(ReplicaState::Active | ReplicaState::Resharding)
@@ -892,8 +892,8 @@ impl ShardReplicaSet {
         is_active_or_resharding && !is_locally_disabled
     }
 
-    fn is_locally_disabled(&self, peer_id: &PeerId) -> bool {
-        self.locally_disabled_peers.read().is_disabled(*peer_id)
+    fn is_locally_disabled(&self, peer_id: PeerId) -> bool {
+        self.locally_disabled_peers.read().is_disabled(peer_id)
     }
 
     /// Locally disable given peer
@@ -1007,16 +1007,16 @@ pub struct ReplicaSetState {
 }
 
 impl ReplicaSetState {
-    pub fn get_peer_state(&self, peer_id: &PeerId) -> Option<&ReplicaState> {
-        self.peers.get(peer_id)
+    pub fn get_peer_state(&self, peer_id: PeerId) -> Option<&ReplicaState> {
+        self.peers.get(&peer_id)
     }
 
     pub fn set_peer_state(&mut self, peer_id: PeerId, state: ReplicaState) {
         self.peers.insert(peer_id, state);
     }
 
-    pub fn remove_peer_state(&mut self, peer_id: &PeerId) -> Option<ReplicaState> {
-        self.peers.remove(peer_id)
+    pub fn remove_peer_state(&mut self, peer_id: PeerId) -> Option<ReplicaState> {
+        self.peers.remove(&peer_id)
     }
 
     pub fn peers(&self) -> HashMap<PeerId, ReplicaState> {

commit 7daf1aa21b2fbc76372b10ec017671e5e9af0ec3
Author: Tim Visée <tim+github@visee.me>
Date:   Fri Nov 1 20:19:16 2024 +0100

    Fix killing replicas too earily on state switches (#5343)
    
    * Send current replica state with disable replica proposals
    
    * Don't be strict about from state if in shard transfer related state
    
    * Link to PR, update formatting
    
    * Fix typo
    
    * Fix test compilation failures
    
    * Provide peer state when deactivating leader, remove unnecessary TODO
    
    * ReplicaState is Copy

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index bc685c913..b1f98073a 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -94,7 +94,7 @@ pub struct ShardReplicaSet {
     locally_disabled_peers: parking_lot::RwLock<locally_disabled_peers::Registry>,
     pub(crate) shard_path: PathBuf,
     pub(crate) shard_id: ShardId,
-    notify_peer_failure_cb: ChangePeerState,
+    notify_peer_failure_cb: ChangePeerFromState,
     abort_shard_transfer_cb: AbortShardTransfer,
     channel_service: ChannelService,
     collection_id: CollectionId,
@@ -113,6 +113,7 @@ pub struct ShardReplicaSet {
 
 pub type AbortShardTransfer = Arc<dyn Fn(ShardTransfer, &str) + Send + Sync>;
 pub type ChangePeerState = Arc<dyn Fn(PeerId, ShardId) + Send + Sync>;
+pub type ChangePeerFromState = Arc<dyn Fn(PeerId, ShardId, Option<ReplicaState>) + Send + Sync>;
 
 const REPLICA_STATE_FILE: &str = "replica_state.json";
 
@@ -125,7 +126,7 @@ impl ShardReplicaSet {
         this_peer_id: PeerId,
         local: bool,
         remotes: HashSet<PeerId>,
-        on_peer_failure: ChangePeerState,
+        on_peer_failure: ChangePeerFromState,
         abort_shard_transfer: AbortShardTransfer,
         collection_path: &Path,
         collection_config: Arc<RwLock<CollectionConfig>>,
@@ -222,7 +223,7 @@ impl ShardReplicaSet {
         shared_storage_config: Arc<SharedStorageConfig>,
         payload_index_schema: Arc<SaveOnDisk<PayloadIndexSchema>>,
         channel_service: ChannelService,
-        on_peer_failure: ChangePeerState,
+        on_peer_failure: ChangePeerFromState,
         abort_shard_transfer: AbortShardTransfer,
         this_peer_id: PeerId,
         update_runtime: Handle,
@@ -367,7 +368,7 @@ impl ShardReplicaSet {
     }
 
     pub fn peer_state(&self, peer_id: PeerId) -> Option<ReplicaState> {
-        self.replica_state.read().get_peer_state(peer_id).copied()
+        self.replica_state.read().get_peer_state(peer_id)
     }
 
     /// List the peer IDs on which this shard is active, both the local and remote peers.
@@ -417,7 +418,7 @@ impl ShardReplicaSet {
     ) -> CollectionResult<()> {
         self.wait_for(
             move |replica_set_state| {
-                replica_set_state.get_peer_state(replica_set_state.this_peer_id) == Some(&state)
+                replica_set_state.get_peer_state(replica_set_state.this_peer_id) == Some(state)
             },
             timeout,
         )
@@ -438,7 +439,7 @@ impl ShardReplicaSet {
         timeout: Duration,
     ) -> CollectionResult<()> {
         self.wait_for(
-            move |replica_set_state| replica_set_state.get_peer_state(peer_id) == Some(&state),
+            move |replica_set_state| replica_set_state.get_peer_state(peer_id) == Some(state),
             timeout,
         )
         .await
@@ -686,7 +687,7 @@ impl ShardReplicaSet {
                         // No way we can provide up-to-date replica right away at this point,
                         // so we report a failure to consensus
                         self.set_local(local_shard, Some(state)).await?;
-                        self.notify_peer_failure(peer_id);
+                        self.notify_peer_failure(peer_id, Some(state));
                     }
 
                     ReplicaState::Dead
@@ -735,16 +736,15 @@ impl ShardReplicaSet {
             .notify_elapsed()
             .collect();
 
-        for failed_peer in peers_to_notify {
-            // TODO: Only `notify_peer_failure` if `failed_peer` is *not* the last `Active` peer? 🤔
-            self.notify_peer_failure(failed_peer);
+        for (failed_peer_id, from_state) in peers_to_notify {
+            self.notify_peer_failure(failed_peer_id, from_state);
 
-            for transfer in get_shard_transfers(self.shard_id, failed_peer) {
+            for transfer in get_shard_transfers(self.shard_id, failed_peer_id) {
                 self.abort_shard_transfer(
                     transfer,
                     &format!(
-                        "{failed_peer}/{}:{} replica failed",
-                        self.collection_id, self.shard_id
+                        "{failed_peer_id}/{}:{} replica failed",
+                        self.collection_id, self.shard_id,
                     ),
                 );
             }
@@ -901,7 +901,15 @@ impl ShardReplicaSet {
     /// Disables the peer and notifies consensus periodically.
     ///
     /// Prevents disabling the last peer (according to consensus).
-    fn add_locally_disabled(&self, state: &ReplicaSetState, peer_id: PeerId) {
+    ///
+    /// If `from_state` is given, the peer will only be disabled if the given state matches
+    /// consensus.
+    fn add_locally_disabled(
+        &self,
+        state: &ReplicaSetState,
+        peer_id: PeerId,
+        from_state: Option<ReplicaState>,
+    ) {
         let other_peers = state
             .active_or_resharding_peers()
             .filter(|id| id != &peer_id);
@@ -919,8 +927,8 @@ impl ShardReplicaSet {
         }
 
         locally_disabled_peers_guard.with_upgraded(|locally_disabled_peers| {
-            if locally_disabled_peers.disable_peer_and_notify_if_elapsed(peer_id) {
-                self.notify_peer_failure(peer_id);
+            if locally_disabled_peers.disable_peer_and_notify_if_elapsed(peer_id, from_state) {
+                self.notify_peer_failure(peer_id, from_state);
             }
         });
     }
@@ -940,9 +948,9 @@ impl ShardReplicaSet {
         }
     }
 
-    fn notify_peer_failure(&self, peer_id: PeerId) {
+    fn notify_peer_failure(&self, peer_id: PeerId, from_state: Option<ReplicaState>) {
         log::debug!("Notify peer failure: {}", peer_id);
-        self.notify_peer_failure_cb.deref()(peer_id, self.shard_id)
+        self.notify_peer_failure_cb.deref()(peer_id, self.shard_id, from_state)
     }
 
     fn abort_shard_transfer(&self, transfer: ShardTransfer, reason: &str) {
@@ -1007,8 +1015,8 @@ pub struct ReplicaSetState {
 }
 
 impl ReplicaSetState {
-    pub fn get_peer_state(&self, peer_id: PeerId) -> Option<&ReplicaState> {
-        self.peers.get(&peer_id)
+    pub fn get_peer_state(&self, peer_id: PeerId) -> Option<ReplicaState> {
+        self.peers.get(&peer_id).copied()
     }
 
     pub fn set_peer_state(&mut self, peer_id: PeerId, state: ReplicaState) {

commit 758dca6f5a68282732d0de771915963fb19fd240
Author: Tim Visée <tim+github@visee.me>
Date:   Fri Nov 1 20:42:30 2024 +0100

    Experiment: ignore clock tags when replica is in partial state (#5349)
    
    * Add internal interface to enable/disable clock tags on recoverable WAL
    
    * Disable WAL clocks when switching replica into partial state
    
    * Synchronize consensus at the end of stream records transfer
    
    * Minor refactoring
    
    * Do not send updates to replicas in recovery state
    
    * Fix test compilation

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index b1f98073a..e72581d35 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -602,7 +602,7 @@ impl ShardReplicaSet {
         state: ReplicaState,
     ) -> CollectionResult<()> {
         if peer_id == self.this_peer_id() {
-            self.set_replica_state(peer_id, state)?;
+            self.set_replica_state(peer_id, state).await?;
         } else {
             // Create remote shard if necessary
             self.add_remote(peer_id, state).await?;
@@ -610,7 +610,11 @@ impl ShardReplicaSet {
         Ok(())
     }
 
-    pub fn set_replica_state(&self, peer_id: PeerId, state: ReplicaState) -> CollectionResult<()> {
+    pub async fn set_replica_state(
+        &self,
+        peer_id: PeerId,
+        state: ReplicaState,
+    ) -> CollectionResult<()> {
         log::debug!(
             "Changing local shard {}:{} state from {:?} to {state:?}",
             self.collection_id,
@@ -624,6 +628,12 @@ impl ShardReplicaSet {
             }
             rs.set_peer_state(peer_id, state);
         })?;
+
+        // Disable WAL clocks only if the replica is in Partial state
+        if let Some(local) = self.local.read().await.as_ref() {
+            local.set_clocks_enabled(state != ReplicaState::Partial);
+        }
+
         self.update_locally_disabled(peer_id);
         Ok(())
     }

commit 2656b7be1bb6247a10a90dea9735993a96c2e0e1
Author: Tim Visée <tim+github@visee.me>
Date:   Tue Nov 5 17:46:02 2024 +0100

    Experiment: disable clocks in initializing state, propagate ignore flag (#5372)
    
    * Propagate flag for ignoring clocks on local shard from replica set
    
    * Also ignore local clocks in initializing state, is similar to partial
    
    * Remove previous logic for disabling clocks
    
    * We can make static replica set state functions inlined const
    
    * Fix typo

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index e72581d35..455d59cf1 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -628,12 +628,6 @@ impl ShardReplicaSet {
             }
             rs.set_peer_state(peer_id, state);
         })?;
-
-        // Disable WAL clocks only if the replica is in Partial state
-        if let Some(local) = self.local.read().await.as_ref() {
-            local.set_clocks_enabled(state != ReplicaState::Partial);
-        }
-
         self.update_locally_disabled(peer_id);
         Ok(())
     }
@@ -1092,7 +1086,8 @@ pub enum ReplicaState {
 
 impl ReplicaState {
     /// Check whether the replica state is active or listener or resharding.
-    pub fn is_active_or_listener_or_resharding(self) -> bool {
+    #[inline]
+    pub const fn is_active_or_listener_or_resharding(self) -> bool {
         match self {
             ReplicaState::Active | ReplicaState::Listener | ReplicaState::Resharding => true,
 
@@ -1107,7 +1102,8 @@ impl ReplicaState {
     /// Check whether the replica state is partial or partial-like.
     ///
     /// In other words: is the state related to shard transfers?
-    pub fn is_partial_or_recovery(self) -> bool {
+    #[inline]
+    pub const fn is_partial_or_recovery(self) -> bool {
         match self {
             ReplicaState::Partial
             | ReplicaState::PartialSnapshot
@@ -1120,6 +1116,24 @@ impl ReplicaState {
             | ReplicaState::Listener => false,
         }
     }
+
+    /// Check whether this is a state in which we ignore local clocks.
+    ///
+    /// During some replica states, using clocks may create gaps. That'll be problematic if WAL
+    /// clocks all together to prevent this problem.
+    /// delta recovery is used later, resulting in missing operations. In these states we ignore
+    #[inline]
+    pub const fn is_ignore_local_clocks(self) -> bool {
+        match self {
+            ReplicaState::Initializing | ReplicaState::Partial => true,
+            ReplicaState::Active
+            | ReplicaState::Listener
+            | ReplicaState::Resharding
+            | ReplicaState::Dead
+            | ReplicaState::PartialSnapshot
+            | ReplicaState::Recovery => false,
+        }
+    }
 }
 
 /// Represents a change in replica set, due to scaling of `replication_factor`

commit 567eb36efcdf570612cdcf8a1eb21e5fbb2cca75
Author: Tim Visée <tim+github@visee.me>
Date:   Wed Nov 6 16:08:38 2024 +0100

    Set replica state function does not need to be async anymore (#5379)

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 455d59cf1..67da948d3 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -602,7 +602,7 @@ impl ShardReplicaSet {
         state: ReplicaState,
     ) -> CollectionResult<()> {
         if peer_id == self.this_peer_id() {
-            self.set_replica_state(peer_id, state).await?;
+            self.set_replica_state(peer_id, state)?;
         } else {
             // Create remote shard if necessary
             self.add_remote(peer_id, state).await?;
@@ -610,11 +610,7 @@ impl ShardReplicaSet {
         Ok(())
     }
 
-    pub async fn set_replica_state(
-        &self,
-        peer_id: PeerId,
-        state: ReplicaState,
-    ) -> CollectionResult<()> {
+    pub fn set_replica_state(&self, peer_id: PeerId, state: ReplicaState) -> CollectionResult<()> {
         log::debug!(
             "Changing local shard {}:{} state from {:?} to {state:?}",
             self.collection_id,

commit aecda86ca5915692a120a7abc91bbcc56e7dae34
Author: Tim Visée <tim+github@visee.me>
Date:   Wed Nov 6 16:48:38 2024 +0100

    Experiment: in stream records, set cutoff to latest clocks receiver is guaranteed to have (#5375)
    
    * On stream records transfer, send cutoff point from before transfers
    
    * Fix swapped lines in description

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 67da948d3..e19c482cf 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -1116,8 +1116,8 @@ impl ReplicaState {
     /// Check whether this is a state in which we ignore local clocks.
     ///
     /// During some replica states, using clocks may create gaps. That'll be problematic if WAL
-    /// clocks all together to prevent this problem.
     /// delta recovery is used later, resulting in missing operations. In these states we ignore
+    /// clocks all together to prevent this problem.
     #[inline]
     pub const fn is_ignore_local_clocks(self) -> bool {
         match self {

commit 4240e71859b86195c03d84ac363f9699b7bc0317
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Fri Nov 8 10:10:44 2024 +0100

    No useless async (#5401)
    
    * Remove unecessary async/await
    
    * clippy aftermath

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index e19c482cf..cac5680be 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -325,7 +325,7 @@ impl ShardReplicaSet {
             clock_set: Default::default(),
         };
 
-        if local_load_failure && replica_set.active_remote_shards().await.is_empty() {
+        if local_load_failure && replica_set.active_remote_shards().is_empty() {
             replica_set
                 .locally_disabled_peers
                 .write()
@@ -372,7 +372,7 @@ impl ShardReplicaSet {
     }
 
     /// List the peer IDs on which this shard is active, both the local and remote peers.
-    pub async fn active_shards(&self) -> Vec<PeerId> {
+    pub fn active_shards(&self) -> Vec<PeerId> {
         let replica_state = self.replica_state.read();
         replica_state
             .active_peers()
@@ -382,7 +382,7 @@ impl ShardReplicaSet {
     }
 
     /// List the remote peer IDs on which this shard is active, excludes the local peer ID.
-    pub async fn active_remote_shards(&self) -> Vec<PeerId> {
+    pub fn active_remote_shards(&self) -> Vec<PeerId> {
         let replica_state = self.replica_state.read();
         let this_peer_id = replica_state.this_peer_id;
         replica_state
@@ -1001,7 +1001,7 @@ impl ShardReplicaSet {
         let Some(shard) = shard.as_ref() else {
             return false;
         };
-        shard.trigger_optimizers().await;
+        shard.trigger_optimizers();
         true
     }
 }

commit 9eb0626a315693619d5191c705f978c56a020dff
Author: Tim Visée <tim+github@visee.me>
Date:   Fri Nov 8 14:48:53 2024 +0100

    Revert experiment: ignore WAL clocks in partial state (#5353)
    
    * Revert "Experiment: in stream records, set cutoff to latest clocks receiver is guaranteed to have (#5375)"
    
    This reverts commit e843647c954c60360fa82287e3aa21a2a773598d.
    
    * Revert "Set replica state function does not need to be async anymore (#5379)"
    
    This reverts commit fdf08e0a59b443d339faffd465211ff36402d896.
    
    * Revert "Experiment: disable clocks in initializing state, propagate ignore flag (#5372)"
    
    This reverts commit 0d98f6fe1a4da3c5d8699f50df41d84844ee3d51.
    
    * Revert "Experiment: also don't write clock tags to WAL in partial state (#5352)"
    
    This reverts commit f69ec6184805e5560632a00cf97761645c2ab01a.
    
    * Revert "Experiment: ignore clock tags when replica is in partial state (#5349)"
    
    This reverts commit 1b8a38f70b86cb9b5fda20f93c3d8e10d1abf80a.
    
    * Keep change to not send updates to replicas in recovery state
    
    Co-authored-by: Andrey Vasnetsov <andrey@vasnetsov.com>
    
    ---------
    
    Co-authored-by: Andrey Vasnetsov <andrey@vasnetsov.com>

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index cac5680be..889486004 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -1082,8 +1082,7 @@ pub enum ReplicaState {
 
 impl ReplicaState {
     /// Check whether the replica state is active or listener or resharding.
-    #[inline]
-    pub const fn is_active_or_listener_or_resharding(self) -> bool {
+    pub fn is_active_or_listener_or_resharding(self) -> bool {
         match self {
             ReplicaState::Active | ReplicaState::Listener | ReplicaState::Resharding => true,
 
@@ -1098,8 +1097,7 @@ impl ReplicaState {
     /// Check whether the replica state is partial or partial-like.
     ///
     /// In other words: is the state related to shard transfers?
-    #[inline]
-    pub const fn is_partial_or_recovery(self) -> bool {
+    pub fn is_partial_or_recovery(self) -> bool {
         match self {
             ReplicaState::Partial
             | ReplicaState::PartialSnapshot
@@ -1112,24 +1110,6 @@ impl ReplicaState {
             | ReplicaState::Listener => false,
         }
     }
-
-    /// Check whether this is a state in which we ignore local clocks.
-    ///
-    /// During some replica states, using clocks may create gaps. That'll be problematic if WAL
-    /// delta recovery is used later, resulting in missing operations. In these states we ignore
-    /// clocks all together to prevent this problem.
-    #[inline]
-    pub const fn is_ignore_local_clocks(self) -> bool {
-        match self {
-            ReplicaState::Initializing | ReplicaState::Partial => true,
-            ReplicaState::Active
-            | ReplicaState::Listener
-            | ReplicaState::Resharding
-            | ReplicaState::Dead
-            | ReplicaState::PartialSnapshot
-            | ReplicaState::Recovery => false,
-        }
-    }
 }
 
 /// Represents a change in replica set, due to scaling of `replication_factor`

commit 98633cbd3fdd01ee3c486a3573ff27dc180e3b6d
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Mon Nov 11 10:26:17 2024 +0100

    Use references for less cloning when possible (#5409)

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 889486004..68caebeee 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -993,7 +993,7 @@ impl ShardReplicaSet {
     }
 
     pub(crate) fn get_snapshots_storage_manager(&self) -> CollectionResult<SnapshotStorageManager> {
-        SnapshotStorageManager::new(self.shared_storage_config.snapshots_config.clone())
+        SnapshotStorageManager::new(&self.shared_storage_config.snapshots_config)
     }
 
     pub(crate) async fn trigger_optimizers(&self) -> bool {

commit 9e06d68661402bb2df271134bab5d9aeda995048
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Sat Nov 16 01:03:50 2024 +0700

    Add UUID to collection config (#5378)
    
    * Add UUID to collection...
    
    ...and recreate collection, when applying Raft snapshot, if UUID of collection is different
    
    * fixup! Add UUID to collection...
    
    Remove UUID field from gRPC and exclude it from OpenAPI spec 🤡
    
    * fixup! fixup! Add UUID to collection...
    
    Always generate collection UUID 🤦‍♀️
    
    * Raft snapshot recreate collection no expose UUID (#5452)
    
    * separate colleciton config structure from API
    
    * fmt
    
    * Update lib/collection/src/operations/types.rs
    
    Co-authored-by: Tim Visée <tim+github@visee.me>
    
    ---------
    
    Co-authored-by: Tim Visée <tim+github@visee.me>
    
    ---------
    
    Co-authored-by: Andrey Vasnetsov <andrey@vasnetsov.com>
    Co-authored-by: Tim Visée <tim+github@visee.me>

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 68caebeee..bb59234db 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -27,7 +27,7 @@ use super::transfer::ShardTransfer;
 use super::CollectionId;
 use crate::collection::payload_index_schema::PayloadIndexSchema;
 use crate::common::snapshots_manager::SnapshotStorageManager;
-use crate::config::CollectionConfig;
+use crate::config::CollectionConfigInternal;
 use crate::operations::point_ops::{self};
 use crate::operations::shared_storage_config::SharedStorageConfig;
 use crate::operations::types::{CollectionError, CollectionResult, UpdateResult, UpdateStatus};
@@ -98,7 +98,7 @@ pub struct ShardReplicaSet {
     abort_shard_transfer_cb: AbortShardTransfer,
     channel_service: ChannelService,
     collection_id: CollectionId,
-    collection_config: Arc<RwLock<CollectionConfig>>,
+    collection_config: Arc<RwLock<CollectionConfigInternal>>,
     optimizers_config: OptimizersConfig,
     pub(crate) shared_storage_config: Arc<SharedStorageConfig>,
     payload_index_schema: Arc<SaveOnDisk<PayloadIndexSchema>>,
@@ -129,7 +129,7 @@ impl ShardReplicaSet {
         on_peer_failure: ChangePeerFromState,
         abort_shard_transfer: AbortShardTransfer,
         collection_path: &Path,
-        collection_config: Arc<RwLock<CollectionConfig>>,
+        collection_config: Arc<RwLock<CollectionConfigInternal>>,
         effective_optimizers_config: OptimizersConfig,
         shared_storage_config: Arc<SharedStorageConfig>,
         payload_index_schema: Arc<SaveOnDisk<PayloadIndexSchema>>,
@@ -218,7 +218,7 @@ impl ShardReplicaSet {
         shard_id: ShardId,
         collection_id: CollectionId,
         shard_path: &Path,
-        collection_config: Arc<RwLock<CollectionConfig>>,
+        collection_config: Arc<RwLock<CollectionConfigInternal>>,
         effective_optimizers_config: OptimizersConfig,
         shared_storage_config: Arc<SharedStorageConfig>,
         payload_index_schema: Arc<SaveOnDisk<PayloadIndexSchema>>,

commit ed05a46889583d907af996d2559ff89cb81a72ef
Author: Tim Visée <tim+github@visee.me>
Date:   Wed Nov 27 14:57:37 2024 +0100

    Expose shard keys in telemetry (#5522)
    
    * Add shard key to replica set
    
    * Expose shard key in replica set telemetry
    
    * Update OpenAPI specification
    
    * Fix two spelling errors

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index bb59234db..57c08f6a1 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -15,7 +15,7 @@ use std::time::Duration;
 use common::cpu::CpuBudget;
 use common::types::TelemetryDetail;
 use schemars::JsonSchema;
-use segment::types::{ExtendedPointId, Filter};
+use segment::types::{ExtendedPointId, Filter, ShardKey};
 use serde::{Deserialize, Serialize};
 use tokio::runtime::Handle;
 use tokio::sync::{Mutex, RwLock};
@@ -94,6 +94,7 @@ pub struct ShardReplicaSet {
     locally_disabled_peers: parking_lot::RwLock<locally_disabled_peers::Registry>,
     pub(crate) shard_path: PathBuf,
     pub(crate) shard_id: ShardId,
+    shard_key: Option<ShardKey>,
     notify_peer_failure_cb: ChangePeerFromState,
     abort_shard_transfer_cb: AbortShardTransfer,
     channel_service: ChannelService,
@@ -122,6 +123,7 @@ impl ShardReplicaSet {
     #[allow(clippy::too_many_arguments)]
     pub async fn build(
         shard_id: ShardId,
+        shard_key: Option<ShardKey>,
         collection_id: CollectionId,
         this_peer_id: PeerId,
         local: bool,
@@ -187,6 +189,7 @@ impl ShardReplicaSet {
 
         Ok(Self {
             shard_id,
+            shard_key,
             local: RwLock::new(local),
             remotes: RwLock::new(remote_shards),
             replica_state: replica_state.into(),
@@ -216,6 +219,7 @@ impl ShardReplicaSet {
     #[allow(clippy::too_many_arguments)]
     pub async fn load(
         shard_id: ShardId,
+        shard_key: Option<ShardKey>,
         collection_id: CollectionId,
         shard_path: &Path,
         collection_config: Arc<RwLock<CollectionConfigInternal>>,
@@ -304,6 +308,7 @@ impl ShardReplicaSet {
 
         let replica_set = Self {
             shard_id,
+            shard_key,
             local: RwLock::new(local),
             remotes: RwLock::new(remote_shards),
             replica_state: replica_state.into(),
@@ -764,6 +769,7 @@ impl ShardReplicaSet {
 
         ReplicaSetTelemetry {
             id: self.shard_id,
+            key: self.shard_key.clone(),
             local: local_telemetry,
             remote: self
                 .remotes

commit 0702854477ae7b23f3f50d94ea9a4cac167bd612
Author: Jojii <15957865+JojiiOfficial@users.noreply.github.com>
Date:   Thu Dec 5 17:28:09 2024 +0100

    Strict mode max collection vector size (#5501)
    
    * Strict mode config: Max collection size
    
    * api specs
    
    * Add tests + set/update payload check
    
    * Improve function names and add comments
    
    * rename config to separate vectors and payload
    
    * fix tests
    
    * Adjust configs docs
    
    * add benchmark
    
    * improve performance by caching shard info
    
    * add bench for size_info() and fix tests
    
    * Also limit the batch-size for vector updates (#5508)
    
    * Also limit the batch-size for vector updates
    
    * clippy
    
    * add lost commit
    
    * Load cache on collection initialization
    
    * add unit type to parameter name
    
    * fix renaming in test
    
    * clearer error message
    
    * fix test
    
    * review remarks
    
    * remove unused function for now
    
    ---------
    
    Co-authored-by: Arnaud Gourlay <arnaud.gourlay@gmail.com>

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 57c08f6a1..b75d420e2 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -26,6 +26,7 @@ use super::remote_shard::RemoteShard;
 use super::transfer::ShardTransfer;
 use super::CollectionId;
 use crate::collection::payload_index_schema::PayloadIndexSchema;
+use crate::common::local_data_stats::LocalDataStats;
 use crate::common::snapshots_manager::SnapshotStorageManager;
 use crate::config::CollectionConfigInternal;
 use crate::operations::point_ops::{self};
@@ -1010,6 +1011,35 @@ impl ShardReplicaSet {
         shard.trigger_optimizers();
         true
     }
+
+    /// Returns the estimated size of all locally stored vectors in bytes.
+    /// Locks and iterates over all segments.
+    /// Cache this value in performance critical scenarios!
+    pub(crate) async fn calculate_local_shards_stats(&self) -> LocalDataStats {
+        self.local
+            .read()
+            .await
+            .as_ref()
+            .map(|i| match i {
+                Shard::Local(local) => {
+                    let mut total_vector_size = 0;
+
+                    for segment in local.segments.read().iter() {
+                        let size_info = segment.1.get().read().size_info();
+                        total_vector_size += size_info.vectors_size_bytes;
+                    }
+
+                    LocalDataStats {
+                        vector_storage_size: total_vector_size,
+                    }
+                }
+                Shard::Proxy(_)
+                | Shard::ForwardProxy(_)
+                | Shard::QueueProxy(_)
+                | Shard::Dummy(_) => LocalDataStats::default(),
+            })
+            .unwrap_or_default()
+    }
 }
 
 /// Represents a replica set state

commit 3c19eea9ca435304c6ea5b42c1000eb52e9ee7ca
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Fri Dec 6 11:02:18 2024 +0100

    Rate limiting for shard operations (#5582)
    
    * Rate limiting for shard operations
    
    * address all review comments in one go

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index b75d420e2..fd1e488d7 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -730,7 +730,15 @@ impl ShardReplicaSet {
         }
     }
 
-    /// Check if the are any locally disabled peers
+    pub(crate) async fn on_strict_mode_config_update(&self) -> CollectionResult<()> {
+        let read_local = self.local.read().await;
+        if let Some(shard) = &*read_local {
+            shard.on_strict_mode_config_update().await
+        }
+        Ok(())
+    }
+
+    /// Check if there are any locally disabled peers
     /// And if so, report them to the consensus
     pub fn sync_local_state<F>(&self, get_shard_transfers: F) -> CollectionResult<()>
     where

commit f5bec253b6e1d9931dec902b0ccaa68475cad05f
Author: Tim Visée <tim+github@visee.me>
Date:   Mon Dec 9 15:33:26 2024 +0100

    Minor refactoring during testing (#5610)

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index fd1e488d7..fb1270c66 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -529,8 +529,8 @@ impl ShardReplicaSet {
         if !self.replica_state.read().is_local || state.is_some() {
             self.replica_state.write(|rs| {
                 rs.is_local = true;
-                if let Some(active) = state {
-                    rs.set_peer_state(self.this_peer_id(), active);
+                if let Some(state) = state {
+                    rs.set_peer_state(self.this_peer_id(), state);
                 }
             })?;
         }

commit 947e79e054977921b0167474bdf08e1b7d67be12
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Tue Dec 17 17:33:46 2024 +0100

    Strict mode for payload storage (#5588)
    
    * Strict mode for payload storage
    
    * Don't increment counter multiple times per request
    
    * Add (loose) integration tests for payload storage limit
    
    * Minor improvements
    
    * minor renaming
    
    * Update lib/api/src/grpc/proto/collections.proto
    
    Co-authored-by: Tim Visée <tim+github@visee.me>
    
    ---------
    
    Co-authored-by: jojii <jojii@gmx.net>
    Co-authored-by: Jojii <15957865+JojiiOfficial@users.noreply.github.com>
    Co-authored-by: Tim Visée <tim+github@visee.me>

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index fb1270c66..e2eef18c4 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -1031,14 +1031,17 @@ impl ShardReplicaSet {
             .map(|i| match i {
                 Shard::Local(local) => {
                     let mut total_vector_size = 0;
+                    let mut total_payload_size = 0;
 
                     for segment in local.segments.read().iter() {
                         let size_info = segment.1.get().read().size_info();
                         total_vector_size += size_info.vectors_size_bytes;
+                        total_payload_size += size_info.payloads_size_bytes;
                     }
 
                     LocalDataStats {
                         vector_storage_size: total_vector_size,
+                        payload_storage_size: total_payload_size,
                     }
                 }
                 Shard::Proxy(_)

commit bf4cae877836bd631a4543f696e7e32c84eff856
Author: Jojii <15957865+JojiiOfficial@users.noreply.github.com>
Date:   Thu Dec 19 16:38:30 2024 +0100

    [Strict Mode] Max collection size in distributed setup (#5592)
    
    * Strict Mode: distributed checking of max collection size
    
    * add size projections in distributed mode
    
    * Add consensus tests
    
    * New Test: All nodes in cluster
    
    * fix tests
    
    * Update lib/collection/src/collection/mod.rs
    
    Co-authored-by: Arnaud Gourlay <arnaud.gourlay@gmail.com>
    
    * increase upsert delay
    
    * add TODO for resharding
    
    * wait for strict mode config to be applied on second node
    
    * remove delays
    
    * Also wait for strict mode in other test
    
    * clearify strict mode config option
    
    ---------
    
    Co-authored-by: Arnaud Gourlay <arnaud.gourlay@gmail.com>

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index e2eef18c4..4f8356973 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -26,7 +26,7 @@ use super::remote_shard::RemoteShard;
 use super::transfer::ShardTransfer;
 use super::CollectionId;
 use crate::collection::payload_index_schema::PayloadIndexSchema;
-use crate::common::local_data_stats::LocalDataStats;
+use crate::common::collection_size_stats::CollectionSizeStats;
 use crate::common::snapshots_manager::SnapshotStorageManager;
 use crate::config::CollectionConfigInternal;
 use crate::operations::point_ops::{self};
@@ -345,6 +345,10 @@ impl ShardReplicaSet {
         self.replica_state.read().this_peer_id
     }
 
+    pub async fn has_remote_shard(&self) -> bool {
+        !self.remotes.read().await.is_empty()
+    }
+
     pub async fn has_local_shard(&self) -> bool {
         self.local.read().await.is_some()
     }
@@ -1020,10 +1024,9 @@ impl ShardReplicaSet {
         true
     }
 
-    /// Returns the estimated size of all locally stored vectors in bytes.
-    /// Locks and iterates over all segments.
-    /// Cache this value in performance critical scenarios!
-    pub(crate) async fn calculate_local_shards_stats(&self) -> LocalDataStats {
+    /// Returns the estimated size of all local segments.
+    /// Since this locks all segments you should cache this value in performance critical scenarios!
+    pub(crate) async fn calculate_local_shard_stats(&self) -> Option<CollectionSizeStats> {
         self.local
             .read()
             .await
@@ -1039,15 +1042,15 @@ impl ShardReplicaSet {
                         total_payload_size += size_info.payloads_size_bytes;
                     }
 
-                    LocalDataStats {
+                    Some(CollectionSizeStats {
                         vector_storage_size: total_vector_size,
                         payload_storage_size: total_payload_size,
-                    }
+                    })
                 }
                 Shard::Proxy(_)
                 | Shard::ForwardProxy(_)
                 | Shard::QueueProxy(_)
-                | Shard::Dummy(_) => LocalDataStats::default(),
+                | Shard::Dummy(_) => None,
             })
             .unwrap_or_default()
     }

commit d3f2b8e79c362b5eb4351d0266382598c3600f08
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Fri Dec 20 12:59:07 2024 +0100

    Fix rate limiting of internal update operations (#5653)
    
    * Fix rate limiting of internal update operations
    
    * code review
    
    * write_rate_limiter turned Option<Mutex> and fix disabling mode
    
    * Update TODO tag
    
    ---------
    
    Co-authored-by: Tim Visée <tim+github@visee.me>

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 4f8356973..ea4e711fc 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -13,6 +13,7 @@ use std::sync::Arc;
 use std::time::Duration;
 
 use common::cpu::CpuBudget;
+use common::rate_limiting::RateLimiter;
 use common::types::TelemetryDetail;
 use schemars::JsonSchema;
 use segment::types::{ExtendedPointId, Filter, ShardKey};
@@ -111,6 +112,7 @@ pub struct ShardReplicaSet {
     write_ordering_lock: Mutex<()>,
     /// Local clock set, used to tag new operations on this shard.
     clock_set: Mutex<ClockSet>,
+    write_rate_limiter: Option<parking_lot::Mutex<RateLimiter>>,
 }
 
 pub type AbortShardTransfer = Arc<dyn Fn(ShardTransfer, &str) + Send + Sync>;
@@ -188,6 +190,16 @@ impl ShardReplicaSet {
         let replica_set_shard_config = ShardConfig::new_replica_set();
         replica_set_shard_config.save(&shard_path)?;
 
+        // Initialize the write rate limiter
+        let config = collection_config.read().await;
+        let write_rate_limiter = config.strict_mode_config.as_ref().and_then(|strict_mode| {
+            strict_mode
+                .write_rate_limit
+                .map(RateLimiter::new_per_minute)
+                .map(parking_lot::Mutex::new)
+        });
+        drop(config);
+
         Ok(Self {
             shard_id,
             shard_key,
@@ -209,6 +221,7 @@ impl ShardReplicaSet {
             optimizer_cpu_budget,
             write_ordering_lock: Mutex::new(()),
             clock_set: Default::default(),
+            write_rate_limiter,
         })
     }
 
@@ -307,6 +320,16 @@ impl ShardReplicaSet {
             None
         };
 
+        // Initialize the write rate limiter
+        let config = collection_config.read().await;
+        let write_rate_limiter = config.strict_mode_config.as_ref().and_then(|strict_mode| {
+            strict_mode
+                .write_rate_limit
+                .map(RateLimiter::new_per_minute)
+                .map(parking_lot::Mutex::new)
+        });
+        drop(config);
+
         let replica_set = Self {
             shard_id,
             shard_key,
@@ -329,6 +352,7 @@ impl ShardReplicaSet {
             optimizer_cpu_budget,
             write_ordering_lock: Mutex::new(()),
             clock_set: Default::default(),
+            write_rate_limiter,
         };
 
         if local_load_failure && replica_set.active_remote_shards().is_empty() {
@@ -734,11 +758,43 @@ impl ShardReplicaSet {
         }
     }
 
-    pub(crate) async fn on_strict_mode_config_update(&self) -> CollectionResult<()> {
+    /// Apply shard's strict mode configuration update
+    /// - Update read and write rate limiters
+    pub(crate) async fn on_strict_mode_config_update(&mut self) -> CollectionResult<()> {
         let read_local = self.local.read().await;
         if let Some(shard) = &*read_local {
+            // TODO(ratelimiting) take &mut self and use Option<Mutex> for read_rate_limiter
             shard.on_strict_mode_config_update().await
         }
+        let config = self.collection_config.read().await;
+        if let Some(strict_mode_config) = &config.strict_mode_config {
+            if strict_mode_config.enabled == Some(true) {
+                // update write rate limiter
+                if let Some(write_rate_limit_per_min) = strict_mode_config.write_rate_limit {
+                    let new_write_rate_limiter =
+                        RateLimiter::new_per_minute(write_rate_limit_per_min);
+                    self.write_rate_limiter
+                        .replace(parking_lot::Mutex::new(new_write_rate_limiter));
+                    return Ok(());
+                }
+            }
+        }
+        // remove write rate limiter for all other situations
+        self.write_rate_limiter.take();
+        Ok(())
+    }
+
+    /// Check if the write rate limiter allows the operation to proceed
+    ///
+    /// Returns an error if the rate limit is exceeded.
+    fn check_write_rate_limiter(&self) -> CollectionResult<()> {
+        if let Some(rate_limiter) = &self.write_rate_limiter {
+            if !rate_limiter.lock().check_and_update() {
+                return Err(CollectionError::RateLimitExceeded {
+                    description: "Write rate limit exceeded, retry later".to_string(),
+                });
+            }
+        }
         Ok(())
     }
 

commit 3d3f8c104e35f0f4e393ed09d61cc063a759fabe
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Fri Dec 20 16:56:17 2024 +0100

    Reshape read rate limiter and fix disabled mode (#5683)

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index ea4e711fc..046d9a154 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -761,11 +761,11 @@ impl ShardReplicaSet {
     /// Apply shard's strict mode configuration update
     /// - Update read and write rate limiters
     pub(crate) async fn on_strict_mode_config_update(&mut self) -> CollectionResult<()> {
-        let read_local = self.local.read().await;
-        if let Some(shard) = &*read_local {
-            // TODO(ratelimiting) take &mut self and use Option<Mutex> for read_rate_limiter
+        let mut read_local = self.local.write().await;
+        if let Some(shard) = read_local.as_mut() {
             shard.on_strict_mode_config_update().await
         }
+        drop(read_local);
         let config = self.collection_config.read().await;
         if let Some(strict_mode_config) = &config.strict_mode_config {
             if strict_mode_config.enabled == Some(true) {

commit 237ff8ca13d4e6a0f477533eb742b7f8b878e689
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Mon Dec 23 11:22:13 2024 +0100

    Read rate limiter handles batched search requests (#5685)
    
    * Read rate limiter handles batched search requests
    
    * Show different rate limiter if big request can never pass
    
    * Improve rate limiter error reporting
    
    ---------
    
    Co-authored-by: timvisee <tim@visee.me>

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 046d9a154..9f7a8bb1f 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -785,14 +785,23 @@ impl ShardReplicaSet {
     }
 
     /// Check if the write rate limiter allows the operation to proceed
+    /// - cost: the cost of the operation
     ///
     /// Returns an error if the rate limit is exceeded.
-    fn check_write_rate_limiter(&self) -> CollectionResult<()> {
+    fn check_write_rate_limiter(&self, cost: usize) -> CollectionResult<()> {
         if let Some(rate_limiter) = &self.write_rate_limiter {
-            if !rate_limiter.lock().check_and_update() {
-                return Err(CollectionError::RateLimitExceeded {
-                    description: "Write rate limit exceeded, retry later".to_string(),
-                });
+            match rate_limiter.lock().try_consume(cost as f64) {
+                Ok(true) => {}
+                Ok(false) => {
+                    return Err(CollectionError::rate_limit_exceeded(
+                        "Write rate limit exceeded, retry later",
+                    ));
+                }
+                Err(msg) => {
+                    return Err(CollectionError::rate_limit_exceeded(format!(
+                        "Write rate limit exceeded, {msg}",
+                    )));
+                }
             }
         }
         Ok(())

commit 69b7e0e4735c4f1cf4012caac553e2b8f2453c00
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Tue Dec 24 13:58:32 2024 +0100

    Abort scale-down resharding on update error (#5670)

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 9f7a8bb1f..1355dc536 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -355,6 +355,7 @@ impl ShardReplicaSet {
             write_rate_limiter,
         };
 
+        // `active_remote_shards` includes `Active` and `ReshardingScaleDown` replicas!
         if local_load_failure && replica_set.active_remote_shards().is_empty() {
             replica_set
                 .locally_disabled_peers
@@ -397,6 +398,7 @@ impl ShardReplicaSet {
     }
 
     pub fn is_last_active_replica(&self, peer_id: PeerId) -> bool {
+        // This includes `Active` and `ReshardingScaleDown` replicas!
         let active_peers = self.replica_state.read().active_peers();
         active_peers.len() == 1 && active_peers.contains(&peer_id)
     }
@@ -409,6 +411,7 @@ impl ShardReplicaSet {
     pub fn active_shards(&self) -> Vec<PeerId> {
         let replica_state = self.replica_state.read();
         replica_state
+            // This is a part of deprecated built-in resharding implementation, so we don't care
             .active_peers()
             .into_iter()
             .filter(|&peer_id| !self.is_locally_disabled(peer_id))
@@ -420,7 +423,7 @@ impl ShardReplicaSet {
         let replica_state = self.replica_state.read();
         let this_peer_id = replica_state.this_peer_id;
         replica_state
-            .active_peers()
+            .active_peers() // This includes `Active` and `ReshardingScaleDown` replicas!
             .into_iter()
             .filter(|&peer_id| !self.is_locally_disabled(peer_id) && peer_id != this_peer_id)
             .collect()
@@ -716,8 +719,11 @@ impl ShardReplicaSet {
                     self.optimizers_config.clone(),
                 )
                 .await?;
+
                 match state {
-                    ReplicaState::Active | ReplicaState::Listener => {
+                    ReplicaState::Active
+                    | ReplicaState::Listener
+                    | ReplicaState::ReshardingScaleDown => {
                         // No way we can provide up-to-date replica right away at this point,
                         // so we report a failure to consensus
                         self.set_local(local_shard, Some(state)).await?;
@@ -733,6 +739,7 @@ impl ShardReplicaSet {
                         self.set_local(local_shard, Some(state)).await?;
                     }
                 }
+
                 continue;
             }
 
@@ -962,13 +969,22 @@ impl ShardReplicaSet {
     /// Check whether a peer is registered as `active`.
     /// Unknown peers are not active.
     fn peer_is_active(&self, peer_id: PeerId) -> bool {
-        self.peer_state(peer_id) == Some(ReplicaState::Active) && !self.is_locally_disabled(peer_id)
+        // This is used *exclusively* during `execute_*_read_operation`, and so it *should* consider
+        // `ReshardingScaleDown` replicas
+        let is_active = matches!(
+            self.peer_state(peer_id),
+            Some(ReplicaState::Active | ReplicaState::ReshardingScaleDown)
+        );
+
+        is_active && !self.is_locally_disabled(peer_id)
     }
 
     fn peer_is_active_or_resharding(&self, peer_id: PeerId) -> bool {
         let is_active_or_resharding = matches!(
             self.peer_state(peer_id),
-            Some(ReplicaState::Active | ReplicaState::Resharding)
+            Some(
+                ReplicaState::Active | ReplicaState::Resharding | ReplicaState::ReshardingScaleDown
+            )
         );
 
         let is_locally_disabled = self.is_locally_disabled(peer_id);
@@ -1150,14 +1166,23 @@ impl ReplicaSetState {
         self.peers
             .iter()
             .filter_map(|(peer_id, state)| {
-                matches!(state, ReplicaState::Active).then_some(*peer_id)
+                // We consider `ReshardingScaleDown` to be `Active`!
+                matches!(
+                    state,
+                    ReplicaState::Active | ReplicaState::ReshardingScaleDown
+                )
+                .then_some(*peer_id)
             })
             .collect()
     }
 
     pub fn active_or_resharding_peers(&self) -> impl Iterator<Item = PeerId> + '_ {
         self.peers.iter().filter_map(|(peer_id, state)| {
-            matches!(state, ReplicaState::Active | ReplicaState::Resharding).then_some(*peer_id)
+            matches!(
+                state,
+                ReplicaState::Active | ReplicaState::Resharding | ReplicaState::ReshardingScaleDown
+            )
+            .then_some(*peer_id)
         })
     }
 
@@ -1190,16 +1215,38 @@ pub enum ReplicaState {
     // Shard is undergoing recovery by an external node
     // Normally rejects updates, accepts updates if force is true
     Recovery,
-    // Points are being migrated to this shard as part of resharding
+    // Points are being migrated to this shard as part of resharding up
     #[schemars(skip)]
     Resharding,
+    // Points are being migrated to this shard as part of resharding down
+    #[schemars(skip)]
+    ReshardingScaleDown,
 }
 
 impl ReplicaState {
+    /// Check if replica state is active
+    pub fn is_active(self) -> bool {
+        match self {
+            ReplicaState::Active => true,
+            ReplicaState::ReshardingScaleDown => true,
+
+            ReplicaState::Dead => false,
+            ReplicaState::Partial => false,
+            ReplicaState::Initializing => false,
+            ReplicaState::Listener => false,
+            ReplicaState::PartialSnapshot => false,
+            ReplicaState::Recovery => false,
+            ReplicaState::Resharding => false,
+        }
+    }
+
     /// Check whether the replica state is active or listener or resharding.
     pub fn is_active_or_listener_or_resharding(self) -> bool {
         match self {
-            ReplicaState::Active | ReplicaState::Listener | ReplicaState::Resharding => true,
+            ReplicaState::Active
+            | ReplicaState::Listener
+            | ReplicaState::Resharding
+            | ReplicaState::ReshardingScaleDown => true,
 
             ReplicaState::Dead
             | ReplicaState::Initializing
@@ -1212,12 +1259,15 @@ impl ReplicaState {
     /// Check whether the replica state is partial or partial-like.
     ///
     /// In other words: is the state related to shard transfers?
+    //
+    // TODO(resharding): What's the best way to handle `ReshardingScaleDown` properly!?
     pub fn is_partial_or_recovery(self) -> bool {
         match self {
             ReplicaState::Partial
             | ReplicaState::PartialSnapshot
             | ReplicaState::Recovery
-            | ReplicaState::Resharding => true,
+            | ReplicaState::Resharding
+            | ReplicaState::ReshardingScaleDown => true,
 
             ReplicaState::Active
             | ReplicaState::Dead

commit 38a951e938fa9ba524c9b5749d26bba20624a917
Author: Tim Visée <tim+github@visee.me>
Date:   Thu Jan 2 17:27:45 2025 +0100

    Expose the PartialSnapshot replica state (#5715)

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 1355dc536..ba6a9c1f6 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -1210,7 +1210,6 @@ pub enum ReplicaState {
     //
     // Snapshot shard transfer is in progress, updates aren't sent to the shard
     // Normally rejects updates. Since 1.8 it allows updates if force is true.
-    #[schemars(skip)]
     PartialSnapshot,
     // Shard is undergoing recovery by an external node
     // Normally rejects updates, accepts updates if force is true

commit cb8667a7ae571cc42d29a381375824bbf60461b2
Author: Jojii <15957865+JojiiOfficial@users.noreply.github.com>
Date:   Wed Jan 8 18:12:10 2025 +0100

    Merge pull request #5729
    
    * Consider filter-cardinality as cost in update ratelimiter
    
    * Use OperationEffectArea and improve rate limit error message
    
    * Clippy
    
    * Fix test

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index ba6a9c1f6..9886adb20 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -797,23 +797,28 @@ impl ShardReplicaSet {
     /// Returns an error if the rate limit is exceeded.
     fn check_write_rate_limiter(&self, cost: usize) -> CollectionResult<()> {
         if let Some(rate_limiter) = &self.write_rate_limiter {
-            match rate_limiter.lock().try_consume(cost as f64) {
-                Ok(true) => {}
-                Ok(false) => {
-                    return Err(CollectionError::rate_limit_exceeded(
-                        "Write rate limit exceeded, retry later",
-                    ));
-                }
-                Err(msg) => {
-                    return Err(CollectionError::rate_limit_exceeded(format!(
-                        "Write rate limit exceeded, {msg}",
-                    )));
-                }
-            }
+            rate_limiter
+                .lock()
+                .try_consume(cost as f64)
+                .map_err(|err| CollectionError::rate_limit_error(err, Some(cost), true))?;
         }
         Ok(())
     }
 
+    /// Check if the write rate limiter allows the operation to proceed
+    /// - cost_fn: the cost function of the operation, evaluated lazily
+    ///
+    /// Returns an error if the rate limit is exceeded.
+    fn check_write_rate_limiter_lazy<F>(&self, cost_fn: F) -> CollectionResult<()>
+    where
+        F: Fn() -> usize,
+    {
+        if self.write_rate_limiter.is_some() {
+            self.check_write_rate_limiter(cost_fn())?;
+        };
+        Ok(())
+    }
+
     /// Check if there are any locally disabled peers
     /// And if so, report them to the consensus
     pub fn sync_local_state<F>(&self, get_shard_transfers: F) -> CollectionResult<()>

commit cfff4c74e0b6f7a607b939c40ad3d57bd19e5573
Author: Tim Visée <tim+github@visee.me>
Date:   Mon Jan 13 19:13:07 2025 +0100

    Resharding: wait on clean up deletes, exact counting in tests (#5787)
    
    * During resharding tests, assert exact counts
    
    * Wait on delete operation while cleaning up points

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 9886adb20..0cfbbf647 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -940,7 +940,7 @@ impl ShardReplicaSet {
             });
 
         // TODO(resharding): Assign clock tag to the operation!? 🤔
-        let result = self.update_local(op.into(), false).await?.ok_or_else(|| {
+        let result = self.update_local(op.into(), true).await?.ok_or_else(|| {
             CollectionError::bad_request(format!(
                 "local shard {}:{} does not exist or is unavailable",
                 self.collection_id, self.shard_id,

commit 2b7fdffb3add8d5773069de44665665866b28ea7
Author: Tim Visée <tim+github@visee.me>
Date:   Tue Jan 14 09:15:11 2025 +0100

    Resharding: expose types (#5718)
    
    * Expose resharding related types
    
    * Still hide some fields, end users are not intended to use them
    
    * Remove unused JsonSchema derives

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 0cfbbf647..c687459d1 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -1220,10 +1220,8 @@ pub enum ReplicaState {
     // Normally rejects updates, accepts updates if force is true
     Recovery,
     // Points are being migrated to this shard as part of resharding up
-    #[schemars(skip)]
     Resharding,
     // Points are being migrated to this shard as part of resharding down
-    #[schemars(skip)]
     ReshardingScaleDown,
 }
 

commit 38f478ddf7a9d03a1c783c5599f3b6ae33a05195
Author: Jojii <15957865+JojiiOfficial@users.noreply.github.com>
Date:   Thu Jan 16 14:25:55 2025 +0100

    Measure payload read IO (#5773)
    
    * Measure read io for payload storage
    
    * Add Hardware Counter to update functions
    
    * Fix tests and benches
    
    * Rename (some) *_measured functions back to original

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index c687459d1..304dca5ec 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -12,6 +12,7 @@ use std::path::{Path, PathBuf};
 use std::sync::Arc;
 use std::time::Duration;
 
+use common::counter::hardware_accumulator::HwMeasurementAcc;
 use common::cpu::CpuBudget;
 use common::rate_limiting::RateLimiter;
 use common::types::TelemetryDetail;
@@ -886,7 +887,11 @@ impl ShardReplicaSet {
         Ok(())
     }
 
-    pub async fn delete_local_points(&self, filter: Filter) -> CollectionResult<UpdateResult> {
+    pub async fn delete_local_points(
+        &self,
+        filter: Filter,
+        hw_measurement_acc: HwMeasurementAcc,
+    ) -> CollectionResult<UpdateResult> {
         let local_shard_guard = self.local.read().await;
 
         let Some(local_shard) = local_shard_guard.deref() else {
@@ -912,6 +917,7 @@ impl ShardReplicaSet {
                     &self.search_runtime,
                     None,
                     None,
+                    hw_measurement_acc.clone(),
                 )
                 .await?;
 

commit 27d5b39aee992d26a95a3c6ff4e6fceba1bf89ad
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Mon Feb 10 11:27:18 2025 +0100

    Retry-After on rate limiter hit for REST API (#5917)
    
    * Retry-After on rate limiter hit for REST API
    
    * fix tests
    
    * Ceil retry after seconds number so clients don't retry too early
    
    * Relax retry after time requirement a bit in case CI machines are slow
    
    * Add rate limit test, test retry after if we replanish within a second
    
    * Report time to wait in rate limit error
    
    ---------
    
    Co-authored-by: timvisee <tim@visee.me>

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 304dca5ec..634f10dcc 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -801,7 +801,7 @@ impl ShardReplicaSet {
             rate_limiter
                 .lock()
                 .try_consume(cost as f64)
-                .map_err(|err| CollectionError::rate_limit_error(err, Some(cost), true))?;
+                .map_err(|err| CollectionError::rate_limit_error(err, cost, true))?;
         }
         Ok(())
     }

commit cb1f3220df572321cf0784835ed8476afa1779b7
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Wed Feb 12 11:10:51 2025 +0100

    Add strict mode for max points count (#5967)
    
    * Add strict mode for max points count
    
    * clarify estimated nature

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 634f10dcc..d7b8dff47 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -1127,16 +1127,19 @@ impl ShardReplicaSet {
                 Shard::Local(local) => {
                     let mut total_vector_size = 0;
                     let mut total_payload_size = 0;
+                    let mut total_points = 0;
 
                     for segment in local.segments.read().iter() {
                         let size_info = segment.1.get().read().size_info();
                         total_vector_size += size_info.vectors_size_bytes;
                         total_payload_size += size_info.payloads_size_bytes;
+                        total_points += size_info.num_points;
                     }
 
                     Some(CollectionSizeStats {
                         vector_storage_size: total_vector_size,
                         payload_storage_size: total_payload_size,
+                        points_count: total_points,
                     })
                 }
                 Shard::Proxy(_)

commit caed5729e5b7ff3db9dcb4531a4af0929b186682
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Thu Feb 20 09:05:00 2025 +0100

    IO resource usage permit (#6015)
    
    * rename cpu_budget -> resource_budget
    
    * clippy
    
    * add io budget to resources
    
    * fmt
    
    * move budget structures into a separate file
    
    * add extend permit function
    
    * dont extend existing permit
    
    * switch from IO to CPU permit
    
    * do not release resource before aquiring an extension
    
    * fmt
    
    * Review remarks
    
    * Improve resource permit number assertion
    
    * Make resource permit replace_with only acquire extra needed permits
    
    * Remove obsolete drop implementation
    
    * allocate IO budget same as CPU
    
    * review fixes
    
    ---------
    
    Co-authored-by: timvisee <tim@visee.me>

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index d7b8dff47..74b2777da 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -12,8 +12,8 @@ use std::path::{Path, PathBuf};
 use std::sync::Arc;
 use std::time::Duration;
 
+use common::budget::ResourceBudget;
 use common::counter::hardware_accumulator::HwMeasurementAcc;
-use common::cpu::CpuBudget;
 use common::rate_limiting::RateLimiter;
 use common::types::TelemetryDetail;
 use schemars::JsonSchema;
@@ -108,7 +108,7 @@ pub struct ShardReplicaSet {
     payload_index_schema: Arc<SaveOnDisk<PayloadIndexSchema>>,
     update_runtime: Handle,
     search_runtime: Handle,
-    optimizer_cpu_budget: CpuBudget,
+    optimizer_resource_budget: ResourceBudget,
     /// Lock to serialized write operations on the replicaset when a write ordering is used.
     write_ordering_lock: Mutex<()>,
     /// Local clock set, used to tag new operations on this shard.
@@ -142,7 +142,7 @@ impl ShardReplicaSet {
         channel_service: ChannelService,
         update_runtime: Handle,
         search_runtime: Handle,
-        optimizer_cpu_budget: CpuBudget,
+        optimizer_resource_budget: ResourceBudget,
         init_state: Option<ReplicaState>,
     ) -> CollectionResult<Self> {
         let shard_path = super::create_shard_dir(collection_path, shard_id).await?;
@@ -156,7 +156,7 @@ impl ShardReplicaSet {
                 payload_index_schema.clone(),
                 update_runtime.clone(),
                 search_runtime.clone(),
-                optimizer_cpu_budget.clone(),
+                optimizer_resource_budget.clone(),
                 effective_optimizers_config.clone(),
             )
             .await?;
@@ -219,7 +219,7 @@ impl ShardReplicaSet {
             payload_index_schema,
             update_runtime,
             search_runtime,
-            optimizer_cpu_budget,
+            optimizer_resource_budget,
             write_ordering_lock: Mutex::new(()),
             clock_set: Default::default(),
             write_rate_limiter,
@@ -247,7 +247,7 @@ impl ShardReplicaSet {
         this_peer_id: PeerId,
         update_runtime: Handle,
         search_runtime: Handle,
-        optimizer_cpu_budget: CpuBudget,
+        optimizer_resource_budget: ResourceBudget,
     ) -> Self {
         let replica_state: SaveOnDisk<ReplicaSetState> =
             SaveOnDisk::load_or_init_default(shard_path.join(REPLICA_STATE_FILE)).unwrap();
@@ -290,7 +290,7 @@ impl ShardReplicaSet {
                     payload_index_schema.clone(),
                     update_runtime.clone(),
                     search_runtime.clone(),
-                    optimizer_cpu_budget.clone(),
+                    optimizer_resource_budget.clone(),
                 )
                 .await;
 
@@ -350,7 +350,7 @@ impl ShardReplicaSet {
             payload_index_schema,
             update_runtime,
             search_runtime,
-            optimizer_cpu_budget,
+            optimizer_resource_budget,
             write_ordering_lock: Mutex::new(()),
             clock_set: Default::default(),
             write_rate_limiter,
@@ -530,7 +530,7 @@ impl ShardReplicaSet {
             self.payload_index_schema.clone(),
             self.update_runtime.clone(),
             self.search_runtime.clone(),
-            self.optimizer_cpu_budget.clone(),
+            self.optimizer_resource_budget.clone(),
             self.optimizers_config.clone(),
         )
         .await;
@@ -716,7 +716,7 @@ impl ShardReplicaSet {
                     self.payload_index_schema.clone(),
                     self.update_runtime.clone(),
                     self.search_runtime.clone(),
-                    self.optimizer_cpu_budget.clone(),
+                    self.optimizer_resource_budget.clone(),
                     self.optimizers_config.clone(),
                 )
                 .await?;

commit 8ad2b34265448ec01b89d4093de5fbb1a86dcd4d
Author: Tim Visée <tim+github@visee.me>
Date:   Tue Feb 25 11:21:25 2025 +0100

    Bump Rust edition to 2024 (#6042)
    
    * Bump Rust edition to 2024
    
    * gen is a reserved keyword now
    
    * Remove ref mut on references
    
    * Mark extern C as unsafe
    
    * Wrap unsafe function bodies in unsafe block
    
    * Geo hash implements Copy, don't reference but pass by value instead
    
    * Replace secluded self import with parent
    
    * Update execute_cluster_read_operation with new match semantics
    
    * Fix lifetime issue
    
    * Replace map_or with is_none_or
    
    * set_var is unsafe now
    
    * Reformat

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 74b2777da..479714403 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -22,19 +22,18 @@ use serde::{Deserialize, Serialize};
 use tokio::runtime::Handle;
 use tokio::sync::{Mutex, RwLock};
 
-use super::local_shard::clock_map::RecoveryPoint;
+use super::CollectionId;
 use super::local_shard::LocalShard;
+use super::local_shard::clock_map::RecoveryPoint;
 use super::remote_shard::RemoteShard;
 use super::transfer::ShardTransfer;
-use super::CollectionId;
 use crate::collection::payload_index_schema::PayloadIndexSchema;
 use crate::common::collection_size_stats::CollectionSizeStats;
 use crate::common::snapshots_manager::SnapshotStorageManager;
 use crate::config::CollectionConfigInternal;
-use crate::operations::point_ops::{self};
 use crate::operations::shared_storage_config::SharedStorageConfig;
 use crate::operations::types::{CollectionError, CollectionResult, UpdateResult, UpdateStatus};
-use crate::operations::CollectionUpdateOperations;
+use crate::operations::{CollectionUpdateOperations, point_ops};
 use crate::optimizers_builder::OptimizersConfig;
 use crate::save_on_disk::SaveOnDisk;
 use crate::shards::channel_service::ChannelService;

commit 78f0428f3e23b41cb5702b7aa6caab5564f4af26
Author: Jojii <15957865+JojiiOfficial@users.noreply.github.com>
Date:   Thu Mar 6 15:03:23 2025 +0100

    Measure hardware IO for update operations (#5922)
    
    * Measure update operations hardware IO
    
    * Add support for distributed setups
    
    * also measure update_local
    
    * Add consensus tests for HW metrics of update operations
    
    * add test for upserting without waiting
    
    * Disable HW usage reporting when not waiting for update API
    
    * Review remarks
    
    * Fix resharding collecting hw measurements
    
    * Fix metric type
    
    * New struct HardwareData for better accumulation
    
    * Ensure we always apply CPU multiplier
    
    * Apply suggestions from code review
    
    * Update src/actix/api/update_api.rs
    
    Co-authored-by: Tim Visée <tim+github@visee.me>
    
    * Fix assert_with_upper_bound_error threshold calculation.
    
    * Clarifying why we don't measure shard cleanup
    
    ---------
    
    Co-authored-by: Tim Visée <tim+github@visee.me>

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 479714403..718542512 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -945,12 +945,15 @@ impl ShardReplicaSet {
             });
 
         // TODO(resharding): Assign clock tag to the operation!? 🤔
-        let result = self.update_local(op.into(), true).await?.ok_or_else(|| {
-            CollectionError::bad_request(format!(
-                "local shard {}:{} does not exist or is unavailable",
-                self.collection_id, self.shard_id,
-            ))
-        })?;
+        let result = self
+            .update_local(op.into(), true, hw_measurement_acc)
+            .await?
+            .ok_or_else(|| {
+                CollectionError::bad_request(format!(
+                    "local shard {}:{} does not exist or is unavailable",
+                    self.collection_id, self.shard_id,
+                ))
+            })?;
 
         Ok(result)
     }
@@ -1285,6 +1288,21 @@ impl ReplicaState {
             | ReplicaState::Listener => false,
         }
     }
+
+    /// Returns `true` if the replica state is resharding, either up or down.
+    pub fn is_resharding(&self) -> bool {
+        match self {
+            ReplicaState::Resharding | ReplicaState::ReshardingScaleDown => true,
+
+            ReplicaState::Partial
+            | ReplicaState::PartialSnapshot
+            | ReplicaState::Recovery
+            | ReplicaState::Active
+            | ReplicaState::Dead
+            | ReplicaState::Initializing
+            | ReplicaState::Listener => false,
+        }
+    }
 }
 
 /// Represents a change in replica set, due to scaling of `replication_factor`

commit 92b9d11506766b933aa43029d53c55cd3d39ebcd
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Tue Mar 11 11:21:04 2025 +0100

    Do not rate limit resharding shard cleaning (#6121)

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 718542512..e7d8d94b7 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -795,7 +795,15 @@ impl ShardReplicaSet {
     /// - cost: the cost of the operation
     ///
     /// Returns an error if the rate limit is exceeded.
-    fn check_write_rate_limiter(&self, cost: usize) -> CollectionResult<()> {
+    fn check_write_rate_limiter(
+        &self,
+        cost: usize,
+        hw_measurement_acc: &HwMeasurementAcc,
+    ) -> CollectionResult<()> {
+        // Do not rate limit internal operation tagged with disposable measurement
+        if hw_measurement_acc.is_disposable() {
+            return Ok(());
+        }
         if let Some(rate_limiter) = &self.write_rate_limiter {
             rate_limiter
                 .lock()
@@ -806,15 +814,20 @@ impl ShardReplicaSet {
     }
 
     /// Check if the write rate limiter allows the operation to proceed
+    /// - hw_measurement_acc: the current hardware measurement accumulator
     /// - cost_fn: the cost function of the operation, evaluated lazily
     ///
     /// Returns an error if the rate limit is exceeded.
-    fn check_write_rate_limiter_lazy<F>(&self, cost_fn: F) -> CollectionResult<()>
+    fn check_write_rate_limiter_lazy<F>(
+        &self,
+        hw_measurement_acc: &HwMeasurementAcc,
+        cost_fn: F,
+    ) -> CollectionResult<()>
     where
         F: Fn() -> usize,
     {
-        if self.write_rate_limiter.is_some() {
-            self.check_write_rate_limiter(cost_fn())?;
+        if self.write_rate_limiter.is_some() && !hw_measurement_acc.is_disposable() {
+            self.check_write_rate_limiter(cost_fn(), hw_measurement_acc)?;
         };
         Ok(())
     }

commit 959fbc5e85bd5e06131e54cffba8dce6206b6767
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Wed Mar 12 15:20:47 2025 +0100

    Write rate limit cost always lazy (#6159)
    
    * Write rate limit cost always lazy
    
    * review once is enough

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index e7d8d94b7..629240e17 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -792,19 +792,24 @@ impl ShardReplicaSet {
     }
 
     /// Check if the write rate limiter allows the operation to proceed
-    /// - cost: the cost of the operation
+    /// - hw_measurement_acc: the current hardware measurement accumulator
+    /// - cost_fn: the cost of the operation called lazily
     ///
     /// Returns an error if the rate limit is exceeded.
-    fn check_write_rate_limiter(
+    fn check_write_rate_limiter<F>(
         &self,
-        cost: usize,
         hw_measurement_acc: &HwMeasurementAcc,
-    ) -> CollectionResult<()> {
+        cost_fn: F,
+    ) -> CollectionResult<()>
+    where
+        F: FnOnce() -> usize,
+    {
         // Do not rate limit internal operation tagged with disposable measurement
         if hw_measurement_acc.is_disposable() {
             return Ok(());
         }
         if let Some(rate_limiter) = &self.write_rate_limiter {
+            let cost = cost_fn();
             rate_limiter
                 .lock()
                 .try_consume(cost as f64)
@@ -813,25 +818,6 @@ impl ShardReplicaSet {
         Ok(())
     }
 
-    /// Check if the write rate limiter allows the operation to proceed
-    /// - hw_measurement_acc: the current hardware measurement accumulator
-    /// - cost_fn: the cost function of the operation, evaluated lazily
-    ///
-    /// Returns an error if the rate limit is exceeded.
-    fn check_write_rate_limiter_lazy<F>(
-        &self,
-        hw_measurement_acc: &HwMeasurementAcc,
-        cost_fn: F,
-    ) -> CollectionResult<()>
-    where
-        F: Fn() -> usize,
-    {
-        if self.write_rate_limiter.is_some() && !hw_measurement_acc.is_disposable() {
-            self.check_write_rate_limiter(cost_fn(), hw_measurement_acc)?;
-        };
-        Ok(())
-    }
-
     /// Check if there are any locally disabled peers
     /// And if so, report them to the consensus
     pub fn sync_local_state<F>(&self, get_shard_transfers: F) -> CollectionResult<()>

commit c41a65a3dea52d12cee8f3cf0fa14cb9020625b9
Author: Tim Visée <tim+github@visee.me>
Date:   Fri Mar 21 09:33:42 2025 +0100

    Fix consensus snapshot not applying shard key mappings (#6212)
    
    * Propagate shard key mapping wrapper deeper
    
    * Also bump reverse shard mapping when applying shard keys from snapshot
    
    * Also apply new shard key mappings to existing replicas
    
    * Apply shard key to replica set through apply_state directly
    
    * Set both new shard key mappings through utility function
    
    * Set shard key mappings directly during directory creation
    
    * Rename get_key to key
    
    * Update comment

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 629240e17..8252563c3 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -675,8 +675,9 @@ impl ShardReplicaSet {
     }
 
     pub async fn apply_state(
-        &self,
+        &mut self,
         replicas: HashMap<PeerId, ReplicaState>,
+        shard_key: Option<ShardKey>,
     ) -> CollectionResult<()> {
         let old_peers = self.replica_state.read().peers();
 
@@ -753,6 +754,10 @@ impl ShardReplicaSet {
             );
             self.remotes.write().await.push(new_remote);
         }
+
+        // Apply shard key
+        self.shard_key = shard_key;
+
         Ok(())
     }
 

commit f230629fa0e62e069e683cce60e24319ab3cc84b
Author: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>
Date:   Tue Mar 25 10:08:21 2025 +0100

    build(deps): bump log from 0.4.26 to 0.4.27 (#6247)
    
    * build(deps): bump log from 0.4.26 to 0.4.27
    
    Bumps [log](https://github.com/rust-lang/log) from 0.4.26 to 0.4.27.
    - [Release notes](https://github.com/rust-lang/log/releases)
    - [Changelog](https://github.com/rust-lang/log/blob/master/CHANGELOG.md)
    - [Commits](https://github.com/rust-lang/log/compare/0.4.26...0.4.27)
    
    ---
    updated-dependencies:
    - dependency-name: log
      dependency-type: direct:production
      update-type: version-update:semver-patch
    ...
    
    Signed-off-by: dependabot[bot] <support@github.com>
    
    * put variables inside the strings for log macros
    
    * also for pyroscope
    
    ---------
    
    Signed-off-by: dependabot[bot] <support@github.com>
    Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>
    Co-authored-by: Luis Cossío <luis.cossio@outlook.com>

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 8252563c3..04da47f09 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -1066,7 +1066,7 @@ impl ShardReplicaSet {
     }
 
     fn notify_peer_failure(&self, peer_id: PeerId, from_state: Option<ReplicaState>) {
-        log::debug!("Notify peer failure: {}", peer_id);
+        log::debug!("Notify peer failure: {peer_id}");
         self.notify_peer_failure_cb.deref()(peer_id, self.shard_id, from_state)
     }
 

commit 90340a098c9667e36c499633bf3d17015ee4ddc1
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Wed Apr 2 15:11:13 2025 +0200

    Implement partial snapshot recovery (#6206)
    
    - Fix creating partial snapshot without RocksDB backup
    - Fix `segment_manifest.json` file placement
    - Implement partial snapshot recovery
    - Allow restoring partial snapshot for all shard types
    - Validate segment manifests when recovering partial snapshot
    - Log error, if streaming shard/partial snapshot failed
    - Contextualize errors in `snapshot_files` function
    - Tweak log messages in `Segment::take_snapshot` method
    - Remove unnecessary lifetimes from `SegmentHolder` iter methods

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 04da47f09..c8b3c3740 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -3,7 +3,7 @@ mod execute_read_operation;
 mod locally_disabled_peers;
 mod read_ops;
 mod shard_transfer;
-mod snapshots;
+pub mod snapshots;
 mod update;
 
 use std::collections::{HashMap, HashSet};

commit b5e5f97bf997b0844806c9cf6fed02e39f3ced5d
Author: Kumar Shivendu <kshivendu1@gmail.com>
Date:   Wed Apr 16 22:19:35 2025 +0530

    Abort resharding if any Resharding replica is to be marked dead (#6364)
    
    * Abort resharding if any Resharding replica is to be marked dead
    
    * Abort resharding before marking shard as Dead
    
    * add comments
    
    * Abort resharding after marking as dead (#6394)
    
    * Force delete points when aborting resharding, even if replica is dead
    
    * Abort resharding after marking replica as dead
    
    * Only abort resharding if we mark related replica as dead
    
    * Just rely on shard replica state
    
    * Propagate shard transfer error right away
    
    ---------
    
    Co-authored-by: Tim Visée <tim+github@visee.me>

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index c8b3c3740..865b63487 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -894,6 +894,7 @@ impl ShardReplicaSet {
         &self,
         filter: Filter,
         hw_measurement_acc: HwMeasurementAcc,
+        force: bool,
     ) -> CollectionResult<UpdateResult> {
         let local_shard_guard = self.local.read().await;
 
@@ -950,7 +951,7 @@ impl ShardReplicaSet {
 
         // TODO(resharding): Assign clock tag to the operation!? 🤔
         let result = self
-            .update_local(op.into(), true, hw_measurement_acc)
+            .update_local(op.into(), true, hw_measurement_acc, force)
             .await?
             .ok_or_else(|| {
                 CollectionError::bad_request(format!(

commit cd1287a2b9be9b3372beb165421297076c6ab11b
Author: Kumar Shivendu <kshivendu1@gmail.com>
Date:   Thu Apr 17 01:22:24 2025 +0530

    Recover dirty shards using other replicas when marked Dead (#6293)
    
    * Test behaviour of Qdrant with shard initializing flag
    
    * Corrupt shard directory and let Qdrant panic like prod
    
    * Wait for shard transfer
    
    * Restore dirty shards using other replicas
    
    * remove unused code
    
    * Request transfer only if replica is dead or dirty
    
    * fmt
    
    * remove comment
    
    * fix clippy
    
    * Delete shard initializing flag after initializing empty local shard
    
    * Expect test to recover shard in existing test
    
    * Review suggestions
    
    * Run tests for longer
    
    * Simplify tests
    
    * Use 2k points
    
    * condition for point_count
    
    * Add comment
    
    * fix flaky tests
    
    * fix flaky tests
    
    * handle edge case
    
    * Include Active in expected states list
    
    * Introduce is_recovery
    
    * simplify tests
    
    * get rid of is_dirty bool in DummyShard
    
    * add missing negation in condition
    
    * fix condition
    
    * final fix for transfer condition
    
    * Don't auto recover if in recovery mode, simplify state checking
    
    * minor comment improvements
    
    * tests scenario where node is killed after deleting shard initializing flag
    
    * Fix failing CI
    
    * Only automatically recover dead replicas
    
    * Mark replica as dead to recover dummy shard
    
    * fix failing test
    
    * Sleep one second after killing peer, give time to release WAL lock
    
    * Prevent waiting for peer to come online indefinitely
    
    * update comment
    
    * minor typo
    
    ---------
    
    Co-authored-by: timvisee <tim@visee.me>

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 865b63487..83383a31c 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -236,6 +236,7 @@ impl ShardReplicaSet {
         shard_key: Option<ShardKey>,
         collection_id: CollectionId,
         shard_path: &Path,
+        is_dirty_shard: bool,
         collection_config: Arc<RwLock<CollectionConfigInternal>>,
         effective_optimizers_config: OptimizersConfig,
         shared_storage_config: Arc<SharedStorageConfig>,
@@ -278,6 +279,14 @@ impl ShardReplicaSet {
         let local = if replica_state.read().is_local {
             let shard = if let Some(recovery_reason) = &shared_storage_config.recovery_mode {
                 Shard::Dummy(DummyShard::new(recovery_reason))
+            } else if is_dirty_shard {
+                log::error!(
+                    "Shard {collection_id}:{shard_id} is not fully initialized - loading as dummy shard"
+                );
+                // This dummy shard will be replaced only when it rejects an update (marked as dead so recovery process kicks in)
+                Shard::Dummy(DummyShard::new(
+                    "Dirty shard - shard is not fully initialized",
+                ))
             } else {
                 let res = LocalShard::load(
                     shard_id,
@@ -514,12 +523,13 @@ impl ShardReplicaSet {
         Ok(())
     }
 
+    /// Clears the local shard data and loads an empty local shard
     pub async fn init_empty_local_shard(&self) -> CollectionResult<()> {
         let mut local = self.local.write().await;
 
         let current_shard = local.take();
 
-        // ToDo: Remove shard files here?
+        LocalShard::clear(&self.shard_path).await?;
         let local_shard_res = LocalShard::build(
             self.shard_id,
             self.collection_id.clone(),

commit bbea80e7d4f675d455053c10e751cc0f45df45e6
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Thu Apr 17 23:32:10 2025 +0200

    Telemetry improvements (#6390)
    
    * allow shard states in anonymize telemetry (with hashed peer ids)
    
    * introduce level3 and level4 for telemetry, where level4 = everything, incl. segments info
    
    * upd openapi
    
    * fix tests
    
    * expose vector count & size stats on shard level to avoid reading of segments
    
    * fix spelling
    
    * upd schema
    
    * fix tests
    
    * Use unwrap_or_default
    
    * [#6390] skip serializing shard details in Level2 (#6398)
    
    * skip serializing shard details in Level2
    
    * upd openapi
    
    ---------
    
    Co-authored-by: generall <andrey@vasnetsov.com>
    
    ---------
    
    Co-authored-by: Tim Visée <tim+github@visee.me>
    Co-authored-by: Luis Cossío <luis.cossio@qdrant.com>

diff --git a/lib/collection/src/shards/replica_set/mod.rs b/lib/collection/src/shards/replica_set/mod.rs
index 83383a31c..05bb74a8b 100644
--- a/lib/collection/src/shards/replica_set/mod.rs
+++ b/lib/collection/src/shards/replica_set/mod.rs
@@ -4,6 +4,7 @@ mod locally_disabled_peers;
 mod read_ops;
 mod shard_transfer;
 pub mod snapshots;
+mod telemetry;
 mod update;
 
 use std::collections::{HashMap, HashSet};
@@ -15,8 +16,8 @@ use std::time::Duration;
 use common::budget::ResourceBudget;
 use common::counter::hardware_accumulator::HwMeasurementAcc;
 use common::rate_limiting::RateLimiter;
-use common::types::TelemetryDetail;
 use schemars::JsonSchema;
+use segment::common::anonymize::Anonymize;
 use segment::types::{ExtendedPointId, Filter, ShardKey};
 use serde::{Deserialize, Serialize};
 use tokio::runtime::Handle;
@@ -41,7 +42,6 @@ use crate::shards::dummy_shard::DummyShard;
 use crate::shards::replica_set::clock_set::ClockSet;
 use crate::shards::shard::{PeerId, Shard, ShardId};
 use crate::shards::shard_config::ShardConfig;
-use crate::shards::telemetry::ReplicaSetTelemetry;
 
 //    │    Collection Created
 //    │
@@ -862,30 +862,6 @@ impl ShardReplicaSet {
         Ok(())
     }
 
-    pub(crate) async fn get_telemetry_data(&self, detail: TelemetryDetail) -> ReplicaSetTelemetry {
-        let local_shard = self.local.read().await;
-        let local = local_shard.as_ref();
-
-        let local_telemetry = match local {
-            Some(local_shard) => Some(local_shard.get_telemetry_data(detail).await),
-            None => None,
-        };
-
-        ReplicaSetTelemetry {
-            id: self.shard_id,
-            key: self.shard_key.clone(),
-            local: local_telemetry,
-            remote: self
-                .remotes
-                .read()
-                .await
-                .iter()
-                .map(|remote| remote.get_telemetry_data(detail))
-                .collect(),
-            replicate_states: self.replica_state.read().peers(),
-        }
-    }
-
     pub(crate) async fn health_check(&self, peer_id: PeerId) -> CollectionResult<()> {
         let remotes = self.remotes.read().await;
 
@@ -1223,7 +1199,9 @@ impl ReplicaSetState {
 }
 
 /// State of the single shard within a replica set.
-#[derive(Debug, Deserialize, Serialize, JsonSchema, Default, PartialEq, Eq, Hash, Clone, Copy)]
+#[derive(
+    Debug, Deserialize, Serialize, JsonSchema, Default, PartialEq, Eq, Hash, Clone, Copy, Anonymize,
+)]
 pub enum ReplicaState {
     // Active and sound
     #[default]

</code></pre>
        </section>
    </main>
    <footer>
        <p>LoCoDiff-bench - <a href="https://github.com/AbanteAI/LoCoDiff-bench">GitHub Repository</a></p>
    </footer>
    
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            hljs.highlightAll();
        });
    </script>
</body>
</html>
    