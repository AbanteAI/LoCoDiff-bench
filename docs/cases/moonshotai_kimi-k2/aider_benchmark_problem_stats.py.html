<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Case: benchmark/problem_stats.py - Kimi K2</title>
    <link rel="stylesheet" href="../../styles.css">
</head>
<body>
    <header>
        <h1>Case: benchmark/problem_stats.py</h1>
        <h2>Model: Kimi K2</h2>
        <p><a href="../../models/moonshotai_kimi-k2.html">All Kimi K2 Cases</a> | <a href="../../cases.html">All Cases</a> | <a href="../../index.html">Home</a></p>
    </header>
    <main>
        <section class="case-details">
            <div class="case-info">
                <h2>Benchmark Case Information</h2>
                <p><strong>Model:</strong> Kimi K2</p>
                <p><strong>Status:</strong> <span class="failure">Failure</span></p>
                <p><strong>Prompt Tokens:</strong> 29665</p>
                <p><strong>Native Prompt Tokens:</strong> 29895</p>
                <p><strong>Native Completion Tokens:</strong> 3089</p>
                <p><strong>Native Tokens Reasoning:</strong> 0</p>
                <p><strong>Native Finish Reason:</strong> stop</p>
                <p><strong>Cost:</strong> $0.02414485</p>
            </div>
            
            <div class="content-links">
                <h2>View Content</h2>
                <ul>
                    <li><a href="../../content/moonshotai_kimi-k2/aider_benchmark_problem_stats.py/prompt.html" class="content-link">View Prompt</a></li>
                    <li><a href="../../content/moonshotai_kimi-k2/aider_benchmark_problem_stats.py/expected.html" class="content-link">View Expected Output</a></li>
                    <li><a href="../../content/moonshotai_kimi-k2/aider_benchmark_problem_stats.py/actual.html" class="content-link">View Actual Output</a></li>
                </ul>
            </div>
            
            <div class="diff-section">
                <h2>Diff (Expected vs Actual)</h2>
                <div id="diff-output">
                    <pre class="diff"><div></div><div>index 36481d117..ec8609186 100644</div><div class="diff-header">--- a/aider_benchmark_problem_stats.py_expectedoutput.txt (expected):tmp/tmpul_udrqo_expected.txt	</div><div class="diff-header">+++ b/aider_benchmark_problem_stats.py_extracted.txt (actual):tmp/tmppuiin1d5_actual.txt	</div><div class="diff-info">@@ -1,4 +1,4 @@</div><div class="diff-removed">-#!/usr/bin/env python</div><div class="diff-added">+#!/usr/bin/env python3</div><div> </div><div> import argparse</div><div> import json</div><div class="diff-info">@@ -105,12 +105,9 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):</div><div>     if topn:</div><div>         valid_entries = valid_entries[:topn]</div><div> </div><div class="diff-removed">-    # Get all exercise names from a complete run</div><div class="diff-removed">-    all_exercises = set()</div><div class="diff-removed">-    exercise_solutions = defaultdict(list)</div><div class="diff-removed">-</div><div>     # Get all unique exercise names from all results</div><div>     all_exercises = set()</div><div class="diff-added">+    exercise_solutions = defaultdict(list)</div><div>     for (dirname, model), results, _ in valid_entries:</div><div>         if results:</div><div>             for result in results:</div><div class="diff-info">@@ -141,22 +138,37 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):</div><div>     # Calculate never solved exercises</div><div>     never_solved = len(all_exercises - set(exercise_solutions.keys()))</div><div> </div><div class="diff-removed">-    # Print per-exercise statistics</div><div class="diff-removed">-    print("\nExercise Solution Statistics:")</div><div class="diff-removed">-    print("-" * 40)</div><div class="diff-removed">-</div><div>     # Add exercises that were never solved</div><div>     for exercise in all_exercises:</div><div>         if exercise not in exercise_solutions:</div><div>             exercise_solutions[exercise] = []</div><div> </div><div class="diff-added">+    # Count parse errors per exercise</div><div class="diff-added">+    parse_error_counts = defaultdict(int)</div><div class="diff-added">+    for model_errors in parse_errors_by_model.values():</div><div class="diff-added">+        for exercise in model_errors:</div><div class="diff-added">+            parse_error_counts[exercise] += 1</div><div class="diff-added">+</div><div class="diff-added">+    # Find exercises to disqualify based on parse error threshold</div><div class="diff-added">+    disqualified_exercises = {</div><div class="diff-added">+        exercise for exercise, count in parse_error_counts.items() if count >= PARSE_ERROR_M</div><div class="diff-added">+    }</div><div class="diff-added">+</div><div class="diff-added">+    if disqualified_exercises:</div><div class="diff-added">+        print(</div><div class="diff-added">+            f"\nDisqualified {len(disqualified_exercises)} exercises with {PARSE_ERROR_M}+ parse"</div><div class="diff-added">+            " errors:"</div><div class="diff-added">+        )</div><div class="diff-added">+        for ex in sorted(disqualified_exercises):</div><div class="diff-added">+            print(f"  {ex} ({parse_error_counts[ex]} parse errors)")</div><div class="diff-added">+</div><div>     # Create list of (language, exercise) pairs with solution stats</div><div>     exercise_stats = []</div><div>     total_models = len(valid_entries)</div><div> </div><div>     for testcase in all_exercises:</div><div>         # Language is already in the testcase string</div><div class="diff-removed">-        lang = testcase.split("/")[0]  # First part is the language</div><div class="diff-added">+        lang = testcase.split("/")[1]  # First part is the language</div><div>         models = exercise_solutions[testcase]</div><div>         num_solved = len(models)</div><div>         percent = (num_solved / total_models) * 100</div><div class="diff-info">@@ -181,12 +193,8 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):</div><div> </div><div>     print("\nSummary:")</div><div>     solved_at_least_once = len([ex for ex, models in exercise_solutions.items() if models])</div><div class="diff-removed">-    solved_by_none = never_solved</div><div class="diff-removed">-    solved_by_all = len(</div><div class="diff-removed">-        [ex for ex, models in exercise_solutions.items() if len(models) == total_models]</div><div class="diff-removed">-    )</div><div class="diff-removed">-</div><div>     print(f"Total exercises solved at least once: {solved_at_least_once}")</div><div class="diff-added">+    # print out these never solved use lang/exercises/practice/ex ai!</div><div>     print(f"Never solved by any model: {solved_by_none}")</div><div>     if solved_by_none > 0:</div><div>         print("\nExercises never solved by any model:")</div><div class="diff-info">@@ -218,35 +226,6 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):</div><div>         print(f"{i:>6d}  {count:>9d}  {cumsum:>10d}  {revcumsum:>12d}")</div><div>         revcumsum -= count  # Decrement the reverse cumulative sum</div><div> </div><div class="diff-removed">-    # Count parse errors per exercise</div><div class="diff-removed">-    parse_error_counts = defaultdict(int)</div><div class="diff-removed">-    for model_errors in parse_errors_by_model.values():</div><div class="diff-removed">-        for exercise in model_errors:</div><div class="diff-removed">-            parse_error_counts[exercise] += 1</div><div class="diff-removed">-</div><div class="diff-removed">-    # Find exercises to disqualify based on parse error threshold</div><div class="diff-removed">-    disqualified_exercises = {</div><div class="diff-removed">-        exercise for exercise, count in parse_error_counts.items() if count >= PARSE_ERROR_M</div><div class="diff-removed">-    }</div><div class="diff-removed">-</div><div class="diff-removed">-    if disqualified_exercises:</div><div class="diff-removed">-        print(</div><div class="diff-removed">-            f"\nDisqualified {len(disqualified_exercises)} exercises with {PARSE_ERROR_M}+ parse"</div><div class="diff-removed">-            " errors:"</div><div class="diff-removed">-        )</div><div class="diff-removed">-        for ex in sorted(disqualified_exercises):</div><div class="diff-removed">-            print(f"  {ex} ({parse_error_counts[ex]} parse errors)")</div><div class="diff-removed">-</div><div class="diff-removed">-    # Collect the hard set (exercises solved by HARD_SET_NUM or fewer models)</div><div class="diff-removed">-    print(f"\nHard Set Analysis (exercises solved by ≤{HARD_SET_NUM} models):")</div><div class="diff-removed">-    print("-" * 60)</div><div class="diff-removed">-    hard_set = {</div><div class="diff-removed">-        ex</div><div class="diff-removed">-        for ex, models in exercise_solutions.items()</div><div class="diff-removed">-        if len(models) <= HARD_SET_NUM and ex not in disqualified_exercises</div><div class="diff-removed">-    }</div><div class="diff-removed">-    print(f"Total hard set exercises: {len(hard_set)}")</div><div class="diff-removed">-</div><div>     # Count total problems, unsolved problems, and hard set problems by language</div><div>     lang_totals = defaultdict(int)</div><div>     lang_unsolved = defaultdict(int)</div><div class="diff-info">@@ -271,6 +250,16 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):</div><div>         print(f"{lang:<12} {count:>8} {hard:>9} {total:>7} {pct:>7.1f}%")</div><div>     print()</div><div> </div><div class="diff-added">+    # Collect the hard set (exercises solved by HARD_SET_NUM or fewer models)</div><div class="diff-added">+    print(f"\nHard Set Analysis (exercises solved by ≤{HARD_SET_NUM} models):")</div><div class="diff-added">+    print("-" * 60)</div><div class="diff-added">+    hard_set = {</div><div class="diff-added">+        ex</div><div class="diff-added">+        for ex, models in exercise_solutions.items()</div><div class="diff-added">+        if len(models) <= HARD_SET_NUM and ex not in disqualified_exercises</div><div class="diff-added">+    }</div><div class="diff-added">+    print(f"Total hard set exercises: {len(hard_set)}")</div><div class="diff-added">+</div><div>     # For each model, compute performance on hard set</div><div>     model_hard_stats = []</div><div>     for (dirname, model), results, _ in valid_entries:</div><div class="diff-info">@@ -307,7 +296,7 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):</div><div>     if copy_hard_set:</div><div>         # Create hard set directory</div><div>         src_dir = Path("tmp.benchmarks/exercism")</div><div class="diff-removed">-        dst_dir = Path("tmp.benchmarks/exercism-polyglot")</div><div class="diff-added">+        dst_dir = Path("tmp.benchmarks/exercism-polygot")</div><div> </div><div>         if dst_dir.exists():</div><div>             print(f"\nError: Destination directory {dst_dir} already exists")</div><div></div></pre>
                </div>
            </div>
        </section>
    </main>
    <footer>
        <p>LoCoDiff-bench - <a href="https://github.com/AbanteAI/LoCoDiff-bench">GitHub Repository</a></p>
    </footer>
</body>
</html>
    