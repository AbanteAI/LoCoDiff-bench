<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Case: benchmark/problem_stats.py - o4-mini-medium</title>
    <link rel="stylesheet" href="../../styles.css">
</head>
<body>
    <header>
        <h1>Case: benchmark/problem_stats.py</h1>
        <p><a href="../../models/openai_o4-mini.html">‚Üê Back to o4-mini-medium Cases</a> | <a href="../../index.html">Home</a></p>
    </header>
    <main>
        <section class="case-details">
            <div class="case-info">
                <h2>Benchmark Case Information</h2>
                <p><strong>Model:</strong> o4-mini-medium</p>
                <p><strong>Status:</strong> <span class="failure">Failure</span></p>
                <p><strong>Prompt Tokens:</strong> 29665</p>
                <p><strong>Output Tokens:</strong> N/A</p>
                <p><strong>Native Prompt Tokens:</strong> 29984</p>
                <p><strong>Native Completion Tokens:</strong> 12286</p>
                <p><strong>Native Tokens Reasoning:</strong> 9216</p>
                <p><strong>Native Finish Reason:</strong> stop</p>
                <p><strong>Cost:</strong> $0.0870408</p>
            </div>
            
            <div class="content-links">
                <h2>View Content</h2>
                <ul>
                    <li><a href="../../content/openai_o4-mini/aider_benchmark_problem_stats.py/prompt.html" class="content-link">View Prompt</a></li>
                    <li><a href="../../content/openai_o4-mini/aider_benchmark_problem_stats.py/expected.html" class="content-link">View Expected Output</a></li>
                    <li><a href="../../content/openai_o4-mini/aider_benchmark_problem_stats.py/actual.html" class="content-link">View Actual Output</a></li>
                </ul>
            </div>
            
            <div class="diff-section">
                <h2>Diff (Expected vs Actual)</h2>
                <div id="diff-output">
                    <pre class="diff"><div class="diff-header">--- aider_benchmark_problem_stats.py_expectedoutput.txt (expected)+++ aider_benchmark_problem_stats.py_extracted.txt (actual)@@ -5,13 +5,10 @@ import shutil</div><div> from collections import defaultdict</div><div> from pathlib import Path</div><div class="diff-removed">-</div><div> import yaml</div><div class="diff-removed">-</div><div> from aider.dump import dump  # noqa</div><div> </div><div> HARD_SET_NUM = 3  # Number of models that defines the hard set threshold</div><div class="diff-removed">-</div><div> </div><div> def get_dirs_from_leaderboard():</div><div>     # Load the leaderboard data</div><div class="diff-info">@@ -19,11 +16,9 @@         leaderboard = yaml.safe_load(f)</div><div>     return [(entry["dirname"], entry["model"]) for entry in leaderboard]</div><div> </div><div class="diff-removed">-</div><div> def load_results(dirname):</div><div>     """Load all result files from a benchmark directory"""</div><div>     dirname = Path(dirname)</div><div class="diff-removed">-</div><div>     benchmark_dir = dirname</div><div>     if not benchmark_dir.exists():</div><div>         benchmark_dir = Path("tmp.benchmarks") / dirname</div><div class="diff-info">@@ -35,16 +30,15 @@ </div><div>     # Look in language subdirectories under exercises/practice</div><div>     for fname in benchmark_dir.glob("*/exercises/practice/*/.aider.results.json"):</div><div class="diff-removed">-        error = False</div><div>         try:</div><div>             results = json.loads(fname.read_text())</div><div class="diff-added">+            error = False</div><div>             error = "testcase" not in results</div><div>             if not error:</div><div>                 # Add language info to results</div><div>                 lang = fname.parts[-5]  # Get language from path</div><div>                 results["language"] = lang</div><div>                 all_results.append(results)</div><div class="diff-removed">-</div><div>         except json.JSONDecodeError:</div><div>             error = True</div><div> </div><div class="diff-info">@@ -58,7 +52,6 @@ </div><div>     return all_results, parse_errors</div><div> </div><div class="diff-removed">-</div><div> def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):</div><div>     PARSE_ERROR_M = 4  # Threshold for number of parse errors to DQ an exercise</div><div> </div><div class="diff-info">@@ -74,17 +67,17 @@     parse_errors_by_model = {}  # Track which exercises had parse errors for each model</div><div> </div><div>     dump(dir_entries)</div><div class="diff-removed">-</div><div>     for dirname, model in dir_entries:</div><div>         results_data = load_results(dirname)</div><div class="diff-removed">-</div><div>         if results_data:</div><div>             results, model_parse_errors = results_data</div><div>             parse_errors_by_model[model] = set(model_parse_errors)</div><div>             # Calculate pass rate for sorting when using custom dirs</div><div>             if dirs is not None:</div><div>                 pass_rate = sum(</div><div class="diff-removed">-                    1 for r in results if r.get("tests_outcomes", []) and r["tests_outcomes"][-1]</div><div class="diff-added">+                    1</div><div class="diff-added">+                    for r in results</div><div class="diff-added">+                    if r.get("tests_outcomes", []) and r["tests_outcomes"][-1]</div><div>                 ) / len(results)</div><div>             else:</div><div>                 # Use existing pass rate from leaderboard</div><div class="diff-info">@@ -105,25 +98,24 @@     if topn:</div><div>         valid_entries = valid_entries[:topn]</div><div> </div><div class="diff-removed">-    # Get all exercise names from a complete run</div><div class="diff-removed">-    all_exercises = set()</div><div class="diff-removed">-    exercise_solutions = defaultdict(list)</div><div class="diff-removed">-</div><div>     # Get all unique exercise names from all results</div><div>     all_exercises = set()</div><div>     for (dirname, model), results, _ in valid_entries:</div><div>         if results:</div><div>             for result in results:</div><div>                 try:</div><div class="diff-removed">-                    all_exercises.add(result["testcase"] + "/" + result["language"])</div><div class="diff-added">+                    all_exercises.add(</div><div class="diff-added">+                        result["testcase"] + "/" + result["language"]</div><div class="diff-added">+                    )</div><div>                 except KeyError:</div><div>                     print(f"Warning: Missing testcase in {dirname}", json.dumps(result, indent=4))</div><div> </div><div class="diff-added">+    # Track which models solved each exercise</div><div class="diff-added">+    exercise_solutions = defaultdict(list)</div><div>     for (dirname, model), results, _ in valid_entries:</div><div>         if not results:</div><div>             print(f"Could not load results for {dirname}")</div><div>             continue</div><div class="diff-removed">-</div><div>         for result in results:</div><div>             testcase = result.get("testcase")</div><div>             if not testcase:</div><div class="diff-info">@@ -131,40 +123,26 @@             lang = result.get("language")</div><div>             if not lang:</div><div>                 continue</div><div class="diff-removed">-</div><div>             testcase = f"{testcase}/{lang}"</div><div>             # Consider it solved if the last test attempt passed</div><div>             tests_outcomes = result.get("tests_outcomes", [])</div><div>             if tests_outcomes and tests_outcomes[-1]:</div><div>                 exercise_solutions[testcase].append(model)</div><div> </div><div class="diff-removed">-    # Calculate never solved exercises</div><div class="diff-removed">-    never_solved = len(all_exercises - set(exercise_solutions.keys()))</div><div class="diff-removed">-</div><div class="diff-removed">-    # Print per-exercise statistics</div><div class="diff-removed">-    print("\nExercise Solution Statistics:")</div><div class="diff-removed">-    print("-" * 40)</div><div class="diff-removed">-</div><div class="diff-removed">-    # Add exercises that were never solved</div><div class="diff-removed">-    for exercise in all_exercises:</div><div class="diff-removed">-        if exercise not in exercise_solutions:</div><div class="diff-removed">-            exercise_solutions[exercise] = []</div><div class="diff-removed">-</div><div>     # Create list of (language, exercise) pairs with solution stats</div><div>     exercise_stats = []</div><div>     total_models = len(valid_entries)</div><div class="diff-removed">-</div><div>     for testcase in all_exercises:</div><div>         # Language is already in the testcase string</div><div class="diff-removed">-        lang = testcase.split("/")[0]  # First part is the language</div><div class="diff-added">+        lang = testcase.split("/")[0]</div><div>         models = exercise_solutions[testcase]</div><div>         num_solved = len(models)</div><div>         percent = (num_solved / total_models) * 100</div><div class="diff-removed">-        testcase = testcase.replace("exercises/", "")  # Remove the exercises/ prefix</div><div class="diff-added">+        cleaned = testcase.replace("exercises/", "")  # Remove the exercises/ prefix</div><div>         # Remove duplicate language prefix (e.g. javascript/javascript/ -> javascript/)</div><div class="diff-removed">-        if testcase.startswith(f"{lang}/{lang}/"):</div><div class="diff-removed">-            testcase = testcase[len(lang) + 1 :]</div><div class="diff-removed">-        exercise_stats.append((lang, testcase, num_solved, percent))</div><div class="diff-added">+        if cleaned.startswith(f"{lang}/{lang}/"):</div><div class="diff-added">+            cleaned = cleaned[len(lang) + 1 :]</div><div class="diff-added">+        exercise_stats.append((lang, cleaned, num_solved, percent))</div><div> </div><div>     # Sort all exercises by solve rate, then by exercise name</div><div>     exercise_stats.sort(</div><div class="diff-info">@@ -179,22 +157,20 @@     for i, (lang, testcase, num_solved, percent) in enumerate(exercise_stats, 1):</div><div>         print(f"{i:>3}. {testcase:<{max_name_len}} : {num_solved:>3} solved ({percent:>5.1f}%)")</div><div> </div><div class="diff-added">+    # Summary</div><div>     print("\nSummary:")</div><div>     solved_at_least_once = len([ex for ex, models in exercise_solutions.items() if models])</div><div class="diff-removed">-    solved_by_none = never_solved</div><div class="diff-added">+    solved_by_none = len(all_exercises - set(exercise_solutions.keys()))</div><div>     solved_by_all = len(</div><div>         [ex for ex, models in exercise_solutions.items() if len(models) == total_models]</div><div>     )</div><div class="diff-removed">-</div><div>     print(f"Total exercises solved at least once: {solved_at_least_once}")</div><div>     print(f"Never solved by any model: {solved_by_none}")</div><div>     if solved_by_none > 0:</div><div>         print("\nExercises never solved by any model:")</div><div>         unsolved = [ex for ex, models in exercise_solutions.items() if not models]</div><div>         for ex in sorted(unsolved):</div><div class="diff-removed">-            # Split into language and exercise parts</div><div>             lang, exercise = ex.split("/")</div><div class="diff-removed">-            # Reconstruct path in desired format</div><div>             formatted_path = f"{lang}/exercises/practice/{exercise}"</div><div>             print(f"  {formatted_path}")</div><div>     print(f"\nSolved by all models: {solved_by_all}")</div><div class="diff-info">@@ -216,7 +192,7 @@     for i, count in enumerate(counts):</div><div>         cumsum += count</div><div>         print(f"{i:>6d}  {count:>9d}  {cumsum:>10d}  {revcumsum:>12d}")</div><div class="diff-removed">-        revcumsum -= count  # Decrement the reverse cumulative sum</div><div class="diff-added">+        revcumsum -= count</div><div> </div><div>     # Count parse errors per exercise</div><div>     parse_error_counts = defaultdict(int)</div><div class="diff-info">@@ -237,7 +213,7 @@         for ex in sorted(disqualified_exercises):</div><div>             print(f"  {ex} ({parse_error_counts[ex]} parse errors)")</div><div> </div><div class="diff-removed">-    # Collect the hard set (exercises solved by HARD_SET_NUM or fewer models)</div><div class="diff-added">+    # Hard Set Analysis (exercises solved by ‚â§ HARD_SET_NUM models)</div><div>     print(f"\nHard Set Analysis (exercises solved by ‚â§{HARD_SET_NUM} models):")</div><div>     print("-" * 60)</div><div>     hard_set = {</div><div class="diff-info">@@ -251,7 +227,6 @@     lang_totals = defaultdict(int)</div><div>     lang_unsolved = defaultdict(int)</div><div>     lang_hard_set = defaultdict(int)</div><div class="diff-removed">-</div><div>     for exercise in all_exercises:</div><div>         lang = exercise.split("/")[1]  # Get language from path</div><div>         lang_totals[lang] += 1</div><div class="diff-info">@@ -337,7 +312,6 @@         print(f"\nCopied {total_copied} hard set problems:")</div><div>         for lang in sorted(copied_by_lang):</div><div>             print(f"  {lang}: {copied_by_lang[lang]}")</div><div class="diff-removed">-</div><div> </div><div> if __name__ == "__main__":</div><div>     parser = argparse.ArgumentParser()</div><div></div></pre>
                </div>
            </div>
        </section>
    </main>
    <footer>
        <p>LoCoDiff-bench - <a href="https://github.com/AbanteAI/LoCoDiff-bench">GitHub Repository</a></p>
    </footer>
</body>
</html>
    