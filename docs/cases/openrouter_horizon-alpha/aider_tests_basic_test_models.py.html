<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Case: tests/basic/test_models.py - Horizon Alpha</title>
    <link rel="stylesheet" href="../../styles.css">
</head>
<body>
    <header>
        <h1>Case: tests/basic/test_models.py</h1>
        <h2>Model: Horizon Alpha</h2>
        <p><a href="../../models/openrouter_horizon-alpha.html">All Horizon Alpha Cases</a> | <a href="../../cases.html">All Cases</a> | <a href="../../index.html">Home</a></p>
    </header>
    <main>
        <section class="case-details">
            <div class="case-info">
                <h2>Benchmark Case Information</h2>
                <p><strong>Model:</strong> Horizon Alpha</p>
                <p><strong>Status:</strong> <span class="failure">Failure</span></p>
                <p><strong>Prompt Tokens:</strong> 34611</p>
                <p><strong>Native Prompt Tokens:</strong> 35097</p>
                <p><strong>Native Completion Tokens:</strong> 4675</p>
                <p><strong>Native Tokens Reasoning:</strong> 0</p>
                <p><strong>Native Finish Reason:</strong> stop</p>
                <p><strong>Cost:</strong> $0.0</p>
            </div>
            
            <div class="content-links">
                <h2>View Content</h2>
                <ul>
                    <li><a href="../../content/openrouter_horizon-alpha/aider_tests_basic_test_models.py/prompt.html" class="content-link">View Prompt</a></li>
                    <li><a href="../../content/openrouter_horizon-alpha/aider_tests_basic_test_models.py/expected.html" class="content-link">View Expected Output</a></li>
                    <li><a href="../../content/openrouter_horizon-alpha/aider_tests_basic_test_models.py/actual.html" class="content-link">View Actual Output</a></li>
                </ul>
            </div>
            
            <div class="diff-section">
                <h2>Diff (Expected vs Actual)</h2>
                <div id="diff-output">
                    <pre class="diff"><div></div><div>index dbe4ed68c..21c32d998 100644</div><div class="diff-header">--- a/aider_tests_basic_test_models.py_expectedoutput.txt (expected):tmp/tmp8pqkr_ju_expected.txt	</div><div class="diff-header">+++ b/aider_tests_basic_test_models.py_extracted.txt (actual):tmp/tmpigzgel25_actual.txt	</div><div class="diff-info">@@ -49,9 +49,7 @@ class TestModels(unittest.TestCase):</div><div>         model = Model("gpt-4-0613")</div><div>         self.assertEqual(model.info["max_input_tokens"], 8 * 1024)</div><div> </div><div class="diff-removed">-    @patch("os.environ")</div><div class="diff-removed">-    def test_sanity_check_model_all_set(self, mock_environ):</div><div class="diff-removed">-        mock_environ.get.return_value = "dummy_value"</div><div class="diff-added">+    def test_sanity_check_model_all_set(self):</div><div>         mock_io = MagicMock()</div><div>         model = MagicMock()</div><div>         model.name = "test-model"</div><div class="diff-info">@@ -94,7 +92,6 @@ class TestModels(unittest.TestCase):</div><div>             result</div><div>         )  # Should return True because there's a problem with the editor model</div><div>         mock_io.tool_warning.assert_called_with(ANY)  # Ensure a warning was issued</div><div class="diff-removed">-</div><div>         warning_messages = [</div><div>             warning_call.args[0] for warning_call in mock_io.tool_warning.call_args_list</div><div>         ]</div><div class="diff-info">@@ -203,7 +200,9 @@ class TestModels(unittest.TestCase):</div><div> </div><div>         # Test with decimal value</div><div>         model.set_thinking_tokens("0.5M")</div><div class="diff-removed">-        self.assertEqual(model.extra_params["thinking"]["budget_tokens"], 0.5 * 1024 * 1024)</div><div class="diff-added">+        self.assertEqual(</div><div class="diff-added">+            model.extra_params["thinking"]["budget_tokens"], 0.5 * 1024 * 1024</div><div class="diff-added">+        )</div><div> </div><div>     @patch("aider.models.check_pip_install_extra")</div><div>     def test_check_for_dependencies_bedrock(self, mock_check_pip):</div><div></div></pre>
                </div>
            </div>
        </section>
    </main>
    <footer>
        <p>LoCoDiff-bench - <a href="https://github.com/AbanteAI/LoCoDiff-bench">GitHub Repository</a></p>
    </footer>
</body>
</html>
    