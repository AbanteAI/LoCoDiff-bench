<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Case: tests/basic/test_models.py - DeepSeek Chat v3-0324</title>
    <link rel="stylesheet" href="../../styles.css">
</head>
<body>
    <header>
        <h1>Case: tests/basic/test_models.py</h1>
        <p><a href="../../models/deepseek_deepseek-chat-v3-0324.html">‚Üê Back to DeepSeek Chat v3-0324 Cases</a> | <a href="../../index.html">Home</a></p>
    </header>
    <main>
        <section class="case-details">
            <div class="case-info">
                <h2>Benchmark Case Information</h2>
                <p><strong>Model:</strong> DeepSeek Chat v3-0324</p>
                <p><strong>Status:</strong> <span class="failure">Failure</span></p>
                <p><strong>Prompt Tokens:</strong> 34611</p>
                <p><strong>Output Tokens:</strong> N/A</p>
                <p><strong>Native Prompt Tokens:</strong> 37744</p>
                <p><strong>Native Completion Tokens:</strong> 4460</p>
                <p><strong>Native Tokens Reasoning:</strong> 0</p>
                <p><strong>Native Finish Reason:</strong> stop</p>
                <p><strong>Cost:</strong> $0.025562</p>
            </div>
            
            <div class="content-links">
                <h2>View Content</h2>
                <ul>
                    <li><a href="../../content/deepseek_deepseek-chat-v3-0324/aider_tests_basic_test_models.py/prompt.html" class="content-link">View Prompt</a></li>
                    <li><a href="../../content/deepseek_deepseek-chat-v3-0324/aider_tests_basic_test_models.py/expected.html" class="content-link">View Expected Output</a></li>
                    <li><a href="../../content/deepseek_deepseek-chat-v3-0324/aider_tests_basic_test_models.py/actual.html" class="content-link">View Actual Output</a></li>
                </ul>
            </div>
            
            <div class="diff-section">
                <h2>Diff (Expected vs Actual)</h2>
                <div id="diff-output">
                    <pre class="diff"><div class="diff-header">--- aider_tests_basic_test_models.py_expectedoutput.txt (expected)+++ aider_tests_basic_test_models.py_extracted.txt (actual)@@ -83,28 +83,6 @@         self.assertIn("- API_KEY1: Not set", str(calls))</div><div>         self.assertIn("- API_KEY2: Not set", str(calls))</div><div> </div><div class="diff-removed">-    def test_sanity_check_models_bogus_editor(self):</div><div class="diff-removed">-        mock_io = MagicMock()</div><div class="diff-removed">-        main_model = Model("gpt-4")</div><div class="diff-removed">-        main_model.editor_model = Model("bogus-model")</div><div class="diff-removed">-</div><div class="diff-removed">-        result = sanity_check_models(mock_io, main_model)</div><div class="diff-removed">-</div><div class="diff-removed">-        self.assertTrue(</div><div class="diff-removed">-            result</div><div class="diff-removed">-        )  # Should return True because there's a problem with the editor model</div><div class="diff-removed">-        mock_io.tool_warning.assert_called_with(ANY)  # Ensure a warning was issued</div><div class="diff-removed">-</div><div class="diff-removed">-        warning_messages = [</div><div class="diff-removed">-            warning_call.args[0] for warning_call in mock_io.tool_warning.call_args_list</div><div class="diff-removed">-        ]</div><div class="diff-removed">-        print("Warning messages:", warning_messages)  # Add this line</div><div class="diff-removed">-</div><div class="diff-removed">-        self.assertGreaterEqual(mock_io.tool_warning.call_count, 1)  # Expect two warnings</div><div class="diff-removed">-        self.assertTrue(</div><div class="diff-removed">-            any("bogus-model" in msg for msg in warning_messages)</div><div class="diff-removed">-        )  # Check that one of the warnings mentions the bogus model</div><div class="diff-removed">-</div><div>     @patch("aider.models.check_for_dependencies")</div><div>     def test_sanity_check_model_calls_check_dependencies(self, mock_check_deps):</div><div>         """Test that sanity_check_model calls check_for_dependencies"""</div><div class="diff-info">@@ -146,11 +124,6 @@         model = Model("opus")</div><div>         self.assertEqual(model.name, "claude-3-opus-20240229")</div><div> </div><div class="diff-removed">-        # Test non-alias passes through unchanged</div><div class="diff-removed">-        model = Model("gpt-4")</div><div class="diff-removed">-        self.assertEqual(model.name, "gpt-4")</div><div class="diff-removed">-</div><div class="diff-removed">-    def test_o1_use_temp_false(self):</div><div>         # Test GitHub Copilot models</div><div>         model = Model("github/o1-mini")</div><div>         self.assertEqual(model.name, "github/o1-mini")</div><div class="diff-info">@@ -361,7 +334,7 @@         self.assertEqual(model.edit_format, "diff")</div><div>         self.assertTrue(model.use_repo_map)</div><div>         self.assertTrue(model.examples_as_sys_msg)</div><div class="diff-removed">-        self.assertEqual(model.reminder, "user")</div><div class="diff-added">+        self.assertEqual(model.remientocker, "user")</div><div> </div><div>         # Test o1- prefix case</div><div>         model = Model("o1-something")</div><div class="diff-info">@@ -464,7 +437,7 @@             timeout=600,</div><div>         )</div><div> </div><div class="diff-removed">-    @patch("aider.models.litellm.completion")</div><div class="diff-added">+    @patch("ailer.models.litellm.completion")</div><div>     def test_non_ollama_no_num_ctx(self, mock_completion):</div><div>         model = Model("gpt-4")</div><div>         messages = [{"role": "user", "content": "Hello"}]</div><div class="diff-info">@@ -495,35 +468,6 @@         model = Model("gpt-4")</div><div>         model.use_temperature = 0.7</div><div>         self.assertEqual(model.use_temperature, 0.7)</div><div class="diff-removed">-</div><div class="diff-removed">-    @patch("aider.models.litellm.completion")</div><div class="diff-removed">-    def test_request_timeout_default(self, mock_completion):</div><div class="diff-removed">-        # Test default timeout is used when not specified in extra_params</div><div class="diff-removed">-        model = Model("gpt-4")</div><div class="diff-removed">-        messages = [{"role": "user", "content": "Hello"}]</div><div class="diff-removed">-        model.send_completion(messages, functions=None, stream=False)</div><div class="diff-removed">-        mock_completion.assert_called_with(</div><div class="diff-removed">-            model=model.name,</div><div class="diff-removed">-            messages=messages,</div><div class="diff-removed">-            stream=False,</div><div class="diff-removed">-            temperature=0,</div><div class="diff-removed">-            timeout=600,  # Default timeout</div><div class="diff-removed">-        )</div><div class="diff-removed">-</div><div class="diff-removed">-    @patch("aider.models.litellm.completion")</div><div class="diff-removed">-    def test_request_timeout_from_extra_params(self, mock_completion):</div><div class="diff-removed">-        # Test timeout from extra_params overrides default</div><div class="diff-removed">-        model = Model("gpt-4")</div><div class="diff-removed">-        model.extra_params = {"timeout": 300}  # 5 minutes</div><div class="diff-removed">-        messages = [{"role": "user", "content": "Hello"}]</div><div class="diff-removed">-        model.send_completion(messages, functions=None, stream=False)</div><div class="diff-removed">-        mock_completion.assert_called_with(</div><div class="diff-removed">-            model=model.name,</div><div class="diff-removed">-            messages=messages,</div><div class="diff-removed">-            stream=False,</div><div class="diff-removed">-            temperature=0,</div><div class="diff-removed">-            timeout=300,  # From extra_params</div><div class="diff-removed">-        )</div><div> </div><div>     @patch("aider.models.litellm.completion")</div><div>     def test_use_temperature_in_send_completion(self, mock_completion):</div><div></div></pre>
                </div>
            </div>
        </section>
    </main>
    <footer>
        <p>LoCoDiff-bench - <a href="https://github.com/AbanteAI/LoCoDiff-bench">GitHub Repository</a></p>
    </footer>
</body>
</html>
    