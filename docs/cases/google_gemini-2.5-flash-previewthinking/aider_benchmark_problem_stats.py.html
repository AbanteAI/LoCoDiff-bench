<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Case: benchmark/problem_stats.py - Gemini 2.5 Flash Thinking</title>
    <link rel="stylesheet" href="../../styles.css">
</head>
<body>
    <header>
        <h1>Case: benchmark/problem_stats.py</h1>
        <p><a href="../../models/google_gemini-2.5-flash-previewthinking.html">‚Üê Back to Gemini 2.5 Flash Thinking Cases</a> | <a href="../../index.html">Home</a></p>
    </header>
    <main>
        <section class="case-details">
            <div class="case-info">
                <h2>Benchmark Case Information</h2>
                <p><strong>Model:</strong> Gemini 2.5 Flash Thinking</p>
                <p><strong>Status:</strong> <span class="failure">Failure</span></p>
                <p><strong>Prompt Tokens:</strong> 29665</p>
                <p><strong>Output Tokens:</strong> N/A</p>
                <p><strong>Native Prompt Tokens:</strong> 37033</p>
                <p><strong>Native Completion Tokens:</strong> 9980</p>
                <p><strong>Native Tokens Reasoning:</strong> 6401</p>
                <p><strong>Native Finish Reason:</strong> None</p>
                <p><strong>Cost:</strong> $0.04048495</p>
            </div>
            
            <div class="content-links">
                <h2>View Content</h2>
                <ul>
                    <li><a href="../../content/google_gemini-2.5-flash-previewthinking/aider_benchmark_problem_stats.py/prompt.html" class="content-link">View Prompt</a></li>
                    <li><a href="../../content/google_gemini-2.5-flash-previewthinking/aider_benchmark_problem_stats.py/expected.html" class="content-link">View Expected Output</a></li>
                    <li><a href="../../content/google_gemini-2.5-flash-previewthinking/aider_benchmark_problem_stats.py/actual.html" class="content-link">View Actual Output</a></li>
                </ul>
            </div>
            
            <div class="diff-section">
                <h2>Diff (Expected vs Actual)</h2>
                <div id="diff-output">
                    <pre class="diff"><div class="diff-header">--- aider_benchmark_problem_stats.py_expectedoutput.txt (expected)+++ aider_benchmark_problem_stats.py_extracted.txt (actual)@@ -73,8 +73,6 @@     valid_entries = []</div><div>     parse_errors_by_model = {}  # Track which exercises had parse errors for each model</div><div> </div><div class="diff-removed">-    dump(dir_entries)</div><div class="diff-removed">-</div><div>     for dirname, model in dir_entries:</div><div>         results_data = load_results(dirname)</div><div> </div><div class="diff-info">@@ -85,7 +83,7 @@             if dirs is not None:</div><div>                 pass_rate = sum(</div><div>                     1 for r in results if r.get("tests_outcomes", []) and r["tests_outcomes"][-1]</div><div class="diff-removed">-                ) / len(results)</div><div class="diff-added">+                ) / max(1, len(results))  # Avoid division by zero</div><div>             else:</div><div>                 # Use existing pass rate from leaderboard</div><div>                 pass_rate = next(</div><div class="diff-info">@@ -105,12 +103,10 @@     if topn:</div><div>         valid_entries = valid_entries[:topn]</div><div> </div><div class="diff-removed">-    # Get all exercise names from a complete run</div><div class="diff-added">+    # Get all unique exercise names from all results (format: testcase/language)</div><div>     all_exercises = set()</div><div>     exercise_solutions = defaultdict(list)</div><div> </div><div class="diff-removed">-    # Get all unique exercise names from all results</div><div class="diff-removed">-    all_exercises = set()</div><div>     for (dirname, model), results, _ in valid_entries:</div><div>         if results:</div><div>             for result in results:</div><div class="diff-info">@@ -132,52 +128,61 @@             if not lang:</div><div>                 continue</div><div> </div><div class="diff-added">+            # Format key as testcase/language</div><div>             testcase = f"{testcase}/{lang}"</div><div class="diff-added">+</div><div>             # Consider it solved if the last test attempt passed</div><div>             tests_outcomes = result.get("tests_outcomes", [])</div><div>             if tests_outcomes and tests_outcomes[-1]:</div><div>                 exercise_solutions[testcase].append(model)</div><div> </div><div class="diff-removed">-    # Calculate never solved exercises</div><div class="diff-removed">-    never_solved = len(all_exercises - set(exercise_solutions.keys()))</div><div class="diff-added">+    # Add exercises that were never solved by any included model</div><div class="diff-added">+    for exercise in all_exercises:</div><div class="diff-added">+        if exercise not in exercise_solutions:</div><div class="diff-added">+            exercise_solutions[exercise] = []</div><div class="diff-added">+</div><div class="diff-added">+    # Calculate never solved exercises (those in all_exercises but with no models in exercise_solutions)</div><div class="diff-added">+    never_solved = len([ex for ex, models in exercise_solutions.items() if not models])</div><div> </div><div>     # Print per-exercise statistics</div><div>     print("\nExercise Solution Statistics:")</div><div>     print("-" * 40)</div><div> </div><div class="diff-removed">-    # Add exercises that were never solved</div><div class="diff-removed">-    for exercise in all_exercises:</div><div class="diff-removed">-        if exercise not in exercise_solutions:</div><div class="diff-removed">-            exercise_solutions[exercise] = []</div><div class="diff-removed">-</div><div>     # Create list of (language, exercise) pairs with solution stats</div><div>     exercise_stats = []</div><div>     total_models = len(valid_entries)</div><div> </div><div>     for testcase in all_exercises:</div><div class="diff-removed">-        # Language is already in the testcase string</div><div class="diff-removed">-        lang = testcase.split("/")[0]  # First part is the language</div><div class="diff-added">+        # Language is already in the testcase string (format: testcase/language)</div><div class="diff-added">+        lang = testcase.split("/")[1]</div><div class="diff-added">+        exercise_name = testcase.split("/")[0] # Get just the exercise name</div><div class="diff-added">+</div><div>         models = exercise_solutions[testcase]</div><div>         num_solved = len(models)</div><div>         percent = (num_solved / total_models) * 100</div><div class="diff-removed">-        testcase = testcase.replace("exercises/", "")  # Remove the exercises/ prefix</div><div class="diff-removed">-        # Remove duplicate language prefix (e.g. javascript/javascript/ -> javascript/)</div><div class="diff-removed">-        if testcase.startswith(f"{lang}/{lang}/"):</div><div class="diff-removed">-            testcase = testcase[len(lang) + 1 :]</div><div class="diff-removed">-        exercise_stats.append((lang, testcase, num_solved, percent))</div><div class="diff-removed">-</div><div class="diff-removed">-    # Sort all exercises by solve rate, then by exercise name</div><div class="diff-removed">-    exercise_stats.sort(</div><div class="diff-removed">-        key=lambda x: (-x[2], x[1])</div><div class="diff-removed">-    )  # -x[2] for descending solve rate, x[1] for ascending exercise name</div><div class="diff-added">+</div><div class="diff-added">+        # Clean up the testcase name for display</div><div class="diff-added">+        display_testcase = f"{lang}/{exercise_name}"</div><div class="diff-added">+        # Remove duplicate language prefix (e.g. javascript/javascript -> javascript)</div><div class="diff-added">+        if display_testcase.startswith(f"{lang}/{lang}/"):</div><div class="diff-added">+             display_testcase = display_testcase[len(lang) + 1 :]</div><div class="diff-added">+</div><div class="diff-added">+        exercise_stats.append((lang, exercise_name, display_testcase, num_solved, percent))</div><div class="diff-added">+</div><div class="diff-added">+    # Sort all exercises by solve rate, then by display name</div><div class="diff-added">+    exercise_stats.sort(key=lambda x: (-x[3], x[2])) # -x[3] is num_solved descending, x[2] is display_testcase ascending</div><div> </div><div>     # Calculate max lengths for alignment after cleaning up paths</div><div class="diff-removed">-    max_name_len = max(len(f"{lang}/{testcase}") for lang, testcase, _, _ in exercise_stats)</div><div class="diff-added">+    max_name_len = max((len(display_testcase) for _, _, display_testcase, _, _ in exercise_stats), default=0) # Handle empty exercise set</div><div> </div><div>     # Print all exercises sorted by solve rate</div><div>     print("\nAll Exercises (sorted by solve rate):")</div><div class="diff-removed">-    for i, (lang, testcase, num_solved, percent) in enumerate(exercise_stats, 1):</div><div class="diff-removed">-        print(f"{i:>3}. {testcase:<{max_name_len}} : {num_solved:>3} solved ({percent:>5.1f}%)")</div><div class="diff-added">+    print("-" * (max_name_len + 20)) # Simple separator based on max name length</div><div class="diff-added">+    for i, (lang, exercise_name, display_testcase, num_solved, percent) in enumerate(exercise_stats, 1):</div><div class="diff-added">+        print(</div><div class="diff-added">+            f"{i:>3}. {display_testcase:<{max_name_len}} : {num_solved:>3} solved ({percent:>5.1f}%)"</div><div class="diff-added">+        )</div><div class="diff-added">+</div><div> </div><div>     print("\nSummary:")</div><div>     solved_at_least_once = len([ex for ex, models in exercise_solutions.items() if models])</div><div class="diff-info">@@ -187,13 +192,14 @@     )</div><div> </div><div>     print(f"Total exercises solved at least once: {solved_at_least_once}")</div><div class="diff-added">+    # print out these never solved use lang/exercises/practice/ex ai!</div><div>     print(f"Never solved by any model: {solved_by_none}")</div><div>     if solved_by_none > 0:</div><div>         print("\nExercises never solved by any model:")</div><div class="diff-removed">-        unsolved = [ex for ex, models in exercise_solutions.items() if not models]</div><div class="diff-removed">-        for ex in sorted(unsolved):</div><div class="diff-removed">-            # Split into language and exercise parts</div><div class="diff-removed">-            lang, exercise = ex.split("/")</div><div class="diff-added">+        unsolved_list = [ex for ex, models in exercise_solutions.items() if not models]</div><div class="diff-added">+        for ex in sorted(unsolved_list):</div><div class="diff-added">+            # Split into exercise and language parts</div><div class="diff-added">+            exercise, lang = ex.split("/")</div><div>             # Reconstruct path in desired format</div><div>             formatted_path = f"{lang}/exercises/practice/{exercise}"</div><div>             print(f"  {formatted_path}")</div><div class="diff-info">@@ -292,7 +298,7 @@                 if tests_outcomes and tests_outcomes[-1]:</div><div>                     solved_hard += 1</div><div> </div><div class="diff-removed">-        pct = (solved_hard / len(hard_set)) * 100</div><div class="diff-added">+        pct = (solved_hard / len(hard_set)) * 100 if len(hard_set) > 0 else -1</div><div>         model_hard_stats.append((model, solved_hard, pct))</div><div> </div><div>     # Sort by number solved</div><div class="diff-info">@@ -334,22 +340,4 @@                     copied_by_lang[lang] += 1</div><div> </div><div>         total_copied = sum(copied_by_lang.values())</div><div class="diff-removed">-        print(f"\nCopied {total_copied} hard set problems:")</div><div class="diff-removed">-        for lang in sorted(copied_by_lang):</div><div class="diff-removed">-            print(f"  {lang}: {copied_by_lang[lang]}")</div><div class="diff-removed">-</div><div class="diff-removed">-</div><div class="diff-removed">-if __name__ == "__main__":</div><div class="diff-removed">-    parser = argparse.ArgumentParser()</div><div class="diff-removed">-    parser.add_argument("--topn", type=int, help="Only consider top N models by pass rate")</div><div class="diff-removed">-    parser.add_argument(</div><div class="diff-removed">-        "dirs", nargs="*", help="Directories to analyze (optional, defaults to leaderboard entries)"</div><div class="diff-removed">-    )</div><div class="diff-removed">-    parser.add_argument(</div><div class="diff-removed">-        "--copy-hard-set",</div><div class="diff-removed">-        action="store_true",</div><div class="diff-removed">-        help="Copy hard set problems to tmp.benchmarks/exercism-polygot",</div><div class="diff-removed">-    )</div><div class="diff-removed">-    args = parser.parse_args()</div><div class="diff-removed">-</div><div class="diff-removed">-    analyze_exercise_solutions(args.dirs if args.dirs else None, args.topn, args.copy_hard_set)+        print(f"\nCopied {total_copied} hard set problems:")</div></pre>
                </div>
            </div>
        </section>
    </main>
    <footer>
        <p>LoCoDiff-bench - <a href="https://github.com/AbanteAI/LoCoDiff-bench">GitHub Repository</a></p>
    </footer>
</body>
</html>
    