<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Case: benchmark/problem_stats.py - Grok 4</title>
    <link rel="stylesheet" href="../../styles.css">
</head>
<body>
    <header>
        <h1>Case: benchmark/problem_stats.py</h1>
        <h2>Model: Grok 4</h2>
        <p><a href="../../models/x-ai_grok-4-07-09.html">All Grok 4 Cases</a> | <a href="../../cases.html">All Cases</a> | <a href="../../index.html">Home</a></p>
    </header>
    <main>
        <section class="case-details">
            <div class="case-info">
                <h2>Benchmark Case Information</h2>
                <p><strong>Model:</strong> Grok 4</p>
                <p><strong>Status:</strong> <span class="failure">Failure</span></p>
                <p><strong>Prompt Tokens:</strong> 29665</p>
                <p><strong>Native Prompt Tokens:</strong> 29281</p>
                <p><strong>Native Completion Tokens:</strong> 15950</p>
                <p><strong>Native Tokens Reasoning:</strong> 12867</p>
                <p><strong>Native Finish Reason:</strong> stop</p>
                <p><strong>Cost:</strong> $0.3266205</p>
            </div>
            
            <div class="content-links">
                <h2>View Content</h2>
                <ul>
                    <li><a href="../../content/x-ai_grok-4-07-09/aider_benchmark_problem_stats.py/prompt.html" class="content-link">View Prompt</a></li>
                    <li><a href="../../content/x-ai_grok-4-07-09/aider_benchmark_problem_stats.py/expected.html" class="content-link">View Expected Output</a></li>
                    <li><a href="../../content/x-ai_grok-4-07-09/aider_benchmark_problem_stats.py/actual.html" class="content-link">View Actual Output</a></li>
                </ul>
            </div>
            
            <div class="diff-section">
                <h2>Diff (Expected vs Actual)</h2>
                <div id="diff-output">
                    <pre class="diff"><div></div><div>index 36481d117..8d5d17ad2 100644</div><div class="diff-header">--- a/aider_benchmark_problem_stats.py_expectedoutput.txt (expected):tmp/tmpv07ztp_x_expected.txt	</div><div class="diff-header">+++ b/aider_benchmark_problem_stats.py_extracted.txt (actual):tmp/tmpo76ljvtv_actual.txt	</div><div class="diff-info">@@ -73,11 +73,8 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):</div><div>     valid_entries = []</div><div>     parse_errors_by_model = {}  # Track which exercises had parse errors for each model</div><div> </div><div class="diff-removed">-    dump(dir_entries)</div><div class="diff-removed">-</div><div>     for dirname, model in dir_entries:</div><div>         results_data = load_results(dirname)</div><div class="diff-removed">-</div><div>         if results_data:</div><div>             results, model_parse_errors = results_data</div><div>             parse_errors_by_model[model] = set(model_parse_errors)</div><div class="diff-info">@@ -117,7 +114,7 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):</div><div>                 try:</div><div>                     all_exercises.add(result["testcase"] + "/" + result["language"])</div><div>                 except KeyError:</div><div class="diff-removed">-                    print(f"Warning: Missing testcase in {dirname}", json.dumps(result, indent=4))</div><div class="diff-added">+                    print(f"Warning: Missing testcase in {dirname}")</div><div> </div><div>     for (dirname, model), results, _ in valid_entries:</div><div>         if not results:</div><div class="diff-info">@@ -138,13 +135,6 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):</div><div>             if tests_outcomes and tests_outcomes[-1]:</div><div>                 exercise_solutions[testcase].append(model)</div><div> </div><div class="diff-removed">-    # Calculate never solved exercises</div><div class="diff-removed">-    never_solved = len(all_exercises - set(exercise_solutions.keys()))</div><div class="diff-removed">-</div><div class="diff-removed">-    # Print per-exercise statistics</div><div class="diff-removed">-    print("\nExercise Solution Statistics:")</div><div class="diff-removed">-    print("-" * 40)</div><div class="diff-removed">-</div><div>     # Add exercises that were never solved</div><div>     for exercise in all_exercises:</div><div>         if exercise not in exercise_solutions:</div><div class="diff-info">@@ -156,7 +146,7 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):</div><div> </div><div>     for testcase in all_exercises:</div><div>         # Language is already in the testcase string</div><div class="diff-removed">-        lang = testcase.split("/")[0]  # First part is the language</div><div class="diff-added">+        lang = testcase.split("/")[1]  # First part is the language</div><div>         models = exercise_solutions[testcase]</div><div>         num_solved = len(models)</div><div>         percent = (num_solved / total_models) * 100</div><div class="diff-info">@@ -177,15 +167,16 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):</div><div>     # Print all exercises sorted by solve rate</div><div>     print("\nAll Exercises (sorted by solve rate):")</div><div>     for i, (lang, testcase, num_solved, percent) in enumerate(exercise_stats, 1):</div><div class="diff-removed">-        print(f"{i:>3}. {testcase:<{max_name_len}} : {num_solved:>3} solved ({percent:>5.1f}%)")</div><div class="diff-added">+        print(</div><div class="diff-added">+            f"{i:>3}. {lang}/{testcase:<{max_name_len}} : {num_solved:>3} solved ({percent:>5.1f}%)"</div><div class="diff-added">+        )</div><div> </div><div>     print("\nSummary:")</div><div>     solved_at_least_once = len([ex for ex, models in exercise_solutions.items() if models])</div><div class="diff-removed">-    solved_by_none = never_solved</div><div class="diff-added">+    solved_by_none = len([ex for ex, models in exercise_solutions.items() if not models])</div><div>     solved_by_all = len(</div><div>         [ex for ex, models in exercise_solutions.items() if len(models) == total_models]</div><div>     )</div><div class="diff-removed">-</div><div>     print(f"Total exercises solved at least once: {solved_at_least_once}")</div><div>     print(f"Never solved by any model: {solved_by_none}")</div><div>     if solved_by_none > 0:</div><div class="diff-info">@@ -197,7 +188,7 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):</div><div>             # Reconstruct path in desired format</div><div>             formatted_path = f"{lang}/exercises/practice/{exercise}"</div><div>             print(f"  {formatted_path}")</div><div class="diff-removed">-    print(f"\nSolved by all models: {solved_by_all}")</div><div class="diff-added">+    print(f"Solved by all models: {solved_by_all}")</div><div>     print(</div><div>         f"Total exercises: {len(all_exercises)} = {solved_by_none} (none) + {solved_by_all} (all) +"</div><div>         f" {len(all_exercises) - solved_by_none - solved_by_all} (some)"</div><div class="diff-info">@@ -231,8 +222,7 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):</div><div> </div><div>     if disqualified_exercises:</div><div>         print(</div><div class="diff-removed">-            f"\nDisqualified {len(disqualified_exercises)} exercises with {PARSE_ERROR_M}+ parse"</div><div class="diff-removed">-            " errors:"</div><div class="diff-added">+            f"\nDisqualified {len(disqualified_exercises)} exercises with {PARSE_ERROR_M}+ parse errors:"</div><div>         )</div><div>         for ex in sorted(disqualified_exercises):</div><div>             print(f"  {ex} ({parse_error_counts[ex]} parse errors)")</div><div class="diff-info">@@ -348,7 +338,7 @@ if __name__ == "__main__":</div><div>     parser.add_argument(</div><div>         "--copy-hard-set",</div><div>         action="store_true",</div><div class="diff-removed">-        help="Copy hard set problems to tmp.benchmarks/exercism-polygot",</div><div class="diff-added">+        help="Copy hard set problems to tmp.benchmarks/exercism-polyglot",</div><div>     )</div><div>     args = parser.parse_args()</div><div> </div><div></div></pre>
                </div>
            </div>
        </section>
    </main>
    <footer>
        <p>LoCoDiff-bench - <a href="https://github.com/AbanteAI/LoCoDiff-bench">GitHub Repository</a></p>
    </footer>
</body>
</html>
    