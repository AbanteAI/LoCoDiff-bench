<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Case: aider/history.py - GPT OSS 120B</title>
    <link rel="stylesheet" href="../../styles.css">
</head>
<body>
    <header>
        <h1>Case: aider/history.py</h1>
        <h2>Model: GPT OSS 120B</h2>
        <p><a href="../../models/openai_gpt-oss-120b.html">All GPT OSS 120B Cases</a> | <a href="../../cases.html">All Cases</a> | <a href="../../index.html">Home</a></p>
    </header>
    <main>
        <section class="case-details">
            <div class="case-info">
                <h2>Benchmark Case Information</h2>
                <p><strong>Model:</strong> GPT OSS 120B</p>
                <p><strong>Status:</strong> <span class="failure">Failure</span></p>
                <p><strong>Prompt Tokens:</strong> 18915</p>
                <p><strong>Native Prompt Tokens:</strong> 18995</p>
                <p><strong>Native Completion Tokens:</strong> 5296</p>
                <p><strong>Native Tokens Reasoning:</strong> 5436</p>
                <p><strong>Native Finish Reason:</strong> stop</p>
                <p><strong>Cost:</strong> $0.00682125</p>
            </div>
            
            <div class="content-links">
                <h2>View Content</h2>
                <ul>
                    <li><a href="../../content/openai_gpt-oss-120b/aider_aider_history.py/prompt.html" class="content-link">View Prompt</a></li>
                    <li><a href="../../content/openai_gpt-oss-120b/aider_aider_history.py/expected.html" class="content-link">View Expected Output</a></li>
                    <li><a href="../../content/openai_gpt-oss-120b/aider_aider_history.py/actual.html" class="content-link">View Actual Output</a></li>
                </ul>
            </div>
            
            <div class="diff-section">
                <h2>Diff (Expected vs Actual)</h2>
                <div id="diff-output">
                    <pre class="diff"><div></div><div>index ce6172c9a..18d5313e9 100644</div><div class="diff-header">--- a/aider_aider_history.py_expectedoutput.txt (expected):tmp/tmpv2065ub6_expected.txt	</div><div class="diff-header">+++ b/aider_aider_history.py_extracted.txt (actual):tmp/tmpy0528c27_actual.txt	</div><div class="diff-info">@@ -33,60 +33,40 @@ class ChatSummary:</div><div>     def summarize_real(self, messages, depth=0):</div><div>         if not self.models:</div><div>             raise ValueError("No models available for summarization")</div><div class="diff-added">+        if len(messages) < 2:</div><div class="diff-added">+            return messages</div><div> </div><div>         sized = self.tokenize(messages)</div><div>         total = sum(tokens for tokens, _msg in sized)</div><div>         if total <= self.max_tokens and depth == 0:</div><div>             return messages</div><div> </div><div class="diff-removed">-        min_split = 4</div><div class="diff-removed">-        if len(messages) <= min_split or depth > 3:</div><div class="diff-removed">-            return self.summarize_all(messages)</div><div class="diff-removed">-</div><div class="diff-removed">-        tail_tokens = 0</div><div>         split_index = len(messages)</div><div class="diff-removed">-        half_max_tokens = self.max_tokens // 2</div><div class="diff-added">+        half_max = self.max_tokens // 2</div><div> </div><div class="diff-removed">-        # Iterate over the messages in reverse order</div><div class="diff-added">+        # Accumulate tokens from the end until we reach half the max tokens</div><div class="diff-added">+        tail_tokens = 0</div><div>         for i in range(len(sized) - 1, -1, -1):</div><div class="diff-removed">-            tokens, _msg = sized[i]</div><div class="diff-removed">-            if tail_tokens + tokens < half_max_tokens:</div><div class="diff-added">+            tokens, _ = sized[i]</div><div class="diff-added">+            if tail_tokens + tokens < half_max:</div><div>                 tail_tokens += tokens</div><div>                 split_index = i</div><div>             else:</div><div>                 break</div><div> </div><div>         # Ensure the head ends with an assistant message</div><div class="diff-removed">-        while messages[split_index - 1]["role"] != "assistant" and split_index > 1:</div><div class="diff-added">+        while split_index > 0 and messages[split_index - 1]["role"] != "assistant":</div><div>             split_index -= 1</div><div class="diff-removed">-</div><div class="diff-removed">-        if split_index <= min_split:</div><div class="diff-removed">-            return self.summarize_all(messages)</div><div class="diff-added">+        if split_index == 0:</div><div class="diff-added">+            split_index = 1</div><div> </div><div>         head = messages[:split_index]</div><div>         tail = messages[split_index:]</div><div> </div><div class="diff-removed">-        sized = sized[:split_index]</div><div class="diff-removed">-        head.reverse()</div><div class="diff-removed">-        sized.reverse()</div><div class="diff-removed">-        keep = []</div><div class="diff-removed">-        total = 0</div><div class="diff-removed">-</div><div class="diff-removed">-        # These sometimes come set with value = None</div><div class="diff-removed">-        model_max_input_tokens = self.models[0].info.get("max_input_tokens") or 4096</div><div class="diff-removed">-        model_max_input_tokens -= 512</div><div class="diff-added">+        summary = self.summarize_all(head)</div><div> </div><div class="diff-removed">-        for i in range(split_index):</div><div class="diff-removed">-            total += sized[i][0]</div><div class="diff-removed">-            if total > model_max_input_tokens:</div><div class="diff-removed">-                break</div><div class="diff-removed">-            keep.append(head[i])</div><div class="diff-removed">-</div><div class="diff-removed">-        keep.reverse()</div><div class="diff-removed">-</div><div class="diff-removed">-        summary = self.summarize_all(keep)</div><div class="diff-removed">-</div><div class="diff-removed">-        tail_tokens = sum(tokens for tokens, msg in sized[split_index:])</div><div class="diff-added">+        # Recalculate token counts for the summary and the tail</div><div class="diff-added">+        tail_tokens = sum(tokens for tokens, _msg in sized[split_index:])</div><div>         summary_tokens = self.token_count(summary)</div><div> </div><div>         result = summary + tail</div><div class="diff-info">@@ -106,6 +86,9 @@ class ChatSummary:</div><div>             if not content.endswith("\n"):</div><div>                 content += "\n"</div><div> </div><div class="diff-added">+        # Dump for debugging</div><div class="diff-added">+        dump(content)</div><div class="diff-added">+</div><div>         summarize_messages = [</div><div>             dict(role="system", content=prompts.summarize),</div><div>             dict(role="user", content=content),</div><div class="diff-info">@@ -135,9 +118,8 @@ def main():</div><div>     with open(args.filename, "r") as f:</div><div>         text = f.read()</div><div> </div><div class="diff-removed">-    summary = summarizer.summarize_chat_history_markdown(text)</div><div class="diff-removed">-    dump(summary)</div><div class="diff-removed">-</div><div class="diff-removed">-</div><div class="diff-removed">-if __name__ == "__main__":</div><div class="diff-removed">-    main()</div><div>\ No newline at end of file</div><div class="diff-added">+    # Placeholder for processing the markdown file.</div><div class="diff-added">+    # This example just prints the raw file content.</div><div class="diff-added">+    # In practice, you would convert `text` into a list of messages and</div><div class="diff-added">+    # call `summarizer.summarize(messages)`, then dump the output.</div><div class="diff-added">+    print(text)</div><div>\ No newline at end of file</div><div></div></pre>
                </div>
            </div>
        </section>
    </main>
    <footer>
        <p>LoCoDiff-bench - <a href="https://github.com/AbanteAI/LoCoDiff-bench">GitHub Repository</a></p>
    </footer>
</body>
</html>
    