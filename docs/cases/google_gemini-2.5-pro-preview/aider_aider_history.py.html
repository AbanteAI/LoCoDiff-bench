<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Case: aider/history.py - Gemini 2.5 Pro 05-06</title>
    <link rel="stylesheet" href="../../styles.css">
</head>
<body>
    <header>
        <h1>Case: aider/history.py</h1>
        <p><a href="../../models/google_gemini-2.5-pro-preview.html">‚Üê Back to Gemini 2.5 Pro 05-06 Cases</a> | <a href="../../index.html">Home</a></p>
    </header>
    <main>
        <section class="case-details">
            <div class="case-info">
                <h2>Benchmark Case Information</h2>
                <p><strong>Model:</strong> Gemini 2.5 Pro 05-06</p>
                <p><strong>Status:</strong> <span class="failure">Failure</span></p>
                <p><strong>Prompt Tokens:</strong> 18915</p>
                <p><strong>Output Tokens:</strong> N/A</p>
                <p><strong>Native Prompt Tokens:</strong> 24078</p>
                <p><strong>Native Completion Tokens:</strong> 7152</p>
                <p><strong>Native Tokens Reasoning:</strong> 2201</p>
                <p><strong>Native Finish Reason:</strong> None</p>
                <p><strong>Cost:</strong> $0.1016175</p>
            </div>
            
            <div class="content-links">
                <h2>View Content</h2>
                <ul>
                    <li><a href="../../content/google_gemini-2.5-pro-preview/aider_aider_history.py/prompt.html" class="content-link">View Prompt</a></li>
                    <li><a href="../../content/google_gemini-2.5-pro-preview/aider_aider_history.py/expected.html" class="content-link">View Expected Output</a></li>
                    <li><a href="../../content/google_gemini-2.5-pro-preview/aider_aider_history.py/actual.html" class="content-link">View Actual Output</a></li>
                </ul>
            </div>
            
            <div class="diff-section">
                <h2>Diff (Expected vs Actual)</h2>
                <div id="diff-output">
                    <pre class="diff"><div></div><div>index ce6172c9..e69de29b 100644</div><div class="diff-header">--- a/aider_aider_history.py_expectedoutput.txt (expected):tmp/tmpxan56638_expected.txt	</div><div class="diff-header">+++ b/aider_aider_history.py_extracted.txt (actual):tmp/tmp8bvay63f_actual.txt	</div><div class="diff-info">@@ -1,143 +0,0 @@</div><div class="diff-removed">-import argparse</div><div class="diff-removed">-</div><div class="diff-removed">-from aider import models, prompts</div><div class="diff-removed">-from aider.dump import dump  # noqa: F401</div><div class="diff-removed">-</div><div class="diff-removed">-</div><div class="diff-removed">-class ChatSummary:</div><div class="diff-removed">-    def __init__(self, models=None, max_tokens=1024):</div><div class="diff-removed">-        if not models:</div><div class="diff-removed">-            raise ValueError("At least one model must be provided")</div><div class="diff-removed">-        self.models = models if isinstance(models, list) else [models]</div><div class="diff-removed">-        self.max_tokens = max_tokens</div><div class="diff-removed">-        self.token_count = self.models[0].token_count</div><div class="diff-removed">-</div><div class="diff-removed">-    def too_big(self, messages):</div><div class="diff-removed">-        sized = self.tokenize(messages)</div><div class="diff-removed">-        total = sum(tokens for tokens, _msg in sized)</div><div class="diff-removed">-        return total > self.max_tokens</div><div class="diff-removed">-</div><div class="diff-removed">-    def tokenize(self, messages):</div><div class="diff-removed">-        sized = []</div><div class="diff-removed">-        for msg in messages:</div><div class="diff-removed">-            tokens = self.token_count(msg)</div><div class="diff-removed">-            sized.append((tokens, msg))</div><div class="diff-removed">-        return sized</div><div class="diff-removed">-</div><div class="diff-removed">-    def summarize(self, messages, depth=0):</div><div class="diff-removed">-        messages = self.summarize_real(messages)</div><div class="diff-removed">-        if messages and messages[-1]["role"] != "assistant":</div><div class="diff-removed">-            messages.append(dict(role="assistant", content="Ok."))</div><div class="diff-removed">-        return messages</div><div class="diff-removed">-</div><div class="diff-removed">-    def summarize_real(self, messages, depth=0):</div><div class="diff-removed">-        if not self.models:</div><div class="diff-removed">-            raise ValueError("No models available for summarization")</div><div class="diff-removed">-</div><div class="diff-removed">-        sized = self.tokenize(messages)</div><div class="diff-removed">-        total = sum(tokens for tokens, _msg in sized)</div><div class="diff-removed">-        if total <= self.max_tokens and depth == 0:</div><div class="diff-removed">-            return messages</div><div class="diff-removed">-</div><div class="diff-removed">-        min_split = 4</div><div class="diff-removed">-        if len(messages) <= min_split or depth > 3:</div><div class="diff-removed">-            return self.summarize_all(messages)</div><div class="diff-removed">-</div><div class="diff-removed">-        tail_tokens = 0</div><div class="diff-removed">-        split_index = len(messages)</div><div class="diff-removed">-        half_max_tokens = self.max_tokens // 2</div><div class="diff-removed">-</div><div class="diff-removed">-        # Iterate over the messages in reverse order</div><div class="diff-removed">-        for i in range(len(sized) - 1, -1, -1):</div><div class="diff-removed">-            tokens, _msg = sized[i]</div><div class="diff-removed">-            if tail_tokens + tokens < half_max_tokens:</div><div class="diff-removed">-                tail_tokens += tokens</div><div class="diff-removed">-                split_index = i</div><div class="diff-removed">-            else:</div><div class="diff-removed">-                break</div><div class="diff-removed">-</div><div class="diff-removed">-        # Ensure the head ends with an assistant message</div><div class="diff-removed">-        while messages[split_index - 1]["role"] != "assistant" and split_index > 1:</div><div class="diff-removed">-            split_index -= 1</div><div class="diff-removed">-</div><div class="diff-removed">-        if split_index <= min_split:</div><div class="diff-removed">-            return self.summarize_all(messages)</div><div class="diff-removed">-</div><div class="diff-removed">-        head = messages[:split_index]</div><div class="diff-removed">-        tail = messages[split_index:]</div><div class="diff-removed">-</div><div class="diff-removed">-        sized = sized[:split_index]</div><div class="diff-removed">-        head.reverse()</div><div class="diff-removed">-        sized.reverse()</div><div class="diff-removed">-        keep = []</div><div class="diff-removed">-        total = 0</div><div class="diff-removed">-</div><div class="diff-removed">-        # These sometimes come set with value = None</div><div class="diff-removed">-        model_max_input_tokens = self.models[0].info.get("max_input_tokens") or 4096</div><div class="diff-removed">-        model_max_input_tokens -= 512</div><div class="diff-removed">-</div><div class="diff-removed">-        for i in range(split_index):</div><div class="diff-removed">-            total += sized[i][0]</div><div class="diff-removed">-            if total > model_max_input_tokens:</div><div class="diff-removed">-                break</div><div class="diff-removed">-            keep.append(head[i])</div><div class="diff-removed">-</div><div class="diff-removed">-        keep.reverse()</div><div class="diff-removed">-</div><div class="diff-removed">-        summary = self.summarize_all(keep)</div><div class="diff-removed">-</div><div class="diff-removed">-        tail_tokens = sum(tokens for tokens, msg in sized[split_index:])</div><div class="diff-removed">-        summary_tokens = self.token_count(summary)</div><div class="diff-removed">-</div><div class="diff-removed">-        result = summary + tail</div><div class="diff-removed">-        if summary_tokens + tail_tokens < self.max_tokens:</div><div class="diff-removed">-            return result</div><div class="diff-removed">-</div><div class="diff-removed">-        return self.summarize_real(result, depth + 1)</div><div class="diff-removed">-</div><div class="diff-removed">-    def summarize_all(self, messages):</div><div class="diff-removed">-        content = ""</div><div class="diff-removed">-        for msg in messages:</div><div class="diff-removed">-            role = msg["role"].upper()</div><div class="diff-removed">-            if role not in ("USER", "ASSISTANT"):</div><div class="diff-removed">-                continue</div><div class="diff-removed">-            content += f"# {role}\n"</div><div class="diff-removed">-            content += msg["content"]</div><div class="diff-removed">-            if not content.endswith("\n"):</div><div class="diff-removed">-                content += "\n"</div><div class="diff-removed">-</div><div class="diff-removed">-        summarize_messages = [</div><div class="diff-removed">-            dict(role="system", content=prompts.summarize),</div><div class="diff-removed">-            dict(role="user", content=content),</div><div class="diff-removed">-        ]</div><div class="diff-removed">-</div><div class="diff-removed">-        for model in self.models:</div><div class="diff-removed">-            try:</div><div class="diff-removed">-                summary = model.simple_send_with_retries(summarize_messages)</div><div class="diff-removed">-                if summary is not None:</div><div class="diff-removed">-                    summary = prompts.summary_prefix + summary</div><div class="diff-removed">-                    return [dict(role="user", content=summary)]</div><div class="diff-removed">-            except Exception as e:</div><div class="diff-removed">-                print(f"Summarization failed for model {model.name}: {str(e)}")</div><div class="diff-removed">-</div><div class="diff-removed">-        raise ValueError("summarizer unexpectedly failed for all models")</div><div class="diff-removed">-</div><div class="diff-removed">-</div><div class="diff-removed">-def main():</div><div class="diff-removed">-    parser = argparse.ArgumentParser()</div><div class="diff-removed">-    parser.add_argument("filename", help="Markdown file to parse")</div><div class="diff-removed">-    args = parser.parse_args()</div><div class="diff-removed">-</div><div class="diff-removed">-    model_names = ["gpt-3.5-turbo", "gpt-4"]  # Add more model names as needed</div><div class="diff-removed">-    model_list = [models.Model(name) for name in model_names]</div><div class="diff-removed">-    summarizer = ChatSummary(model_list)</div><div class="diff-removed">-</div><div class="diff-removed">-    with open(args.filename, "r") as f:</div><div class="diff-removed">-        text = f.read()</div><div class="diff-removed">-</div><div class="diff-removed">-    summary = summarizer.summarize_chat_history_markdown(text)</div><div class="diff-removed">-    dump(summary)</div><div class="diff-removed">-</div><div class="diff-removed">-</div><div class="diff-removed">-if __name__ == "__main__":</div><div class="diff-removed">-    main()</div><div>\ No newline at end of file</div><div></div></pre>
                </div>
            </div>
        </section>
    </main>
    <footer>
        <p>LoCoDiff-bench - <a href="https://github.com/AbanteAI/LoCoDiff-bench">GitHub Repository</a></p>
    </footer>
</body>
</html>
    