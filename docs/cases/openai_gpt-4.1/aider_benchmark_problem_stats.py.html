<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Case: benchmark/problem_stats.py - GPT-4.1</title>
    <link rel="stylesheet" href="../../styles.css">
</head>
<body>
    <header>
        <h1>Case: benchmark/problem_stats.py</h1>
        <h2>Model: GPT-4.1</h2>
        <p><a href="../../models/openai_gpt-4.1.html">All GPT-4.1 Cases</a> | <a href="../../cases.html">All Cases</a> | <a href="../../index.html">Home</a></p>
    </header>
    <main>
        <section class="case-details">
            <div class="case-info">
                <h2>Benchmark Case Information</h2>
                <p><strong>Model:</strong> GPT-4.1</p>
                <p><strong>Status:</strong> <span class="failure">Failure</span></p>
                <p><strong>Prompt Tokens:</strong> 29665</p>
                <p><strong>Native Prompt Tokens:</strong> 29985</p>
                <p><strong>Native Completion Tokens:</strong> 3156</p>
                <p><strong>Native Tokens Reasoning:</strong> 0</p>
                <p><strong>Native Finish Reason:</strong> stop</p>
                <p><strong>Cost:</strong> $0.0042609</p>
            </div>
            
            <div class="content-links">
                <h2>View Content</h2>
                <ul>
                    <li><a href="../../content/openai_gpt-4.1/aider_benchmark_problem_stats.py/prompt.html" class="content-link">View Prompt</a></li>
                    <li><a href="../../content/openai_gpt-4.1/aider_benchmark_problem_stats.py/expected.html" class="content-link">View Expected Output</a></li>
                    <li><a href="../../content/openai_gpt-4.1/aider_benchmark_problem_stats.py/actual.html" class="content-link">View Actual Output</a></li>
                </ul>
            </div>
            
            <div class="diff-section">
                <h2>Diff (Expected vs Actual)</h2>
                <div id="diff-output">
                    <pre class="diff"><div></div><div>index 36481d11..531ff7dc 100644</div><div class="diff-header">--- a/aider_benchmark_problem_stats.py_expectedoutput.txt (expected):tmp/tmp8gmpyxng_expected.txt	</div><div class="diff-header">+++ b/aider_benchmark_problem_stats.py_extracted.txt (actual):tmp/tmpcc0fok77_actual.txt	</div><div class="diff-info">@@ -61,7 +61,6 @@ def load_results(dirname):</div><div> </div><div> def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):</div><div>     PARSE_ERROR_M = 4  # Threshold for number of parse errors to DQ an exercise</div><div class="diff-removed">-</div><div>     if dirs is None:</div><div>         # Use leaderboard data if no directories specified</div><div>         dir_entries = get_dirs_from_leaderboard()</div><div class="diff-info">@@ -84,7 +83,9 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):</div><div>             # Calculate pass rate for sorting when using custom dirs</div><div>             if dirs is not None:</div><div>                 pass_rate = sum(</div><div class="diff-removed">-                    1 for r in results if r.get("tests_outcomes", []) and r["tests_outcomes"][-1]</div><div class="diff-added">+                    1</div><div class="diff-added">+                    for r in results</div><div class="diff-added">+                    if r.get("tests_outcomes", []) and r["tests_outcomes"][-1]</div><div>                 ) / len(results)</div><div>             else:</div><div>                 # Use existing pass rate from leaderboard</div><div class="diff-info">@@ -105,12 +106,10 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):</div><div>     if topn:</div><div>         valid_entries = valid_entries[:topn]</div><div> </div><div class="diff-removed">-    # Get all exercise names from a complete run</div><div class="diff-added">+    # Get all unique exercise names from all results</div><div>     all_exercises = set()</div><div>     exercise_solutions = defaultdict(list)</div><div> </div><div class="diff-removed">-    # Get all unique exercise names from all results</div><div class="diff-removed">-    all_exercises = set()</div><div>     for (dirname, model), results, _ in valid_entries:</div><div>         if results:</div><div>             for result in results:</div><div class="diff-info">@@ -123,7 +122,6 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):</div><div>         if not results:</div><div>             print(f"Could not load results for {dirname}")</div><div>             continue</div><div class="diff-removed">-</div><div>         for result in results:</div><div>             testcase = result.get("testcase")</div><div>             if not testcase:</div><div class="diff-info">@@ -138,25 +136,25 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):</div><div>             if tests_outcomes and tests_outcomes[-1]:</div><div>                 exercise_solutions[testcase].append(model)</div><div> </div><div class="diff-added">+    # Add exercises that were never solved</div><div class="diff-added">+    for exercise in all_exercises:</div><div class="diff-added">+        if exercise not in exercise_solutions:</div><div class="diff-added">+            exercise_solutions[exercise] = []</div><div class="diff-added">+</div><div>     # Calculate never solved exercises</div><div class="diff-removed">-    never_solved = len(all_exercises - set(exercise_solutions.keys()))</div><div class="diff-added">+    never_solved = len([e for e in all_exercises if not exercise_solutions[e]])</div><div> </div><div>     # Print per-exercise statistics</div><div>     print("\nExercise Solution Statistics:")</div><div>     print("-" * 40)</div><div> </div><div class="diff-removed">-    # Add exercises that were never solved</div><div class="diff-removed">-    for exercise in all_exercises:</div><div class="diff-removed">-        if exercise not in exercise_solutions:</div><div class="diff-removed">-            exercise_solutions[exercise] = []</div><div class="diff-removed">-</div><div>     # Create list of (language, exercise) pairs with solution stats</div><div>     exercise_stats = []</div><div>     total_models = len(valid_entries)</div><div> </div><div>     for testcase in all_exercises:</div><div>         # Language is already in the testcase string</div><div class="diff-removed">-        lang = testcase.split("/")[0]  # First part is the language</div><div class="diff-added">+        lang = testcase.split("/")[1]  # First part is the language</div><div>         models = exercise_solutions[testcase]</div><div>         num_solved = len(models)</div><div>         percent = (num_solved / total_models) * 100</div><div class="diff-info">@@ -292,7 +290,7 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):</div><div>                 if tests_outcomes and tests_outcomes[-1]:</div><div>                     solved_hard += 1</div><div> </div><div class="diff-removed">-        pct = (solved_hard / len(hard_set)) * 100</div><div class="diff-added">+        pct = (solved_hard / len(hard_set)) * 100 if hard_set else 0</div><div>         model_hard_stats.append((model, solved_hard, pct))</div><div> </div><div>     # Sort by number solved</div><div></div></pre>
                </div>
            </div>
        </section>
    </main>
    <footer>
        <p>LoCoDiff-bench - <a href="https://github.com/AbanteAI/LoCoDiff-bench">GitHub Repository</a></p>
    </footer>
</body>
</html>
    