<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Case: benchmark/problem_stats.py - o3</title>
    <link rel="stylesheet" href="../../styles.css">
</head>
<body>
    <header>
        <h1>Case: benchmark/problem_stats.py</h1>
        <h2>Model: o3</h2>
        <p><a href="../../models/openai_o3.html">All o3 Cases</a> | <a href="../../cases.html">All Cases</a> | <a href="../../index.html">Home</a></p>
    </header>
    <main>
        <section class="case-details">
            <div class="case-info">
                <h2>Benchmark Case Information</h2>
                <p><strong>Model:</strong> o3</p>
                <p><strong>Status:</strong> <span class="failure">Failure</span></p>
                <p><strong>Prompt Tokens:</strong> 29665</p>
                <p><strong>Native Prompt Tokens:</strong> 29984</p>
                <p><strong>Native Completion Tokens:</strong> 5447</p>
                <p><strong>Native Tokens Reasoning:</strong> 2368</p>
                <p><strong>Native Finish Reason:</strong> stop</p>
                <p><strong>Cost:</strong> $0.5436059999999999</p>
            </div>
            
            <div class="content-links">
                <h2>View Content</h2>
                <ul>
                    <li><a href="../../content/openai_o3/aider_benchmark_problem_stats.py/prompt.html" class="content-link">View Prompt</a></li>
                    <li><a href="../../content/openai_o3/aider_benchmark_problem_stats.py/expected.html" class="content-link">View Expected Output</a></li>
                    <li><a href="../../content/openai_o3/aider_benchmark_problem_stats.py/actual.html" class="content-link">View Actual Output</a></li>
                </ul>
            </div>
            
            <div class="diff-section">
                <h2>Diff (Expected vs Actual)</h2>
                <div id="diff-output">
                    <pre class="diff"><div></div><div>index 36481d11..7018af71 100644</div><div class="diff-header">--- a/aider_benchmark_problem_stats.py_expectedoutput.txt (expected):tmp/tmpfxog99r1_expected.txt	</div><div class="diff-header">+++ b/aider_benchmark_problem_stats.py_extracted.txt (actual):tmp/tmp9sa2ekf4_actual.txt	</div><div class="diff-info">@@ -1,4 +1,16 @@</div><div> #!/usr/bin/env python</div><div class="diff-added">+"""</div><div class="diff-added">+Analyze benchmark results for Exercism “polyglot” runs.</div><div class="diff-added">+</div><div class="diff-added">+This script scans the benchmark result JSON blobs produced by aider, tallies</div><div class="diff-added">+which models solved which Exercism practice exercises, and prints a variety of</div><div class="diff-added">+stats.  It can also copy the “hard set” (poorly-solved) exercises into a new</div><div class="diff-added">+directory for further study.</div><div class="diff-added">+</div><div class="diff-added">+The script intentionally keeps lots of debugging and exploratory output that is</div><div class="diff-added">+useful when iterating on benchmarking.  Accordingly, the code style is a bit</div><div class="diff-added">+looser than production quality.</div><div class="diff-added">+"""</div><div> </div><div> import argparse</div><div> import json</div><div class="diff-info">@@ -7,23 +19,29 @@ from collections import defaultdict</div><div> from pathlib import Path</div><div> </div><div> import yaml</div><div class="diff-removed">-</div><div> from aider.dump import dump  # noqa</div><div> </div><div class="diff-removed">-HARD_SET_NUM = 3  # Number of models that defines the hard set threshold</div><div class="diff-added">+HARD_SET_NUM = 3  # Number of models (≤) that defines the hard-set threshold</div><div> </div><div> </div><div> def get_dirs_from_leaderboard():</div><div class="diff-removed">-    # Load the leaderboard data</div><div class="diff-added">+    """Return (dirname, model) tuples from the polyglot leaderboard."""</div><div>     with open("aider/website/_data/aider_benchmark_problem_stats.py_expectedoutput.txt (expected):</div><div>         leaderboard = yaml.safe_load(f)</div><div>     return [(entry["dirname"], entry["model"]) for entry in leaderboard]</div><div> </div><div> </div><div> def load_results(dirname):</div><div class="diff-removed">-    """Load all result files from a benchmark directory"""</div><div class="diff-added">+    """</div><div class="diff-added">+    Load all .aider.results.json blobs for a benchmark directory.</div><div class="diff-added">+</div><div class="diff-added">+    Returns a tuple: (results_list, parse_error_exercises)</div><div class="diff-added">+    – results_list            : list of dicts for successfully parsed results</div><div class="diff-added">+    – parse_error_exercises   : list of exercise strings that failed to parse</div><div class="diff-added">+    """</div><div>     dirname = Path(dirname)</div><div> </div><div class="diff-added">+    # Allow callers to pass either the full path or just the leaf “benchmark id”</div><div>     benchmark_dir = dirname</div><div>     if not benchmark_dir.exists():</div><div>         benchmark_dir = Path("tmp.benchmarks") / dirname</div><div class="diff-info">@@ -31,63 +49,60 @@ def load_results(dirname):</div><div>             return None</div><div> </div><div>     all_results = []</div><div class="diff-removed">-    parse_errors = []  # Track which exercises had parse errors for this model</div><div class="diff-added">+    parse_errors = []</div><div> </div><div class="diff-removed">-    # Look in language subdirectories under exercises/practice</div><div class="diff-added">+    # Look in language sub-dirs: */exercises/practice/*/.aider.results.json</div><div>     for fname in benchmark_dir.glob("*/exercises/practice/*/.aider.results.json"):</div><div>         error = False</div><div>         try:</div><div>             results = json.loads(fname.read_text())</div><div>             error = "testcase" not in results</div><div>             if not error:</div><div class="diff-removed">-                # Add language info to results</div><div class="diff-removed">-                lang = fname.parts[-5]  # Get language from path</div><div class="diff-added">+                lang = fname.parts[-5]  # language component of the path</div><div>                 results["language"] = lang</div><div>                 all_results.append(results)</div><div class="diff-removed">-</div><div>         except json.JSONDecodeError:</div><div>             error = True</div><div> </div><div>         if error:</div><div class="diff-removed">-            # Track the parse error for this exercise/model combination</div><div class="diff-added">+            # Track which exercise failed for later disqualification</div><div>             lang = fname.parts[-5]</div><div class="diff-removed">-            exercise = f"{fname.parts[-2]}/{lang}"  # Use directory name as testcase</div><div class="diff-added">+            exercise = f"{fname.parts[-2]}/{lang}"</div><div>             parse_errors.append(exercise)</div><div>             print(f"Bad results file {fname}")</div><div class="diff-removed">-            continue</div><div> </div><div>     return all_results, parse_errors</div><div> </div><div> </div><div> def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):</div><div class="diff-removed">-    PARSE_ERROR_M = 4  # Threshold for number of parse errors to DQ an exercise</div><div class="diff-added">+    PARSE_ERROR_M = 4  # Disqualify exercises with ≥M parse errors</div><div> </div><div class="diff-added">+    # Build list of (dirname, model) entries</div><div>     if dirs is None:</div><div class="diff-removed">-        # Use leaderboard data if no directories specified</div><div>         dir_entries = get_dirs_from_leaderboard()</div><div>     else:</div><div class="diff-removed">-        # Use provided directories, with dirname as model name</div><div class="diff-removed">-        dir_entries = [(d, d) for d in dirs]</div><div class="diff-added">+        dir_entries = [(d, d) for d in dirs]  # Use dir name as “model” label</div><div> </div><div class="diff-removed">-    # Filter out entries that don't load and sort by pass rate</div><div class="diff-removed">-    valid_entries = []</div><div class="diff-removed">-    parse_errors_by_model = {}  # Track which exercises had parse errors for each model</div><div class="diff-added">+    valid_entries = []  # [( (dirname, model), results, pass_rate ), …]</div><div class="diff-added">+    parse_errors_by_model = {}</div><div> </div><div>     dump(dir_entries)</div><div> </div><div>     for dirname, model in dir_entries:</div><div>         results_data = load_results(dirname)</div><div class="diff-removed">-</div><div>         if results_data:</div><div>             results, model_parse_errors = results_data</div><div>             parse_errors_by_model[model] = set(model_parse_errors)</div><div class="diff-removed">-            # Calculate pass rate for sorting when using custom dirs</div><div class="diff-added">+</div><div class="diff-added">+            # Compute pass rate for custom dirs; otherwise pull from leaderboard</div><div>             if dirs is not None:</div><div class="diff-removed">-                pass_rate = sum(</div><div class="diff-removed">-                    1 for r in results if r.get("tests_outcomes", []) and r["tests_outcomes"][-1]</div><div class="diff-removed">-                ) / len(results)</div><div class="diff-added">+                solved = sum(</div><div class="diff-added">+                    1</div><div class="diff-added">+                    for r in results</div><div class="diff-added">+                    if r.get("tests_outcomes", []) and r["tests_outcomes"][-1]</div><div class="diff-added">+                )</div><div class="diff-added">+                pass_rate = solved / len(results) if results else 0</div><div>             else:</div><div class="diff-removed">-                # Use existing pass rate from leaderboard</div><div>                 pass_rate = next(</div><div>                     (</div><div>                         entry["pass_rate_2"]</div><div class="diff-info">@@ -98,146 +113,123 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):</div><div>                     ),</div><div>                     0,</div><div>                 )</div><div class="diff-added">+</div><div>             valid_entries.append(((dirname, model), results, float(pass_rate)))</div><div> </div><div class="diff-removed">-    # Sort by pass rate and take top N if specified</div><div class="diff-added">+    # Sort by pass rate and truncate to topn if requested</div><div>     valid_entries.sort(key=lambda x: x[2], reverse=True)</div><div>     if topn:</div><div class="diff-removed">-        valid_entries = valid_entries[:topn]</div><div class="diff-added">+        valid_entries = valid_entries[: topn]</div><div> </div><div class="diff-removed">-    # Get all exercise names from a complete run</div><div class="diff-added">+    # Gather all exercise names (exercise/language)</div><div>     all_exercises = set()</div><div class="diff-removed">-    exercise_solutions = defaultdict(list)</div><div class="diff-added">+    exercise_solutions = defaultdict(list)  # exercise → [models]</div><div> </div><div class="diff-removed">-    # Get all unique exercise names from all results</div><div class="diff-removed">-    all_exercises = set()</div><div>     for (dirname, model), results, _ in valid_entries:</div><div>         if results:</div><div>             for result in results:</div><div>                 try:</div><div class="diff-removed">-                    all_exercises.add(result["testcase"] + "/" + result["language"])</div><div class="diff-added">+                    all_exercises.add(f'{result["testcase"]}/{result["language"]}')</div><div>                 except KeyError:</div><div class="diff-removed">-                    print(f"Warning: Missing testcase in {dirname}", json.dumps(result, indent=4))</div><div class="diff-added">+                    print(</div><div class="diff-added">+                        f"Warning: Missing testcase in {dirname}",</div><div class="diff-added">+                        json.dumps(result, indent=4),</div><div class="diff-added">+                    )</div><div> </div><div class="diff-added">+    # Populate per-exercise solutions</div><div>     for (dirname, model), results, _ in valid_entries:</div><div>         if not results:</div><div>             print(f"Could not load results for {dirname}")</div><div>             continue</div><div class="diff-removed">-</div><div>         for result in results:</div><div>             testcase = result.get("testcase")</div><div class="diff-removed">-            if not testcase:</div><div class="diff-removed">-                continue</div><div>             lang = result.get("language")</div><div class="diff-removed">-            if not lang:</div><div class="diff-added">+            if not testcase or not lang:</div><div>                 continue</div><div class="diff-removed">-</div><div class="diff-removed">-            testcase = f"{testcase}/{lang}"</div><div class="diff-removed">-            # Consider it solved if the last test attempt passed</div><div class="diff-added">+            testcase_combined = f"{testcase}/{lang}"</div><div>             tests_outcomes = result.get("tests_outcomes", [])</div><div>             if tests_outcomes and tests_outcomes[-1]:</div><div class="diff-removed">-                exercise_solutions[testcase].append(model)</div><div class="diff-removed">-</div><div class="diff-removed">-    # Calculate never solved exercises</div><div class="diff-removed">-    never_solved = len(all_exercises - set(exercise_solutions.keys()))</div><div class="diff-removed">-</div><div class="diff-removed">-    # Print per-exercise statistics</div><div class="diff-removed">-    print("\nExercise Solution Statistics:")</div><div class="diff-removed">-    print("-" * 40)</div><div class="diff-added">+                exercise_solutions[testcase_combined].append(model)</div><div> </div><div class="diff-removed">-    # Add exercises that were never solved</div><div class="diff-added">+    # Ensure every exercise key exists (even if unsolved)</div><div>     for exercise in all_exercises:</div><div class="diff-removed">-        if exercise not in exercise_solutions:</div><div class="diff-removed">-            exercise_solutions[exercise] = []</div><div class="diff-added">+        exercise_solutions.setdefault(exercise, [])</div><div> </div><div class="diff-removed">-    # Create list of (language, exercise) pairs with solution stats</div><div class="diff-removed">-    exercise_stats = []</div><div class="diff-added">+    # Per-exercise solve stats -------------------------------------------------</div><div>     total_models = len(valid_entries)</div><div> </div><div class="diff-removed">-    for testcase in all_exercises:</div><div class="diff-removed">-        # Language is already in the testcase string</div><div class="diff-removed">-        lang = testcase.split("/")[0]  # First part is the language</div><div class="diff-removed">-        models = exercise_solutions[testcase]</div><div class="diff-added">+    exercise_stats = []</div><div class="diff-added">+    for exercise in all_exercises:</div><div class="diff-added">+        lang = exercise.split("/")[0]  # already “exercise/lang”</div><div class="diff-added">+        models = exercise_solutions[exercise]</div><div>         num_solved = len(models)</div><div class="diff-removed">-        percent = (num_solved / total_models) * 100</div><div class="diff-removed">-        testcase = testcase.replace("exercises/", "")  # Remove the exercises/ prefix</div><div class="diff-removed">-        # Remove duplicate language prefix (e.g. javascript/javascript/ -> javascript/)</div><div class="diff-removed">-        if testcase.startswith(f"{lang}/{lang}/"):</div><div class="diff-removed">-            testcase = testcase[len(lang) + 1 :]</div><div class="diff-removed">-        exercise_stats.append((lang, testcase, num_solved, percent))</div><div class="diff-removed">-</div><div class="diff-removed">-    # Sort all exercises by solve rate, then by exercise name</div><div class="diff-removed">-    exercise_stats.sort(</div><div class="diff-removed">-        key=lambda x: (-x[2], x[1])</div><div class="diff-removed">-    )  # -x[2] for descending solve rate, x[1] for ascending exercise name</div><div class="diff-removed">-</div><div class="diff-removed">-    # Calculate max lengths for alignment after cleaning up paths</div><div class="diff-removed">-    max_name_len = max(len(f"{lang}/{testcase}") for lang, testcase, _, _ in exercise_stats)</div><div class="diff-removed">-</div><div class="diff-removed">-    # Print all exercises sorted by solve rate</div><div class="diff-added">+        percent = (num_solved / total_models) * 100 if total_models else 0</div><div class="diff-added">+        cleaned = exercise.replace("exercises/", "")</div><div class="diff-added">+        if cleaned.startswith(f"{lang}/{lang}/"):</div><div class="diff-added">+            cleaned = cleaned[len(lang) + 1 :]</div><div class="diff-added">+        exercise_stats.append((lang, cleaned, num_solved, percent))</div><div class="diff-added">+</div><div class="diff-added">+    # Sort by solve rate (desc), then name (asc)</div><div class="diff-added">+    exercise_stats.sort(key=lambda x: (-x[2], x[1]))</div><div class="diff-added">+    max_name_len = max(len(f"{lang}/{ex}") for lang, ex, _, _ in exercise_stats)</div><div class="diff-added">+</div><div>     print("\nAll Exercises (sorted by solve rate):")</div><div class="diff-removed">-    for i, (lang, testcase, num_solved, percent) in enumerate(exercise_stats, 1):</div><div class="diff-added">+    for i, (_, testcase, num_solved, percent) in enumerate(exercise_stats, 1):</div><div>         print(f"{i:>3}. {testcase:<{max_name_len}} : {num_solved:>3} solved ({percent:>5.1f}%)")</div><div> </div><div class="diff-removed">-    print("\nSummary:")</div><div class="diff-removed">-    solved_at_least_once = len([ex for ex, models in exercise_solutions.items() if models])</div><div class="diff-removed">-    solved_by_none = never_solved</div><div class="diff-removed">-    solved_by_all = len(</div><div class="diff-removed">-        [ex for ex, models in exercise_solutions.items() if len(models) == total_models]</div><div class="diff-removed">-    )</div><div class="diff-added">+    # Summary -----------------------------------------------------------------</div><div class="diff-added">+    solved_by_none = len([ex for ex, models in exercise_solutions.items() if not models])</div><div class="diff-added">+    solved_by_all = len([ex for ex, models in exercise_solutions.items() if len(models) == total_models])</div><div class="diff-added">+    solved_at_least_once = len(all_exercises) - solved_by_none</div><div class="diff-added">+    never_solved = solved_by_none</div><div> </div><div class="diff-added">+    print("\nSummary:")</div><div>     print(f"Total exercises solved at least once: {solved_at_least_once}")</div><div class="diff-removed">-    print(f"Never solved by any model: {solved_by_none}")</div><div class="diff-removed">-    if solved_by_none > 0:</div><div class="diff-added">+    print(f"Never solved by any model: {never_solved}")</div><div class="diff-added">+    if never_solved:</div><div>         print("\nExercises never solved by any model:")</div><div class="diff-removed">-        unsolved = [ex for ex, models in exercise_solutions.items() if not models]</div><div class="diff-removed">-        for ex in sorted(unsolved):</div><div class="diff-removed">-            # Split into language and exercise parts</div><div class="diff-added">+        for ex in sorted(ex for ex, models in exercise_solutions.items() if not models):</div><div>             lang, exercise = ex.split("/")</div><div class="diff-removed">-            # Reconstruct path in desired format</div><div class="diff-removed">-            formatted_path = f"{lang}/exercises/practice/{exercise}"</div><div class="diff-removed">-            print(f"  {formatted_path}")</div><div class="diff-added">+            print(f"  {lang}/exercises/practice/{exercise}")</div><div>     print(f"\nSolved by all models: {solved_by_all}")</div><div>     print(</div><div class="diff-removed">-        f"Total exercises: {len(all_exercises)} = {solved_by_none} (none) + {solved_by_all} (all) +"</div><div class="diff-removed">-        f" {len(all_exercises) - solved_by_none - solved_by_all} (some)"</div><div class="diff-added">+        f"Total exercises: {len(all_exercises)} = {never_solved} (none) + "</div><div class="diff-added">+        f"{solved_by_all} (all) + {len(all_exercises) - never_solved - solved_by_all} (some)"</div><div>     )</div><div> </div><div class="diff-removed">-    # Distribution table of how many models solved each exercise</div><div class="diff-added">+    # Distribution table ------------------------------------------------------</div><div>     print("\nDistribution of solutions:")</div><div>     print("Models  Exercises  Cumulative  RevCumulative")</div><div>     print("-" * 50)</div><div>     counts = [0] * (total_models + 1)</div><div class="diff-removed">-    for ex, models in exercise_solutions.items():</div><div class="diff-added">+    for models in exercise_solutions.values():</div><div>         counts[len(models)] += 1</div><div> </div><div>     cumsum = 0</div><div class="diff-removed">-    revcumsum = sum(counts)  # Start with total number of exercises</div><div class="diff-added">+    revcumsum = sum(counts)</div><div>     for i, count in enumerate(counts):</div><div>         cumsum += count</div><div>         print(f"{i:>6d}  {count:>9d}  {cumsum:>10d}  {revcumsum:>12d}")</div><div class="diff-removed">-        revcumsum -= count  # Decrement the reverse cumulative sum</div><div class="diff-added">+        revcumsum -= count</div><div> </div><div class="diff-removed">-    # Count parse errors per exercise</div><div class="diff-added">+    # Disqualify exercises with many parse errors ----------------------------</div><div>     parse_error_counts = defaultdict(int)</div><div>     for model_errors in parse_errors_by_model.values():</div><div>         for exercise in model_errors:</div><div>             parse_error_counts[exercise] += 1</div><div> </div><div class="diff-removed">-    # Find exercises to disqualify based on parse error threshold</div><div>     disqualified_exercises = {</div><div class="diff-removed">-        exercise for exercise, count in parse_error_counts.items() if count >= PARSE_ERROR_M</div><div class="diff-added">+        ex for ex, cnt in parse_error_counts.items() if cnt >= PARSE_ERROR_M</div><div>     }</div><div class="diff-removed">-</div><div>     if disqualified_exercises:</div><div>         print(</div><div class="diff-removed">-            f"\nDisqualified {len(disqualified_exercises)} exercises with {PARSE_ERROR_M}+ parse"</div><div class="diff-removed">-            " errors:"</div><div class="diff-added">+            f"\nDisqualified {len(disqualified_exercises)} exercises with "</div><div class="diff-added">+            f"{PARSE_ERROR_M}+ parse errors:"</div><div>         )</div><div>         for ex in sorted(disqualified_exercises):</div><div>             print(f"  {ex} ({parse_error_counts[ex]} parse errors)")</div><div> </div><div class="diff-removed">-    # Collect the hard set (exercises solved by HARD_SET_NUM or fewer models)</div><div class="diff-added">+    # Hard-set (poorly solved) analysis --------------------------------------</div><div>     print(f"\nHard Set Analysis (exercises solved by ≤{HARD_SET_NUM} models):")</div><div>     print("-" * 60)</div><div>     hard_set = {</div><div class="diff-info">@@ -247,23 +239,23 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):</div><div>     }</div><div>     print(f"Total hard set exercises: {len(hard_set)}")</div><div> </div><div class="diff-removed">-    # Count total problems, unsolved problems, and hard set problems by language</div><div class="diff-added">+    # Per-language unsolved & hard-set counts</div><div>     lang_totals = defaultdict(int)</div><div>     lang_unsolved = defaultdict(int)</div><div>     lang_hard_set = defaultdict(int)</div><div> </div><div>     for exercise in all_exercises:</div><div class="diff-removed">-        lang = exercise.split("/")[1]  # Get language from path</div><div class="diff-added">+        _, lang = exercise.split("/")</div><div>         lang_totals[lang] += 1</div><div class="diff-removed">-        if not exercise_solutions[exercise]:  # No models solved this exercise</div><div class="diff-added">+        if not exercise_solutions[exercise]:</div><div>             lang_unsolved[lang] += 1</div><div class="diff-removed">-        if exercise in hard_set:  # Exercise is in the hard set</div><div class="diff-added">+        if exercise in hard_set:</div><div>             lang_hard_set[lang] += 1</div><div> </div><div>     print("\nUnsolved and hard set problems by language:")</div><div>     print(f"{'Language':<12} {'Unsolved':>8} {'Hard Set':>9} {'Total':>7} {'%hardUnsolved':>8}")</div><div>     print("-" * 47)</div><div class="diff-removed">-    for lang in sorted(lang_totals.keys()):</div><div class="diff-added">+    for lang in sorted(lang_totals):</div><div>         count = lang_unsolved[lang]</div><div>         hard = lang_hard_set[lang]</div><div>         total = lang_totals[lang]</div><div class="diff-info">@@ -271,31 +263,24 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):</div><div>         print(f"{lang:<12} {count:>8} {hard:>9} {total:>7} {pct:>7.1f}%")</div><div>     print()</div><div> </div><div class="diff-removed">-    # For each model, compute performance on hard set</div><div class="diff-added">+    # Per-model performance on hard set</div><div>     model_hard_stats = []</div><div>     for (dirname, model), results, _ in valid_entries:</div><div>         if not results:</div><div>             continue</div><div class="diff-removed">-</div><div>         solved_hard = 0</div><div>         for result in results:</div><div>             testcase = result.get("testcase")</div><div class="diff-removed">-            if not testcase:</div><div class="diff-removed">-                continue</div><div>             lang = result.get("language")</div><div class="diff-removed">-            if not lang:</div><div class="diff-added">+            if not testcase or not lang:</div><div>                 continue</div><div class="diff-removed">-</div><div class="diff-removed">-            testcase = f"{testcase}/{lang}"</div><div class="diff-removed">-            if testcase in hard_set:</div><div class="diff-removed">-                tests_outcomes = result.get("tests_outcomes", [])</div><div class="diff-removed">-                if tests_outcomes and tests_outcomes[-1]:</div><div class="diff-added">+            combined = f"{testcase}/{lang}"</div><div class="diff-added">+            if combined in hard_set:</div><div class="diff-added">+                if result.get("tests_outcomes", []) and result["tests_outcomes"][-1]:</div><div>                     solved_hard += 1</div><div class="diff-removed">-</div><div class="diff-removed">-        pct = (solved_hard / len(hard_set)) * 100</div><div class="diff-added">+        pct = (solved_hard / len(hard_set)) * 100 if hard_set else 0</div><div>         model_hard_stats.append((model, solved_hard, pct))</div><div> </div><div class="diff-removed">-    # Sort by number solved</div><div>     model_hard_stats.sort(key=lambda x: x[1], reverse=True)</div><div> </div><div>     print("\nModel performance on hard set:")</div><div class="diff-info">@@ -304,27 +289,23 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):</div><div>     for model, solved, pct in model_hard_stats:</div><div>         print(f"{model:<55} {solved:>6d}   {pct:>6.1f}%")</div><div> </div><div class="diff-added">+    # Optionally copy hard-set problems for manual inspection ---------------</div><div>     if copy_hard_set:</div><div class="diff-removed">-        # Create hard set directory</div><div>         src_dir = Path("tmp.benchmarks/exercism")</div><div>         dst_dir = Path("tmp.benchmarks/exercism-polyglot")</div><div> </div><div>         if dst_dir.exists():</div><div class="diff-removed">-            print(f"\nError: Destination directory {dst_dir} already exists")</div><div class="diff-added">+            print(f"\nError: destination directory {dst_dir} already exists")</div><div>             return</div><div> </div><div>         print(f"\nCopying hard set problems to {dst_dir}...")</div><div class="diff-removed">-</div><div class="diff-removed">-        # Create a set of (exercise, language) pairs from hard_set</div><div>         hard_set_pairs = {tuple(exercise.split("/")) for exercise in hard_set}</div><div> </div><div class="diff-removed">-        # Copy each hard set problem's directory</div><div>         copied_by_lang = defaultdict(int)</div><div>         for lang_dir in src_dir.glob("*/exercises/practice"):</div><div>             if not lang_dir.is_dir():</div><div>                 continue</div><div class="diff-removed">-</div><div class="diff-removed">-            lang = lang_dir.parts[-3]  # Get language from path</div><div class="diff-added">+            lang = lang_dir.parts[-3]</div><div>             for problem_dir in lang_dir.glob("*"):</div><div>                 if (problem_dir.name, lang) in hard_set_pairs:</div><div>                     rel_path = problem_dir.relative_to(src_dir)</div><div class="diff-info">@@ -343,13 +324,20 @@ if __name__ == "__main__":</div><div>     parser = argparse.ArgumentParser()</div><div>     parser.add_argument("--topn", type=int, help="Only consider top N models by pass rate")</div><div>     parser.add_argument(</div><div class="diff-removed">-        "dirs", nargs="*", help="Directories to analyze (optional, defaults to leaderboard entries)"</div><div class="diff-added">+        "dirs",</div><div class="diff-added">+        nargs="*",</div><div class="diff-added">+        help="Benchmark directories to analyze "</div><div class="diff-added">+        "(default = all entries from the polyglot leaderboard)",</div><div>     )</div><div>     parser.add_argument(</div><div>         "--copy-hard-set",</div><div>         action="store_true",</div><div class="diff-removed">-        help="Copy hard set problems to tmp.benchmarks/exercism-polygot",</div><div class="diff-added">+        help="Copy hard set problems to tmp.benchmarks/exercism-polyglot",</div><div>     )</div><div>     args = parser.parse_args()</div><div> </div><div class="diff-removed">-    analyze_exercise_solutions(args.dirs if args.dirs else None, args.topn, args.copy_hard_set)</div><div>\ No newline at end of file</div><div class="diff-added">+    analyze_exercise_solutions(</div><div class="diff-added">+        args.dirs if args.dirs else None,</div><div class="diff-added">+        args.topn,</div><div class="diff-added">+        args.copy_hard_set,</div><div class="diff-added">+    )</div><div>\ No newline at end of file</div><div></div></pre>
                </div>
            </div>
        </section>
    </main>
    <footer>
        <p>LoCoDiff-bench - <a href="https://github.com/AbanteAI/LoCoDiff-bench">GitHub Repository</a></p>
    </footer>
</body>
</html>
    