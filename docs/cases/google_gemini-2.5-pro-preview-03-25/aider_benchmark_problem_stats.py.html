<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Case: benchmark/problem_stats.py - Gemini 2.5 Pro (03-25)</title>
    <link rel="stylesheet" href="../../styles.css">
</head>
<body>
    <header>
        <h1>Case: benchmark/problem_stats.py</h1>
        <p><a href="../../models/google_gemini-2.5-pro-preview-03-25.html">‚Üê Back to Gemini 2.5 Pro (03-25) Cases</a> | <a href="../../index.html">Home</a></p>
    </header>
    <main>
        <section class="case-details">
            <div class="case-info">
                <h2>Benchmark Case Information</h2>
                <p><strong>Model:</strong> Gemini 2.5 Pro (03-25)</p>
                <p><strong>Status:</strong> <span class="failure">Failure</span></p>
                <p><strong>Prompt Tokens:</strong> 29665</p>
                <p><strong>Output Tokens:</strong> N/A</p>
                <p><strong>Native Prompt Tokens:</strong> 37033</p>
                <p><strong>Native Completion Tokens:</strong> 8853</p>
                <p><strong>Native Tokens Reasoning:</strong> 5264</p>
                <p><strong>Native Finish Reason:</strong> STOP</p>
                <p><strong>Runtime:</strong> N/As</p>
                <p><strong>Cost:</strong> $0.13482125</p>
            </div>
            
            <div class="content-links">
                <h2>View Content</h2>
                <ul>
                    <li><a href="../../content/google_gemini-2.5-pro-preview-03-25/aider_benchmark_problem_stats.py/prompt.html" class="content-link">View Prompt</a></li>
                    <li><a href="../../content/google_gemini-2.5-pro-preview-03-25/aider_benchmark_problem_stats.py/expected.html" class="content-link">View Expected Output</a></li>
                    <li><a href="../../content/google_gemini-2.5-pro-preview-03-25/aider_benchmark_problem_stats.py/actual.html" class="content-link">View Actual Output</a></li>
                </ul>
            </div>
            
            <div class="diff-section">
                <h2>Diff (Expected vs Actual)</h2>
                <div id="diff-output">
                    <pre class="diff"><div class="diff-header">--- aider_benchmark_problem_stats.py_expectedoutput.txt (expected)+++ aider_benchmark_problem_stats.py_extracted.txt (actual)@@ -11,6 +11,7 @@ from aider.dump import dump  # noqa</div><div> </div><div> HARD_SET_NUM = 3  # Number of models that defines the hard set threshold</div><div class="diff-added">+PARSE_ERROR_M = 4  # Threshold for number of parse errors to DQ an exercise</div><div> </div><div> </div><div> def get_dirs_from_leaderboard():</div><div class="diff-info">@@ -60,8 +61,6 @@ </div><div> </div><div> def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):</div><div class="diff-removed">-    PARSE_ERROR_M = 4  # Threshold for number of parse errors to DQ an exercise</div><div class="diff-removed">-</div><div>     if dirs is None:</div><div>         # Use leaderboard data if no directories specified</div><div>         dir_entries = get_dirs_from_leaderboard()</div><div class="diff-info">@@ -105,12 +104,9 @@     if topn:</div><div>         valid_entries = valid_entries[:topn]</div><div> </div><div class="diff-removed">-    # Get all exercise names from a complete run</div><div class="diff-added">+    # Get all unique exercise names from all results</div><div>     all_exercises = set()</div><div>     exercise_solutions = defaultdict(list)</div><div class="diff-removed">-</div><div class="diff-removed">-    # Get all unique exercise names from all results</div><div class="diff-removed">-    all_exercises = set()</div><div>     for (dirname, model), results, _ in valid_entries:</div><div>         if results:</div><div>             for result in results:</div><div class="diff-info">@@ -156,10 +152,10 @@ </div><div>     for testcase in all_exercises:</div><div>         # Language is already in the testcase string</div><div class="diff-removed">-        lang = testcase.split("/")[0]  # First part is the language</div><div class="diff-added">+        lang = testcase.split("/")[1]  # First part is the language</div><div>         models = exercise_solutions[testcase]</div><div>         num_solved = len(models)</div><div class="diff-removed">-        percent = (num_solved / total_models) * 100</div><div class="diff-added">+        percent = (num_solved / total_models) * 100 if total_models else 0</div><div>         testcase = testcase.replace("exercises/", "")  # Remove the exercises/ prefix</div><div>         # Remove duplicate language prefix (e.g. javascript/javascript/ -> javascript/)</div><div>         if testcase.startswith(f"{lang}/{lang}/"):</div><div class="diff-info">@@ -172,7 +168,7 @@     )  # -x[2] for descending solve rate, x[1] for ascending exercise name</div><div> </div><div>     # Calculate max lengths for alignment after cleaning up paths</div><div class="diff-removed">-    max_name_len = max(len(f"{lang}/{testcase}") for lang, testcase, _, _ in exercise_stats)</div><div class="diff-added">+    max_name_len = max(len(f"{testcase}") for lang, testcase, _, _ in exercise_stats)</div><div> </div><div>     # Print all exercises sorted by solve rate</div><div>     print("\nAll Exercises (sorted by solve rate):")</div><div class="diff-info">@@ -292,7 +288,7 @@                 if tests_outcomes and tests_outcomes[-1]:</div><div>                     solved_hard += 1</div><div> </div><div class="diff-removed">-        pct = (solved_hard / len(hard_set)) * 100</div><div class="diff-added">+        pct = (solved_hard / len(hard_set)) * 100 if hard_set else 0</div><div>         model_hard_stats.append((model, solved_hard, pct))</div><div> </div><div>     # Sort by number solved</div><div></div></pre>
                </div>
            </div>
        </section>
    </main>
    <footer>
        <p>LoCoDiff-bench - <a href="https://github.com/AbanteAI/LoCoDiff-bench">GitHub Repository</a></p>
    </footer>
</body>
</html>
    